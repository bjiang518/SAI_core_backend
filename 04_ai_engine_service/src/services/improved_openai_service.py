"""
Enhanced AI Service with Streaming, Caching, and Optimized Performance

This optimized version provides:
1. Response streaming for better UX
2. Redis caching for frequent patterns  
3. Connection pooling and retry logic
4. Memory-efficient processing
5. Batch request handling
6. Response compression
"""

import openai
import asyncio
import json
import re
from typing import Dict, List, Optional, Any, AsyncGenerator, Literal
from pydantic import BaseModel, Field
from .prompt_service import AdvancedPromptService, Subject
import os
import hashlib
from datetime import datetime, timedelta
from dotenv import load_dotenv
import gzip
import time

load_dotenv()


# MARK: - Pydantic Models for Structured Output

class QuestionGrade(BaseModel):
    """Structured model for a graded homework question."""
    question_number: int
    raw_question_text: str
    question_text: str
    student_answer: str
    correct_answer: str
    grade: Literal["CORRECT", "INCORRECT", "EMPTY", "PARTIAL_CREDIT"]
    points_earned: float = Field(ge=0, le=1)
    points_possible: float = Field(default=1.0)
    confidence: float = Field(ge=0, le=1)
    feedback: str
    has_visuals: bool = False
    sub_parts: List[str] = Field(default_factory=list)


class PerformanceSummary(BaseModel):
    """Summary of student performance on homework."""
    total_correct: int
    total_incorrect: int
    total_empty: int
    accuracy_rate: float = Field(ge=0, le=1)
    summary_text: str


class HomeworkGradingResult(BaseModel):
    """Complete structured homework grading result."""
    subject: str
    subject_confidence: float = Field(ge=0, le=1)
    total_questions_found: int
    questions: List[QuestionGrade]
    performance_summary: PerformanceSummary
    processing_notes: str = ""


class OptimizedEducationalAIService:
    """
    High-performance AI service with streaming, caching, and advanced optimization.
    Designed for production-scale usage with memory efficiency and response speed.
    """
    
    def __init__(self):
        print("ðŸ”„ === INITIALIZING OPTIMIZED AI SERVICE ===")
        
        # Check OpenAI API key first
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            print("âŒ WARNING: OPENAI_API_KEY not found in environment")
        else:
            print(f"âœ… OpenAI API key found: {api_key[:10]}..." if len(api_key) > 10 else "âœ… OpenAI API key found (short)")
        
        # OpenAI client with connection pooling
        try:
            self.client = openai.AsyncOpenAI(
                api_key=api_key,
                max_retries=3,
                timeout=60.0
            )
            print(f"âœ… OpenAI AsyncClient initialized: {type(self.client)}")
        except Exception as e:
            print(f"âŒ Failed to initialize OpenAI client: {e}")
            raise
        
        self.prompt_service = AdvancedPromptService()
        self.model = "gpt-4o-mini"  # Optimized for cost and speed
        self.vision_model = "gpt-4o-2024-08-06"  # Required for structured outputs
        self.structured_output_model = "gpt-4o-2024-08-06"  # For structured outputs
        print(f"âœ… Models configured - Text: {self.model}, Vision: {self.vision_model}, Structured: {self.structured_output_model}")
        
        # In-memory cache (fallback if Redis not available)
        self.memory_cache = {}
        self.cache_size_limit = 1000
        
        # Request deduplication
        self.pending_requests = {}
        
        # Performance metrics
        self.request_count = 0
        self.cache_hits = 0
        
        print("âœ… AI Service initialization complete")
        print("================================================")
    
    # MARK: - Caching System
    
    def _generate_cache_key(self, content: str, model: str) -> str:
        """Generate cache key from request content."""
        combined = f"{model}:{content}"
        return hashlib.sha256(combined.encode()).hexdigest()[:16]
    
    def _get_cached_response(self, cache_key: str) -> Optional[Dict]:
        """Get cached response if available."""
        if cache_key in self.memory_cache:
            cached_data = self.memory_cache[cache_key]
            
            # Check if expired (1 hour TTL)
            if time.time() - cached_data['timestamp'] < 3600:
                self.cache_hits += 1
                return cached_data['response']
            else:
                # Remove expired entry
                del self.memory_cache[cache_key]
        
        return None
    
    def _set_cached_response(self, cache_key: str, response: Dict):
        """Cache response in memory."""
        # Clean old entries if cache is full
        if len(self.memory_cache) >= self.cache_size_limit:
            # Remove oldest entries
            oldest_keys = sorted(
                self.memory_cache.keys(),
                key=lambda k: self.memory_cache[k]['timestamp']
            )[:100]
            
            for key in oldest_keys:
                del self.memory_cache[key]
        
        self.memory_cache[cache_key] = {
            'response': response,
            'timestamp': time.time()
        }
    
    # MARK: - Request Deduplication
    
    async def _deduplicate_request(self, cache_key: str, request_func):
        """Prevent duplicate requests for same content."""
        if cache_key in self.pending_requests:
            # Wait for existing request to complete
            return await self.pending_requests[cache_key]
        
        # Create new request
        task = asyncio.create_task(request_func())
        self.pending_requests[cache_key] = task
        
        try:
            result = await task
            return result
        finally:
            # Clean up pending request
            self.pending_requests.pop(cache_key, None)

    # MARK: - Robust Text Parsing for Questions

    def _parse_questions_from_text(self, raw_response: str) -> List[Dict]:
        """
        Parse questions from text using robust delimiter-based approach.
        Much more reliable than JSON parsing for question generation.
        """
        print(f"ðŸ” Starting robust text parsing for questions...")
        print(f"ðŸ“ Raw response length: {len(raw_response)} characters")

        questions = []

        # Try JSON parsing first as fallback
        try:
            json_data = json.loads(raw_response)
            if isinstance(json_data, list):
                print(f"âœ… Successfully parsed as direct JSON array with {len(json_data)} questions")
                return json_data
            elif isinstance(json_data, dict) and "questions" in json_data:
                print(f"âœ… Successfully parsed as JSON object with {len(json_data['questions'])} questions")
                return json_data["questions"]
        except (json.JSONDecodeError, KeyError):
            print(f"âš ï¸ JSON parsing failed, switching to text parsing...")

        # Text-based parsing with delimiters
        # Look for question patterns: "question": "...", "type": "...", etc.
        lines = raw_response.split('\n')
        current_question = {}

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # Look for field patterns with robust regex
            if '"question"' in line and ':' in line:
                # Extract question text
                question_match = re.search(r'"question":\s*"([^"]*)"', line)
                if question_match:
                    current_question['question'] = question_match.group(1)

            elif '"type"' in line and ':' in line:
                type_match = re.search(r'"type":\s*"([^"]*)"', line)
                if type_match:
                    current_question['type'] = type_match.group(1)

            elif '"correct_answer"' in line and ':' in line:
                answer_match = re.search(r'"correct_answer":\s*"([^"]*)"', line)
                if answer_match:
                    current_question['correct_answer'] = answer_match.group(1)

            elif '"explanation"' in line and ':' in line:
                explanation_match = re.search(r'"explanation":\s*"([^"]*)"', line)
                if explanation_match:
                    current_question['explanation'] = explanation_match.group(1)

            elif '"topic"' in line and ':' in line:
                topic_match = re.search(r'"topic":\s*"([^"]*)"', line)
                if topic_match:
                    current_question['topic'] = topic_match.group(1)

            elif '"difficulty"' in line and ':' in line:
                difficulty_match = re.search(r'"difficulty":\s*"([^"]*)"', line)
                if difficulty_match:
                    current_question['difficulty'] = difficulty_match.group(1)

            elif '"options"' in line and '[' in line:
                # Extract options array
                options_match = re.search(r'"options":\s*\[(.*?)\]', line)
                if options_match:
                    options_str = options_match.group(1)
                    # Parse individual options
                    options = []
                    for opt in re.findall(r'"([^"]*)"', options_str):
                        options.append(opt)
                    current_question['options'] = options

            # Check if we have a complete question
            if (len(current_question) >= 4 and
                'question' in current_question and
                'type' in current_question and
                'correct_answer' in current_question and
                'explanation' in current_question):

                # Set defaults for missing fields
                if 'topic' not in current_question:
                    current_question['topic'] = 'General'
                if 'difficulty' not in current_question:
                    current_question['difficulty'] = 'intermediate'
                if 'options' not in current_question:
                    current_question['options'] = None

                questions.append(current_question.copy())
                print(f"âœ… Parsed question {len(questions)}: {current_question['question'][:50]}...")
                current_question = {}

        print(f"ðŸŽ¯ Text parsing completed: {len(questions)} questions extracted")

        # If text parsing failed, try even more aggressive parsing
        if len(questions) == 0:
            print(f"âš ï¸ Text parsing found no questions, trying aggressive fallback...")
            questions = self._aggressive_question_extraction(raw_response)

        return questions

    def _aggressive_question_extraction(self, text: str) -> List[Dict]:
        """
        Last resort: extract questions using pattern matching and heuristics.
        """
        questions = []

        # Look for question-like patterns
        question_patterns = [
            r'(?i)what\s+is.*\?',
            r'(?i)which\s+of.*\?',
            r'(?i)how\s+.*\?',
            r'(?i)calculate.*\?',
            r'(?i)solve.*\?',
            r'(?i)find.*\?'
        ]

        for pattern in question_patterns:
            matches = re.findall(pattern, text)
            for i, match in enumerate(matches[:3]):  # Max 3 questions per pattern
                questions.append({
                    'question': match.strip(),
                    'type': 'short_answer',
                    'correct_answer': 'Please provide the correct answer',
                    'explanation': 'Generated question needs manual review',
                    'topic': 'General',
                    'difficulty': 'intermediate',
                    'options': None
                })

        print(f"ðŸ” Aggressive extraction found {len(questions)} question patterns")
        return questions[:5]  # Limit to 5 questions max
    
    async def parse_homework_image_json(
        self,
        base64_image: str,
        custom_prompt: Optional[str] = None,
        student_context: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """
        Parse homework images with guaranteed JSON structure using OpenAI structured outputs.

        This method guarantees 100% consistent response format by:
        1. Using OpenAI's beta.chat.completions.parse with Pydantic models
        2. Automatic JSON schema generation from Pydantic models
        3. Guaranteed valid structured output (no parsing failures)
        4. Converting to legacy format for iOS compatibility

        Args:
            base64_image: Base64 encoded image data
            custom_prompt: Optional additional context
            student_context: Optional student learning context

        Returns:
            Structured response with guaranteed consistent formatting
        """

        try:
            # Create the strict JSON schema prompt
            system_prompt = self._create_json_schema_prompt(custom_prompt, student_context)

            # Prepare image message for OpenAI Vision API
            image_url = f"data:image/jpeg;base64,{base64_image}"

            # Get fixed max tokens - no heuristics, just generous allocation
            max_tokens = self._get_max_tokens_for_homework()

            print(f"ðŸ” Allocating {max_tokens} tokens for homework parsing (supports ~30 questions)")

            # Call OpenAI with structured output (guaranteed JSON)
            # Note: Structured outputs are only available in certain OpenAI SDK versions
            # For now, use regular chat completions with JSON mode
            response = await self.client.chat.completions.create(
                model=self.structured_output_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": f"Grade ALL questions in this homework. {custom_prompt or ''}"
                            },
                            {
                                "type": "image_url",
                                "image_url": {"url": image_url, "detail": "auto"}  # Auto for faster processing
                            }
                        ]
                    }
                ],
                response_format={"type": "json_object"},  # JSON mode instead of beta parse
                temperature=0,  # Zero temperature for maximum speed and consistency
                max_tokens=max_tokens,  # Optimized allocation
            )

            # Parse JSON response manually
            raw_response = response.choices[0].message.content

            print(f"âœ… === OPENAI RESPONSE RECEIVED ===")
            print(f"ðŸ“Š Raw response length: {len(raw_response)} chars")
            print(f"ðŸ“„ Response preview: {raw_response[:200]}...")
            print(f"=====================================")

            try:
                # Parse JSON
                import json

                # Try to clean malformed JSON before parsing
                cleaned_response = self._clean_json_response(raw_response)
                result_dict = json.loads(cleaned_response)

                print(f"âœ… === JSON PARSED SUCCESSFULLY ===")
                print(f"ðŸ“Š Keys: {list(result_dict.keys())}")
                print(f"ðŸ“š Subject: {result_dict.get('subject', 'Unknown')}")
                print(f"ðŸ“Š Total questions found: {result_dict.get('total_questions_found', 0)}")
                print(f"ðŸ“ Questions array length: {len(result_dict.get('questions', []))}")

                # Validate JSON structure
                if not self._validate_json_structure(result_dict):
                    print(f"âš ï¸ JSON structure validation failed, attempting repair...")
                    result_dict = self._repair_json_structure(result_dict)

                # Log full JSON for debugging
                print(f"ðŸ“„ === FULL JSON RESPONSE ===")
                print(json.dumps(result_dict, indent=2))
                print(f"=====================================")

                # Convert to legacy format for iOS compatibility
                legacy_response = self._convert_json_to_legacy_format(result_dict)

                print(f"âœ… === LEGACY FORMAT CONVERSION ===")
                print(f"ðŸ“ Legacy response length: {len(legacy_response)} chars")
                print(f"ðŸ“„ === FULL LEGACY RESPONSE ===")
                print(legacy_response)
                print(f"=====================================")

                print(f"âœ… Structured parsing complete")

                return {
                    "success": True,
                    "structured_response": legacy_response,
                    "parsing_method": "json_mode",  # Updated method indicator
                    "total_questions": len(result_dict.get("questions", [])),
                    "subject_detected": result_dict.get("subject", "Other"),
                    "subject_confidence": result_dict.get("subject_confidence", 0.5),
                    "raw_json": result_dict
                }
            except json.JSONDecodeError as je:
                print(f"âš ï¸ JSON decode error: {je}")
                print(f"ðŸ“„ Raw response: {raw_response[:500]}...")
                # Fallback to text parsing
                return await self._fallback_text_parsing(raw_response, custom_prompt)

        except Exception as e:
            print(f"âŒ Structured output parsing error: {e}")
            print(f"âŒ Error type: {type(e).__name__}")

            # Fallback to old method if structured outputs fail
            return {
                "success": False,
                "structured_response": self._create_error_response(str(e)),
                "parsing_method": "error_fallback",
                "error": str(e)
            }

    def _get_max_tokens_for_homework(self) -> int:
        """
        Get max tokens for homework parsing.

        Optimized allocation based on typical homework:
        - Concise prompt: ~200 tokens (down from ~800)
        - Per question with grading: ~250 tokens (down from ~400)
        - Target: handle up to 30 questions comfortably
        - Result: 200 + (30 Ã— 250) = 7700 tokens
        """
        return 8000  # Reduced from 14000 for faster processing

    def _convert_json_to_legacy_format(self, result_dict: Dict) -> str:
        """Convert JSON dict to legacy iOS-compatible text format."""
        lines = []

        # Subject line
        lines.append(f"SUBJECT: {result_dict.get('subject', 'Other')}")
        lines.append(f"CONFIDENCE: {result_dict.get('subject_confidence', 0.5):.2f}")
        lines.append("")

        # Performance summary (if present)
        if "performance_summary" in result_dict:
            perf = result_dict['performance_summary']
            lines.append("PERFORMANCE SUMMARY:")
            lines.append(f"Total Correct: {perf.get('total_correct', 0)}")
            lines.append(f"Total Incorrect: {perf.get('total_incorrect', 0)}")
            lines.append(f"Total Empty: {perf.get('total_empty', 0)}")
            lines.append(f"Accuracy: {perf.get('accuracy_rate', 0):.0%}")
            lines.append(f"Summary: {perf.get('summary_text', '')}")
            lines.append("")

        # Questions - with proper separator for iOS parsing
        questions = result_dict.get('questions', [])
        for i, q in enumerate(questions):
            lines.append(f"QUESTION_NUMBER: {q.get('question_number', i + 1)}")
            lines.append(f"RAW_QUESTION: {q.get('raw_question_text', q.get('question_text', ''))}")
            lines.append(f"QUESTION: {q.get('question_text', '')}")
            lines.append(f"STUDENT_ANSWER: {q.get('student_answer', '')}")
            lines.append(f"CORRECT_ANSWER: {q.get('correct_answer', '')}")
            lines.append(f"GRADE: {q.get('grade', 'UNKNOWN')}")
            lines.append(f"POINTS: {q.get('points_earned', 0)}/{q.get('points_possible', 1)}")
            lines.append(f"CONFIDENCE: {q.get('confidence', 0.5):.2f}")
            lines.append(f"FEEDBACK: {q.get('feedback', '')}")
            lines.append(f"HAS_VISUALS: {q.get('has_visuals', False)}")
            if q.get('sub_parts'):
                lines.append(f"SUB_PARTS: {', '.join(q['sub_parts'])}")

            # Add separator between questions (not after the last one)
            if i < len(questions) - 1:
                lines.append("â•â•â•QUESTION_SEPARATORâ•â•â•")
            else:
                lines.append("")  # Empty line at the end

        return "\n".join(lines)

    def _convert_pydantic_to_legacy_format(self, result_dict: Dict) -> str:
        """Legacy method - now calls _convert_json_to_legacy_format."""
        return self._convert_json_to_legacy_format(result_dict)
    
    def _create_json_schema_prompt(self, custom_prompt: Optional[str], student_context: Optional[Dict]) -> str:
        """Create a concise prompt that enforces strict JSON schema for grading."""

        base_prompt = """Grade this completed homework. Return ONLY valid JSON:

{
  "subject": "Mathematics|Physics|Chemistry|Biology|English|History|Geography|Computer Science|Other",
  "subject_confidence": 0.95,
  "total_questions_found": <COUNT>,
  "questions": [
    {
      "question_number": 1,
      "raw_question_text": "exact text from image",
      "question_text": "cleaned text",
      "student_answer": "what student wrote",
      "correct_answer": "expected answer",
      "grade": "CORRECT|INCORRECT|EMPTY|PARTIAL_CREDIT",
      "points_earned": 1.0,
      "points_possible": 1.0,
      "confidence": 0.9,
      "has_visuals": false,
      "feedback": "brief feedback",
      "sub_parts": []
    }
  ],
  "performance_summary": {
    "total_correct": <N>,
    "total_incorrect": <N>,
    "total_empty": <N>,
    "accuracy_rate": <0.0-1.0>,
    "summary_text": "concise summary"
  },
  "processing_notes": "optional notes"
}

RULES:
1. Questions a,b,c,d = separate questions (NOT sub-parts)
2. Questions 1a,1b,2a,2b = sub_parts under parent
3. Grade: CORRECT (1.0 pts), INCORRECT (0.0 pts), EMPTY (0.0 pts), PARTIAL_CREDIT (0.5 pts)
4. Feedback: Keep under 15 words
5. Extract ALL questions and student answers from image"""

        if student_context:
            base_prompt += f"\nStudent: {student_context.get('student_id', 'anonymous')}"

        if custom_prompt:
            base_prompt += f"\nContext: {custom_prompt}"

        return base_prompt
    
    def _validate_json_structure(self, json_data: Dict) -> bool:
        """Validate that JSON has required grading structure."""
        required_fields = ["subject", "questions", "performance_summary"]
        
        if not isinstance(json_data, dict):
            return False
        
        for field in required_fields:
            if field not in json_data:
                return False
        
        if not isinstance(json_data.get("questions"), list):
            return False
        
        if len(json_data["questions"]) == 0:
            return False
        
        # Validate first question structure for grading
        first_question = json_data["questions"][0]
        question_fields = ["raw_question_text", "question_text", "student_answer", "correct_answer", "grade"]
        
        for field in question_fields:
            if field not in first_question:
                return False
        
        # Validate performance_summary structure
        performance_summary = json_data.get("performance_summary", {})
        summary_fields = ["total_correct", "total_incorrect", "accuracy_rate", "summary_text"]
        
        for field in summary_fields:
            if field not in performance_summary:
                return False
        
        # Validate grade values
        valid_grades = ["CORRECT", "INCORRECT", "EMPTY", "PARTIAL_CREDIT"]
        if first_question.get("grade") not in valid_grades:
            return False
        
        return True

    def _clean_json_response(self, raw_response: str) -> str:
        """Clean malformed JSON by removing duplicates and fixing common issues."""
        import re

        # Remove any markdown code blocks
        cleaned = re.sub(r'```json\n?', '', raw_response)
        cleaned = re.sub(r'```\n?', '', cleaned)

        # Remove duplicate keys by finding the last occurrence
        # This handles cases where OpenAI returns duplicate fields
        try:
            # First try to extract just the JSON object
            json_match = re.search(r'\{.*\}', cleaned, re.DOTALL)
            if json_match:
                cleaned = json_match.group(0)

            # Remove trailing commas before closing braces/brackets
            cleaned = re.sub(r',(\s*[}\]])', r'\1', cleaned)

            return cleaned
        except Exception as e:
            print(f"âš ï¸ JSON cleaning error: {e}")
            return raw_response

    def _repair_json_structure(self, json_data: Dict) -> Dict:
        """Repair incomplete or malformed JSON structure to meet validation requirements."""
        repaired = json_data.copy()

        # Ensure required top-level fields exist
        if "subject" not in repaired:
            repaired["subject"] = "Other"
        if "subject_confidence" not in repaired:
            repaired["subject_confidence"] = 0.5
        if "total_questions_found" not in repaired:
            repaired["total_questions_found"] = len(repaired.get("questions", []))

        # Ensure questions array exists
        if "questions" not in repaired or not isinstance(repaired["questions"], list):
            repaired["questions"] = []

        # Repair each question to have all required fields
        for i, question in enumerate(repaired["questions"]):
            if not isinstance(question, dict):
                continue

            # Set defaults for missing fields
            question.setdefault("question_number", i + 1)
            question.setdefault("raw_question_text", question.get("question_text", ""))
            question.setdefault("question_text", question.get("raw_question_text", f"Question {i + 1}"))
            question.setdefault("student_answer", "")
            question.setdefault("correct_answer", "")
            question.setdefault("grade", "EMPTY")
            question.setdefault("points_earned", 0.0)
            question.setdefault("points_possible", 1.0)
            question.setdefault("confidence", 0.5)
            question.setdefault("has_visuals", False)
            question.setdefault("feedback", "")
            question.setdefault("sub_parts", [])

            # Validate and fix grade values
            valid_grades = ["CORRECT", "INCORRECT", "EMPTY", "PARTIAL_CREDIT"]
            if question.get("grade") not in valid_grades:
                question["grade"] = "EMPTY"

        # Ensure performance_summary exists with all required fields
        if "performance_summary" not in repaired:
            repaired["performance_summary"] = {}

        perf = repaired["performance_summary"]
        total_questions = len(repaired["questions"])
        correct_count = sum(1 for q in repaired["questions"] if q.get("grade") == "CORRECT")
        incorrect_count = sum(1 for q in repaired["questions"] if q.get("grade") == "INCORRECT")
        empty_count = sum(1 for q in repaired["questions"] if q.get("grade") == "EMPTY")

        perf.setdefault("total_correct", correct_count)
        perf.setdefault("total_incorrect", incorrect_count)
        perf.setdefault("total_empty", empty_count)
        perf.setdefault("accuracy_rate", correct_count / total_questions if total_questions > 0 else 0.0)
        perf.setdefault("summary_text", f"Graded {total_questions} questions: {correct_count} correct, {incorrect_count} incorrect")

        # Ensure processing_notes exists
        repaired.setdefault("processing_notes", "JSON structure repaired for consistency")

        print(f"âœ… JSON structure repaired: {len(repaired['questions'])} questions validated")

        return repaired

    def _normalize_json_response(self, json_data: Dict) -> Dict:
        """Normalize JSON response to consistent grading format."""
        
        # Extract performance summary
        performance_summary = json_data.get("performance_summary", {})
        
        normalized = {
            "subject": json_data.get("subject", "Other"),
            "subject_confidence": float(json_data.get("subject_confidence", 0.5)),
            "total_questions": json_data.get("total_questions_found", len(json_data.get("questions", []))),
            "questions": [],
            "processing_notes": json_data.get("processing_notes", "Graded successfully"),
            "performance_summary": {
                "total_correct": performance_summary.get("total_correct", 0),
                "total_incorrect": performance_summary.get("total_incorrect", 0),
                "total_empty": performance_summary.get("total_empty", 0),
                "accuracy_rate": float(performance_summary.get("accuracy_rate", 0.0)),
                "summary_text": performance_summary.get("summary_text", "No summary available")
            }
        }
        
        # Normalize each graded question
        for i, question in enumerate(json_data.get("questions", [])):
            normalized_question = {
                "question_number": question.get("question_number", i + 1),
                "raw_question_text": question.get("raw_question_text", question.get("question_text", "Raw question not found")),
                "question_text": question.get("question_text", "Question text not found"),
                "student_answer": question.get("student_answer", ""),
                "correct_answer": question.get("correct_answer", question.get("answer", "Answer not provided")),
                "grade": question.get("grade", "EMPTY"),
                "points_earned": float(question.get("points_earned", 0.0)),
                "points_possible": float(question.get("points_possible", 1.0)),
                "confidence": float(question.get("confidence", 0.8)),
                "has_visuals": bool(question.get("has_visuals", False)),
                "feedback": question.get("feedback", "No feedback provided"),
                "sub_parts": question.get("sub_parts", question.get("sub_questions", []))
            }
            normalized["questions"].append(normalized_question)
        
        return normalized
    
    def _convert_to_legacy_format(self, normalized_data: Dict) -> str:
        """Convert normalized JSON to legacy â•â•â•QUESTION_SEPARATORâ•â•â• format for iOS grading compatibility."""
        
        performance_summary = normalized_data.get("performance_summary", {})
        
        legacy_response = f"SUBJECT: {normalized_data['subject']}\n"
        legacy_response += f"SUBJECT_CONFIDENCE: {normalized_data['subject_confidence']}\n"
        legacy_response += f"TOTAL_QUESTIONS: {normalized_data['total_questions']}\n"
        legacy_response += f"JSON_PARSING: true\n"
        legacy_response += f"PARSING_METHOD: Enhanced AI Backend Grading with JSON Schema\n"
        
        # Add performance summary
        legacy_response += f"TOTAL_CORRECT: {performance_summary.get('total_correct', 0)}\n"
        legacy_response += f"TOTAL_INCORRECT: {performance_summary.get('total_incorrect', 0)}\n"
        legacy_response += f"TOTAL_EMPTY: {performance_summary.get('total_empty', 0)}\n"
        legacy_response += f"ACCURACY_RATE: {performance_summary.get('accuracy_rate', 0.0)}\n"
        legacy_response += f"SUMMARY_TEXT: {performance_summary.get('summary_text', 'No summary available')}\n\n"
        
        for i, question in enumerate(normalized_data["questions"]):
            if i > 0:
                legacy_response += "â•â•â•QUESTION_SEPARATORâ•â•â•\n"
            
            legacy_response += f"QUESTION_NUMBER: {question['question_number']}\n"
            legacy_response += f"RAW_QUESTION: {question['raw_question_text']}\n"
            legacy_response += f"QUESTION: {question['question_text']}\n"
            legacy_response += f"STUDENT_ANSWER: {question['student_answer']}\n"
            legacy_response += f"CORRECT_ANSWER: {question['correct_answer']}\n"
            legacy_response += f"GRADE: {question['grade']}\n"
            legacy_response += f"POINTS_EARNED: {question['points_earned']}\n"
            legacy_response += f"POINTS_POSSIBLE: {question['points_possible']}\n"
            legacy_response += f"FEEDBACK: {question['feedback']}\n"
            legacy_response += f"CONFIDENCE: {question['confidence']}\n"
            legacy_response += f"HAS_VISUALS: {'true' if question['has_visuals'] else 'false'}\n"
            
            if question['sub_parts']:
                legacy_response += f"SUB_PARTS: {'; '.join(question['sub_parts'])}\n"
        
        return legacy_response
    
    def _convert_to_fallback_legacy_format(self, normalized_data: Dict) -> str:
        """Convert normalized data to legacy format marked as fallback parsing."""
        
        legacy_response = f"SUBJECT: {normalized_data['subject']}\n"
        legacy_response += f"SUBJECT_CONFIDENCE: {normalized_data['subject_confidence']}\n"
        legacy_response += f"TOTAL_QUESTIONS: {normalized_data['total_questions']}\n"
        legacy_response += f"JSON_PARSING: false\n"
        legacy_response += f"PARSING_METHOD: Enhanced AI Backend Parsing with Fallback Text Analysis\n\n"
        
        for i, question in enumerate(normalized_data["questions"]):
            if i > 0:
                legacy_response += "â•â•â•QUESTION_SEPARATORâ•â•â•\n"
            
            legacy_response += f"QUESTION_NUMBER: {question['question_number']}\n"
            legacy_response += f"QUESTION: {question['question_text']}\n"
            legacy_response += f"ANSWER: {question['answer']}\n"
            legacy_response += f"CONFIDENCE: {question['confidence']}\n"
            legacy_response += f"HAS_VISUALS: {'true' if question['has_visuals'] else 'false'}\n"
            
            if question['sub_parts']:
                legacy_response += f"SUB_PARTS: {'; '.join(question['sub_parts'])}\n"
        
        return legacy_response
    
    async def _fallback_text_parsing(self, raw_response: str, custom_prompt: Optional[str]) -> Dict[str, Any]:
        """Robust fallback parsing when JSON format fails."""
        
        print("ðŸ”„ Using fallback text parsing...")
        
        try:
            # Try to extract structured information from text
            subject = self._extract_subject_from_text(raw_response)
            confidence = self._extract_confidence_from_text(raw_response)
            questions = self._extract_questions_from_text(raw_response)
            
            # If no structured questions found, create a basic response
            if not questions:
                questions = [{
                    "question_number": 1,
                    "question_text": "Unable to parse specific questions from image",
                    "answer": raw_response[:500] + "..." if len(raw_response) > 500 else raw_response,
                    "confidence": 0.3,
                    "has_visuals": False,
                    "sub_parts": []
                }]
            
            # Create normalized format
            normalized_data = {
                "subject": subject,
                "subject_confidence": confidence,
                "total_questions": len(questions),
                "questions": questions,
                "processing_notes": "Parsed using fallback text analysis"
            }
            
            # Convert to legacy format (mark as fallback)
            legacy_response = self._convert_to_fallback_legacy_format(normalized_data)
            
            return {
                "success": True,
                "structured_response": legacy_response,
                "parsing_method": "fallback_text",
                "total_questions": len(questions),
                "subject_detected": subject,
                "subject_confidence": confidence
            }
            
        except Exception as fallback_error:
            print(f"âŒ Fallback parsing also failed: {fallback_error}")
            return {
                "success": False,
                "structured_response": self._create_error_response(f"All parsing methods failed: {fallback_error}"),
                "parsing_method": "complete_failure",
                "error": str(fallback_error)
            }
    
    def _extract_subject_from_text(self, text: str) -> str:
        """Extract subject from unstructured text."""
        text_lower = text.lower()
        
        subjects = {
            "mathematics": ["math", "equation", "algebra", "geometry", "calculus", "statistics"],
            "physics": ["physics", "force", "energy", "momentum", "wave", "electromagnetic"],
            "chemistry": ["chemistry", "molecule", "atom", "reaction", "compound", "element"],
            "biology": ["biology", "cell", "organism", "genetics", "evolution", "ecosystem"],
            "english": ["english", "literature", "grammar", "writing", "poetry", "essay"],
            "history": ["history", "historical", "event", "century", "civilization", "culture"]
        }
        
        for subject, keywords in subjects.items():
            if any(keyword in text_lower for keyword in keywords):
                return subject.title()
        
        return "Other"
    
    def _extract_confidence_from_text(self, text: str) -> float:
        """Extract confidence score from text or estimate based on content."""
        # Look for explicit confidence patterns
        confidence_patterns = [
            r"confidence[:\s]*([0-9.]+)",
            r"certainty[:\s]*([0-9.]+)",
            r"sure[:\s]*([0-9.]+)"
        ]
        
        for pattern in confidence_patterns:
            match = re.search(pattern, text.lower())
            if match:
                try:
                    return float(match.group(1))
                except ValueError:
                    continue
        
        # Estimate confidence based on text quality
        if len(text) > 100 and "step" in text.lower():
            return 0.8
        elif len(text) > 50:
            return 0.6
        else:
            return 0.4
    
    def _extract_questions_from_text(self, text: str) -> List[Dict]:
        """Extract questions from unstructured text."""
        questions = []
        
        # Split by common question indicators
        question_patterns = [
            r'\n\s*\d+[\.)]\s+',  # "1. " or "1) "
            r'\n\s*[Qq]uestion\s*\d*[:\s]+',  # "Question 1:"
            r'\n\s*[a-z][\.)]\s+',  # "a) " or "b."
            r'â•â•â•QUESTION_SEPARATORâ•â•â•'  # Legacy separator
        ]
        
        # Try to split the text using these patterns
        text_parts = [text]  # Start with full text
        
        for pattern in question_patterns:
            new_parts = []
            for part in text_parts:
                splits = re.split(pattern, part)
                new_parts.extend([s.strip() for s in splits if s.strip()])
            text_parts = new_parts
        
        # Process each potential question
        for i, part in enumerate(text_parts):
            if len(part) > 20:  # Filter out very short fragments
                # Try to split into question and answer
                lines = part.split('\n')
                question_text = lines[0] if lines else part[:100]
                answer_text = '\n'.join(lines[1:]) if len(lines) > 1 else part
                
                questions.append({
                    "question_number": i + 1,
                    "question_text": self._clean_question_text(question_text),
                    "answer": answer_text.strip(),
                    "confidence": 0.7,
                    "has_visuals": self._detect_visual_content(part),
                    "sub_parts": []
                })
        
        return questions[:5]  # Limit to 5 questions max
    
    def _clean_question_text(self, text: str) -> str:
        """Clean question text by removing numbering and formatting."""
        # Remove common question prefixes
        text = re.sub(r'^\d+[\.)]\s*', '', text)  # Remove "1. " or "1) "
        text = re.sub(r'^[Qq]uestion\s*\d*[:\s]*', '', text, flags=re.IGNORECASE)  # Remove "Question:"
        text = re.sub(r'^[a-z][\.)]\s*', '', text, flags=re.IGNORECASE)  # Remove "a) "
        
        return text.strip()
    
    def _detect_visual_content(self, text: str) -> bool:
        """Detect if text suggests visual elements."""
        visual_indicators = [
            "diagram", "graph", "chart", "figure", "image", "picture",
            "drawing", "illustration", "plot", "visual", "shown", "depicted"
        ]
        
        text_lower = text.lower()
        return any(indicator in text_lower for indicator in visual_indicators)
    
    def _create_error_response(self, error_message: str) -> str:
        """Create a standard error response in legacy format."""
        return f"""SUBJECT: Other
SUBJECT_CONFIDENCE: 0.1

QUESTION_NUMBER: 1
QUESTION: Unable to parse homework image
ANSWER: Processing error occurred: {error_message}. Please try again with a clearer image.
CONFIDENCE: 0.1
HAS_VISUALS: false"""
    
    async def health_check(self) -> Dict[str, Any]:
        """Enhanced health check for the improved AI service."""
        try:
            # Test basic JSON response capability
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "user", "content": "Respond with JSON: {\"status\": \"healthy\", \"test\": true}"}
                ],
                max_tokens=50,
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            
            return {
                "status": "healthy",
                "model": self.model,
                "json_support": True,
                "strict_formatting": True,
                "fallback_parsing": True,
                "test_response": result
            }
            
        except Exception as e:
            return {
                "status": "unhealthy",
                "model": self.model,
                "json_support": False,
                "error": str(e)
            }


# Maintain backward compatibility by extending the original service
class EducationalAIService:
    """
    Enhanced version of the original EducationalAIService with improved parsing.
    This maintains all existing functionality while adding the new consistent parsing.
    """
    
    def __init__(self):
        # Set up OpenAI client properly
        self.client = openai.AsyncOpenAI(
            api_key=os.getenv('OPENAI_API_KEY'),
            max_retries=3,
            timeout=60.0
        )
        self.prompt_service = AdvancedPromptService()
        self.model = "gpt-4o-mini"
        self.vision_model = "gpt-4o"  # Full model for vision tasks
        
        # Add the improved service for homework parsing
        self.improved_service = OptimizedEducationalAIService()
    
    async def parse_homework_image(
        self,
        base64_image: str,
        custom_prompt: Optional[str] = None,
        student_context: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """
        Enhanced homework parsing with improved consistency.
        
        This method now uses the improved AI service for better results while
        maintaining backward compatibility with the existing iOS app.
        """
        
        # Use the improved service for better consistency
        return await self.improved_service.parse_homework_image_json(
            base64_image=base64_image,
            custom_prompt=custom_prompt,
            student_context=student_context
        )
    
    async def process_session_conversation(
        self, 
        session_id: str,
        message: str, 
        image_data: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Process session-based conversation messages with specialized prompting for tutoring sessions.
        
        This method is specifically designed for conversational AI tutoring sessions, different from
        simple question processing. It uses specialized prompting strategies optimized for:
        - Back-and-forth educational conversations
        - Consistent LaTeX formatting for iOS post-processing  
        - Conversational flow and engagement
        - Session-specific context handling
        """
        
        try:
            # Use specialized conversational prompting
            system_prompt = self.prompt_service.create_session_conversation_prompt(
                message=message,
                session_id=session_id
            )
            
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": message}
            ]
            
            # Add image analysis if provided
            if image_data:
                # For session conversations with images, add image content to the message
                messages[-1]["content"] = [
                    {"type": "text", "text": message},
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{image_data}",
                            "detail": "high"
                        }
                    }
                ]
            
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.4,  # Slightly higher for more conversational responses
                max_tokens=1200,  # Shorter responses for conversations
                presence_penalty=0.1,
                frequency_penalty=0.1
            )
            
            raw_answer = response.choices[0].message.content
            
            # Apply session-specific optimization (focuses on conversational flow)
            optimized_answer = self.prompt_service.optimize_session_response(raw_answer)
            
            return {
                "success": True,
                "answer": optimized_answer,
                "tokens_used": response.usage.total_tokens if response.usage else 0,
                "compressed": False,
                "session_id": session_id,
                "processing_details": {
                    "model_used": self.model,
                    "prompt_optimization": True,
                    "response_optimization": True,
                    "conversation_mode": True,
                    "session_specific": True
                }
            }
        
        except Exception as e:
            return {
                "success": False,
                "error": f"Session conversation processing failed: {str(e)}",
                "session_id": session_id
            }
    async def process_educational_question(
        self, 
        question: str, 
        subject: str,
        student_context: Optional[Dict] = None,
        include_followups: bool = True
    ) -> Dict[str, Any]:
        """Process educational questions with advanced AI reasoning (existing method)."""
        
        try:
            system_prompt = self.prompt_service.create_enhanced_prompt(
                question=question,
                subject_string=subject,
                context=student_context
            )
            
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": question}
                ],
                temperature=0.3,
                max_tokens=1500,
                presence_penalty=0.1,
                frequency_penalty=0.1
            )
            
            raw_answer = response.choices[0].message.content
            optimized_answer = self.prompt_service.optimize_response(raw_answer, subject)
            
            follow_ups = []
            if include_followups:
                follow_ups = self.prompt_service.generate_follow_up_questions(question, subject)
            
            reasoning_steps = self._extract_reasoning_steps(optimized_answer)
            concepts = self._identify_key_concepts(optimized_answer, subject)
            
            return {
                "success": True,
                "answer": optimized_answer,
                "reasoning_steps": reasoning_steps,
                "key_concepts": concepts,
                "follow_up_questions": follow_ups,
                "subject": subject,
                "processing_details": {
                    "model_used": self.model,
                    "prompt_optimization": True,
                    "response_optimization": True,
                    "educational_enhancement": True
                }
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"Educational question processing failed: {str(e)}"
            }
    
    def _extract_reasoning_steps(self, text: str) -> List[str]:
        """Extract reasoning steps from text (existing method)."""
        steps = []

        # Look for numbered steps
        step_patterns = [
            r'\d+\.\s*([^\n]+)',
            r'Step\s*\d+[:\s]*([^\n]+)',
            r'First[,\s]*([^\n]+)',
            r'Next[,\s]*([^\n]+)',
            r'Then[,\s]*([^\n]+)',
            r'Finally[,\s]*([^\n]+)'
        ]

        for pattern in step_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            steps.extend([match.strip() for match in matches])

        return steps[:5]  # Limit to 5 steps

    def _parse_questions_from_text(self, raw_response: str) -> List[Dict]:
        """
        Parse questions from text using robust delimiter-based approach.
        Much more reliable than JSON parsing for question generation.
        """
        print(f"ðŸ” Starting robust text parsing for questions...")
        print(f"ðŸ“ Raw response length: {len(raw_response)} characters")

        questions = []

        # Try JSON parsing first as fallback
        try:
            json_data = json.loads(raw_response)
            if isinstance(json_data, list):
                print(f"âœ… Successfully parsed as direct JSON array with {len(json_data)} questions")
                return json_data
            elif isinstance(json_data, dict) and "questions" in json_data:
                print(f"âœ… Successfully parsed as JSON object with {len(json_data['questions'])} questions")
                return json_data["questions"]
        except (json.JSONDecodeError, KeyError):
            print(f"âš ï¸ JSON parsing failed, switching to text parsing...")

        # Text-based parsing with delimiters
        # Look for question patterns: "question": "...", "type": "...", etc.
        lines = raw_response.split('\n')
        current_question = {}

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # Look for field patterns with robust regex
            if '"question"' in line and ':' in line:
                # Extract question text
                question_match = re.search(r'"question":\s*"([^"]*)"', line)
                if question_match:
                    current_question['question'] = question_match.group(1)

            elif '"type"' in line and ':' in line:
                type_match = re.search(r'"type":\s*"([^"]*)"', line)
                if type_match:
                    current_question['type'] = type_match.group(1)

            elif '"correct_answer"' in line and ':' in line:
                answer_match = re.search(r'"correct_answer":\s*"([^"]*)"', line)
                if answer_match:
                    current_question['correct_answer'] = answer_match.group(1)

            elif '"explanation"' in line and ':' in line:
                explanation_match = re.search(r'"explanation":\s*"([^"]*)"', line)
                if explanation_match:
                    current_question['explanation'] = explanation_match.group(1)

            elif '"topic"' in line and ':' in line:
                topic_match = re.search(r'"topic":\s*"([^"]*)"', line)
                if topic_match:
                    current_question['topic'] = topic_match.group(1)

            elif '"difficulty"' in line and ':' in line:
                difficulty_match = re.search(r'"difficulty":\s*"([^"]*)"', line)
                if difficulty_match:
                    current_question['difficulty'] = difficulty_match.group(1)

            elif '"options"' in line and '[' in line:
                # Extract options array
                options_match = re.search(r'"options":\s*\[(.*?)\]', line)
                if options_match:
                    options_str = options_match.group(1)
                    # Parse individual options
                    options = []
                    for opt in re.findall(r'"([^"]*)"', options_str):
                        options.append(opt)
                    current_question['options'] = options

            # Check if we have a complete question
            if (len(current_question) >= 4 and
                'question' in current_question and
                'type' in current_question and
                'correct_answer' in current_question and
                'explanation' in current_question):

                # Set defaults for missing fields
                if 'topic' not in current_question:
                    current_question['topic'] = 'General'
                if 'difficulty' not in current_question:
                    current_question['difficulty'] = 'intermediate'
                if 'options' not in current_question:
                    current_question['options'] = None

                questions.append(current_question.copy())
                print(f"âœ… Parsed question {len(questions)}: {current_question['question'][:50]}...")
                current_question = {}

        print(f"ðŸŽ¯ Text parsing completed: {len(questions)} questions extracted")

        # If text parsing failed, try even more aggressive parsing
        if len(questions) == 0:
            print(f"âš ï¸ Text parsing found no questions, trying aggressive fallback...")
            questions = self._aggressive_question_extraction(raw_response)

        return questions

    def _aggressive_question_extraction(self, text: str) -> List[Dict]:
        """
        Last resort: extract questions using pattern matching and heuristics.
        """
        questions = []

        # Look for question-like patterns
        question_patterns = [
            r'(?i)what\s+is.*\?',
            r'(?i)which\s+of.*\?',
            r'(?i)how\s+.*\?',
            r'(?i)calculate.*\?',
            r'(?i)solve.*\?',
            r'(?i)find.*\?'
        ]

        for pattern in question_patterns:
            matches = re.findall(pattern, text)
            for i, match in enumerate(matches[:3]):  # Max 3 questions per pattern
                questions.append({
                    'question': match.strip(),
                    'type': 'short_answer',
                    'correct_answer': 'Please provide the correct answer',
                    'explanation': 'Generated question needs manual review',
                    'topic': 'General',
                    'difficulty': 'intermediate',
                    'options': None
                })

        print(f"ðŸ” Aggressive extraction found {len(questions)} question patterns")
        return questions[:5]  # Limit to 5 questions max
    
    def _identify_key_concepts(self, text: str, subject: str) -> List[str]:
        """Identify key concepts from text (existing method)."""
        concepts = []
        
        # Subject-specific concept patterns
        concept_patterns = {
            "mathematics": [
                r"(algebra|geometry|calculus|trigonometry|statistics)",
                r"(equation|formula|theorem|proof|solution)",
                r"(variable|constant|function|derivative|integral)"
            ],
            "physics": [
                r"(force|energy|momentum|acceleration|velocity)",
                r"(wave|frequency|amplitude|electromagnetic)",
                r"(quantum|relativity|thermodynamics|mechanics)"
            ]
        }
        
        patterns = concept_patterns.get(subject.lower(), [
            r"([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)",
            r"(important|key|fundamental|basic|advanced)"
        ])
        
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            concepts.extend([match.strip() for match in matches if isinstance(match, str)])
        
        return list(set(concepts))[:5]  # Remove duplicates and limit to 5

    async def analyze_image_with_chat_context(
        self,
        base64_image: str,
        user_prompt: str,
        subject: Optional[str] = "general",
        session_id: Optional[str] = None,
        student_context: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """
        Analyze image with chat context for conversational responses.
        Optimized for fast, natural language responses suitable for chat interfaces.
        
        Args:
            base64_image: Base64 encoded image data
            user_prompt: User's question or prompt about the image
            subject: Subject context (math, science, etc.)
            session_id: Optional session ID for context awareness
            student_context: Optional student learning context
            
        Returns:
            Dict with response, tokens_used, and success status
        """
        
        try:
            print(f"ðŸ”„ === AI SERVICE: CHAT IMAGE ANALYSIS START ===")
            print(f"ðŸ“ User Prompt: {user_prompt}")
            print(f"ðŸ“š Subject: {subject}")
            print(f"ðŸ†” Session: {session_id}")
            print(f"ðŸ“„ Image length: {len(base64_image)} chars")
            
            # Check if OpenAI API key is available
            api_key = os.getenv('OPENAI_API_KEY')
            if not api_key:
                raise Exception("OpenAI API key not configured")
            print(f"âœ… OpenAI API key verified: {api_key[:10]}..." if len(api_key) > 10 else "âœ… API key verified")
            
            # Verify client is available
            if not self.client:
                raise Exception("OpenAI client not initialized")
            print(f"âœ… OpenAI client available: {type(self.client)}")
            
            # Create conversational prompt optimized for chat
            print("ðŸ”„ Creating chat image prompt...")
            system_prompt = self._create_chat_image_prompt(user_prompt, subject, student_context)
            print(f"âœ… System prompt created: {len(system_prompt)} chars")
            
            # Prepare image for OpenAI Vision API
            print("ðŸ”„ Preparing image for OpenAI Vision API...")
            image_url = f"data:image/jpeg;base64,{base64_image}"
            
            messages = [
                {
                    "role": "system",
                    "content": system_prompt
                },
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": user_prompt
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": image_url,
                                "detail": "high"
                            }
                        }
                    ]
                }
            ]
            print(f"âœ… Messages prepared: {len(messages)} messages")
            print(f"ðŸ“Š System prompt length: {len(system_prompt)} chars")
            print(f"ðŸ“Š User prompt length: {len(user_prompt)} chars")
            print(f"ðŸ“Š Image URL length: {len(image_url)} chars")
            
            print(f"ðŸ“¡ Calling OpenAI Vision API...")
            print(f"ðŸ¤– Model: {self.vision_model}")
            print(f"âš™ï¸ Settings: max_tokens=800, temperature=0.7")
            
            # Use vision model with optimized settings for chat
            response = await self.client.chat.completions.create(
                model=self.vision_model,  # Use full vision model for image analysis
                messages=messages,
                max_tokens=800,  # Shorter responses for chat
                temperature=0.7,  # Slightly more conversational
                stream=False
            )
            
            print(f"âœ… OpenAI API call completed successfully")
            print(f"ðŸ“Š Response type: {type(response)}")
            
            ai_response = response.choices[0].message.content.strip()
            tokens_used = response.usage.total_tokens if response.usage else 0
            
            print(f"âœ… === AI SERVICE: CHAT IMAGE ANALYSIS SUCCESS ===")
            print(f"ðŸ“ Response length: {len(ai_response)} chars")
            print(f"ðŸŽ¯ Tokens used: {tokens_used}")
            
            return {
                "success": True,
                "response": ai_response,
                "tokens_used": tokens_used,
                "processing_type": "chat_image",
                "model_used": self.vision_model
            }
            
        except Exception as e:
            # Comprehensive error logging
            import traceback
            error_msg = f"Chat image analysis failed: {str(e) if str(e) else 'Unknown error'}"
            full_traceback = traceback.format_exc()
            
            print(f"âŒ === AI SERVICE: CHAT IMAGE ANALYSIS ERROR ===")
            print(f"ðŸ’¥ Error message: '{str(e)}'")
            print(f"ðŸ’¥ Error repr: {repr(e)}")
            print(f"ðŸ”§ Exception type: {type(e).__name__}")
            print(f"ðŸ”§ Exception module: {type(e).__module__}")
            print(f"ðŸ”§ Exception args: {e.args}")
            print(f"ðŸ“‹ Full traceback:")
            print(full_traceback)
            print(f"ðŸ” OpenAI API key available: {bool(os.getenv('OPENAI_API_KEY'))}")
            print(f"ðŸ” Client type: {type(self.client)}")
            print(f"=====================================")
            
            # Return more detailed error for debugging
            detailed_error = f"{error_msg} | Type: {type(e).__name__} | Args: {e.args} | Traceback: {full_traceback[:500]}"
            
            return {
                "success": False,
                "error": detailed_error,
                "response": "I'm having trouble analyzing this image right now. Please try again in a moment.",
                "tokens_used": 0
            }
    
    def _create_chat_image_prompt(
        self,
        user_prompt: str,
        subject: str,
        student_context: Optional[Dict] = None
    ) -> str:
        """
        Create an optimized prompt for chat image analysis.
        Focuses on conversational, helpful responses.
        """
        
        base_prompt = f"""You are StudyAI, a helpful educational assistant specializing in {subject}. 

The user has sent you an image with this question: "{user_prompt}"

Please analyze the image and provide a clear, conversational response that:
1. Directly addresses their question
2. Explains what you see in the image relevant to their question
3. Provides educational value appropriate for their level
4. Uses a natural, friendly tone suitable for chat
5. Keeps the response concise but informative (aim for 2-4 sentences)

If the image contains:
- Math problems: Show the solution steps clearly
- Diagrams/charts: Explain what they represent
- Text: Help them understand the content
- Homework: Provide guidance without just giving answers

Focus on being helpful and educational while maintaining a conversational tone."""

        # Add student context if available
        if student_context and student_context.get("student_id"):
            base_prompt += f"\n\nStudent context: {student_context.get('student_id', 'anonymous')}"

        return base_prompt

    # MARK: - Question Generation Methods

    async def generate_random_questions(
        self,
        subject: str,
        config: Dict[str, Any],
        user_profile: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Generate random practice questions for a given subject.

        Args:
            subject: Subject area (e.g., 'mathematics', 'physics')
            config: Configuration with topics, focus_notes, difficulty, question_count
            user_profile: User details like grade, location, preferences

        Returns:
            Dict with generated questions in JSON format
        """

        try:
            print(f"ðŸŽ¯ === AI SERVICE: RANDOM QUESTIONS GENERATION START ===")
            print(f"ðŸ“š Subject: {subject}")
            print(f"âš™ï¸  Config: {config}")
            print(f"ðŸ‘¤ User Profile: {user_profile}")

            # Generate the comprehensive prompt
            system_prompt = self.prompt_service.get_random_questions_prompt(subject, config, user_profile)

            print(f"ðŸ“ === FULL INPUT PROMPT FOR RANDOM QUESTIONS ===")
            print(f"ðŸ“„ Prompt Length: {len(system_prompt)} characters")
            print(f"ðŸ“‹ Full Prompt Content:")
            print("=" * 80)
            print(system_prompt)
            print("=" * 80)

            # Call OpenAI with JSON response format enforcement
            print(f"ðŸ“¡ Calling OpenAI API for question generation...")
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"Generate {config.get('question_count', 5)} random questions for {subject} now."}
                ],
                temperature=0.7,  # Higher temperature for variety in questions
                max_tokens=3000,
                # Remove JSON format requirement for more reliable parsing
            )

            raw_response = response.choices[0].message.content
            tokens_used = response.usage.total_tokens if response.usage else 0

            print(f"âœ… OpenAI API call completed")
            print(f"ðŸ“Š Tokens used: {tokens_used}")
            print(f"ðŸ“ Raw response length: {len(raw_response)} characters")
            print(f"ðŸ“‹ Raw Response Preview: {raw_response[:200]}...")

            # Use robust text parsing instead of fragile JSON parsing
            try:
                questions_json = self._parse_questions_from_text(raw_response)

                # Validate that we have questions
                if not questions_json or len(questions_json) == 0:
                    raise ValueError("No valid questions could be extracted from response")

                # Validate each question has required fields
                required_fields = ["question", "type", "correct_answer", "explanation", "topic"]
                for i, question in enumerate(questions_json):
                    for field in required_fields:
                        if field not in question:
                            raise ValueError(f"Question {i+1} missing required field: {field}")

                print(f"âœ… === AI SERVICE: RANDOM QUESTIONS GENERATION SUCCESS ===")
                print(f"ðŸŽ¯ Generated {len(questions_json)} questions")

                return {
                    "success": True,
                    "questions": questions_json,
                    "generation_type": "random",
                    "subject": subject,
                    "tokens_used": tokens_used,
                    "question_count": len(questions_json),
                    "config_used": config,
                    "processing_details": {
                        "model_used": self.model,
                        "prompt_optimization": True,
                        "json_format": True,
                        "validation_passed": True
                    }
                }

            except (ValueError, Exception) as parse_error:
                print(f"âš ï¸ Question parsing failed: {parse_error}")
                print(f"ðŸ“„ Raw response: {raw_response}")

                return {
                    "success": False,
                    "error": f"Question generation parsing failed: {parse_error}",
                    "raw_response": raw_response,
                    "generation_type": "random",
                    "subject": subject
                }

        except Exception as e:
            print(f"âŒ === AI SERVICE: RANDOM QUESTIONS GENERATION ERROR ===")
            print(f"ðŸ’¥ Error: {str(e)}")
            import traceback
            print(f"ðŸ“‹ Full traceback: {traceback.format_exc()}")

            return {
                "success": False,
                "error": f"Random question generation failed: {str(e)}",
                "generation_type": "random",
                "subject": subject
            }

    async def generate_mistake_based_questions(
        self,
        subject: str,
        mistakes_data: List[Dict],
        config: Dict[str, Any],
        user_profile: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Generate questions based on previous mistakes to help remedial learning.

        Args:
            subject: Subject area
            mistakes_data: List of previous mistakes with context
            config: Configuration including question_count
            user_profile: User details

        Returns:
            Dict with generated remedial questions in JSON format
        """

        try:
            print(f"ðŸŽ¯ === AI SERVICE: MISTAKE-BASED QUESTIONS GENERATION START ===")
            print(f"ðŸ“š Subject: {subject}")
            print(f"âŒ Mistakes Count: {len(mistakes_data)}")
            print(f"âš™ï¸  Config: {config}")
            print(f"ðŸ‘¤ User Profile: {user_profile}")

            # Generate the comprehensive prompt
            system_prompt = self.prompt_service.get_mistake_based_questions_prompt(
                subject, mistakes_data, config, user_profile
            )

            print(f"ðŸ“ === FULL INPUT PROMPT FOR MISTAKE-BASED QUESTIONS ===")
            print(f"ðŸ“„ Prompt Length: {len(system_prompt)} characters")
            print(f"ðŸ“‹ Full Prompt Content:")
            print("=" * 80)
            print(system_prompt)
            print("=" * 80)

            # Call OpenAI with JSON response format enforcement
            print(f"ðŸ“¡ Calling OpenAI API for mistake-based question generation...")
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"Generate {config.get('question_count', 5)} remedial questions based on the mistake patterns for {subject}."}
                ],
                temperature=0.6,  # Moderate temperature for focused remedial questions
                max_tokens=3000,
                # Remove JSON format requirement for more reliable parsing
            )

            raw_response = response.choices[0].message.content
            tokens_used = response.usage.total_tokens if response.usage else 0

            print(f"âœ… OpenAI API call completed")
            print(f"ðŸ“Š Tokens used: {tokens_used}")
            print(f"ðŸ“ Raw response length: {len(raw_response)} characters")
            print(f"ðŸ“‹ Raw Response Preview: {raw_response[:200]}...")

            # Use robust text parsing instead of fragile JSON parsing
            try:
                questions_json = self._parse_questions_from_text(raw_response)

                # Validate that we have questions
                if not questions_json or len(questions_json) == 0:
                    raise ValueError("No valid questions could be extracted from response")

                # Validate each question has required fields
                required_fields = ["question", "type", "correct_answer", "explanation", "topic"]
                for i, question in enumerate(questions_json):
                    for field in required_fields:
                        if field not in question:
                            raise ValueError(f"Question {i+1} missing required field: {field}")

                print(f"âœ… === AI SERVICE: MISTAKE-BASED QUESTIONS GENERATION SUCCESS ===")
                print(f"ðŸŽ¯ Generated {len(questions_json)} remedial questions")

                return {
                    "success": True,
                    "questions": questions_json,
                    "generation_type": "mistake_based",
                    "subject": subject,
                    "tokens_used": tokens_used,
                    "question_count": len(questions_json),
                    "mistakes_analyzed": len(mistakes_data),
                    "config_used": config,
                    "processing_details": {
                        "model_used": self.model,
                        "prompt_optimization": True,
                        "json_format": True,
                        "validation_passed": True,
                        "remedial_focus": True
                    }
                }

            except (json.JSONDecodeError, ValueError) as parse_error:
                print(f"âš ï¸ JSON parsing failed: {parse_error}")
                print(f"ðŸ“„ Raw response: {raw_response}")

                return {
                    "success": False,
                    "error": f"Mistake-based question generation parsing failed: {parse_error}",
                    "raw_response": raw_response,
                    "generation_type": "mistake_based",
                    "subject": subject
                }

        except Exception as e:
            print(f"âŒ === AI SERVICE: MISTAKE-BASED QUESTIONS GENERATION ERROR ===")
            print(f"ðŸ’¥ Error: {str(e)}")
            import traceback
            print(f"ðŸ“‹ Full traceback: {traceback.format_exc()}")

            return {
                "success": False,
                "error": f"Mistake-based question generation failed: {str(e)}",
                "generation_type": "mistake_based",
                "subject": subject
            }

    async def generate_conversation_based_questions(
        self,
        subject: str,
        conversation_data: List[Dict],
        config: Dict[str, Any],
        user_profile: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Generate personalized questions based on previous conversations.

        Args:
            subject: Subject area
            conversation_data: List of conversation summaries and contexts
            config: Configuration including question_count
            user_profile: User details

        Returns:
            Dict with generated personalized questions in JSON format
        """

        try:
            print(f"ðŸŽ¯ === AI SERVICE: CONVERSATION-BASED QUESTIONS GENERATION START ===")
            print(f"ðŸ“š Subject: {subject}")
            print(f"ðŸ’¬ Conversations Count: {len(conversation_data)}")
            print(f"âš™ï¸  Config: {config}")
            print(f"ðŸ‘¤ User Profile: {user_profile}")

            # Generate the comprehensive prompt
            system_prompt = self.prompt_service.get_conversation_based_questions_prompt(
                subject, conversation_data, config, user_profile
            )

            print(f"ðŸ“ === FULL INPUT PROMPT FOR CONVERSATION-BASED QUESTIONS ===")
            print(f"ðŸ“„ Prompt Length: {len(system_prompt)} characters")
            print(f"ðŸ“‹ Full Prompt Content:")
            print("=" * 80)
            print(system_prompt)
            print("=" * 80)

            # Call OpenAI with JSON response format enforcement
            print(f"ðŸ“¡ Calling OpenAI API for conversation-based question generation...")
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"Generate {config.get('question_count', 5)} personalized questions based on the conversation history for {subject}."}
                ],
                temperature=0.8,  # Higher temperature for more personalized, creative questions
                max_tokens=3000,
                # Remove JSON format requirement for more reliable parsing
            )

            raw_response = response.choices[0].message.content
            tokens_used = response.usage.total_tokens if response.usage else 0

            print(f"âœ… OpenAI API call completed")
            print(f"ðŸ“Š Tokens used: {tokens_used}")
            print(f"ðŸ“ Raw response length: {len(raw_response)} characters")
            print(f"ðŸ“‹ Raw Response Preview: {raw_response[:200]}...")

            # Use robust text parsing instead of fragile JSON parsing
            try:
                questions_json = self._parse_questions_from_text(raw_response)

                # Validate that we have questions
                if not questions_json or len(questions_json) == 0:
                    raise ValueError("No valid questions could be extracted from response")

                # Validate each question has required fields
                required_fields = ["question", "type", "correct_answer", "explanation", "topic"]
                for i, question in enumerate(questions_json):
                    for field in required_fields:
                        if field not in question:
                            raise ValueError(f"Question {i+1} missing required field: {field}")

                print(f"âœ… === AI SERVICE: CONVERSATION-BASED QUESTIONS GENERATION SUCCESS ===")
                print(f"ðŸŽ¯ Generated {len(questions_json)} personalized questions")

                return {
                    "success": True,
                    "questions": questions_json,
                    "generation_type": "conversation_based",
                    "subject": subject,
                    "tokens_used": tokens_used,
                    "question_count": len(questions_json),
                    "conversations_analyzed": len(conversation_data),
                    "config_used": config,
                    "processing_details": {
                        "model_used": self.model,
                        "prompt_optimization": True,
                        "json_format": True,
                        "validation_passed": True,
                        "personalization": True
                    }
                }

            except (json.JSONDecodeError, ValueError) as parse_error:
                print(f"âš ï¸ JSON parsing failed: {parse_error}")
                print(f"ðŸ“„ Raw response: {raw_response}")

                return {
                    "success": False,
                    "error": f"Conversation-based question generation parsing failed: {parse_error}",
                    "raw_response": raw_response,
                    "generation_type": "conversation_based",
                    "subject": subject
                }

        except Exception as e:
            print(f"âŒ === AI SERVICE: CONVERSATION-BASED QUESTIONS GENERATION ERROR ===")
            print(f"ðŸ’¥ Error: {str(e)}")
            import traceback
            print(f"ðŸ“‹ Full traceback: {traceback.format_exc()}")

            return {
                "success": False,
                "error": f"Conversation-based question generation failed: {str(e)}",
                "generation_type": "conversation_based",
                "subject": subject
            }
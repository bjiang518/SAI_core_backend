/**\n * Performance Tests for API Gateway\n * Tests to ensure gateway doesn't add significant overhead\n */\n\nconst { test } = require('tap');\nconst { build } = require('./helper');\nconst nock = require('nock');\n\n// Performance thresholds\nconst PERFORMANCE_THRESHOLDS = {\n  maxGatewayOverhead: 50, // ms\n  maxResponseTime: 200,   // ms\n  minThroughput: 100      // requests/second\n};\n\ntest('Gateway Performance Tests', async (t) => {\n  const app = await build(t);\n\n  t.test('response time overhead is minimal', async (t) => {\n    const mockResponse = { success: true, data: 'test' };\n    \n    // Mock AI Engine with controlled delay\n    const aiEngineDelay = 100; // ms\n    const scope = nock(process.env.AI_ENGINE_URL)\n      .post('/api/v1/process-question')\n      .delay(aiEngineDelay)\n      .reply(200, mockResponse);\n\n    const startTime = Date.now();\n    \n    const res = await app.inject({\n      method: 'POST',\n      url: '/api/ai/process-question',\n      payload: {\n        question: 'Test question',\n        subject: 'test'\n      }\n    });\n    \n    const totalTime = Date.now() - startTime;\n    const gatewayOverhead = totalTime - aiEngineDelay;\n    \n    t.equal(res.statusCode, 200);\n    t.ok(gatewayOverhead < PERFORMANCE_THRESHOLDS.maxGatewayOverhead, \n         `Gateway overhead ${gatewayOverhead}ms should be < ${PERFORMANCE_THRESHOLDS.maxGatewayOverhead}ms`);\n    \n    scope.done();\n  });\n\n  t.test('health check is fast', async (t) => {\n    const startTime = Date.now();\n    \n    const res = await app.inject({\n      method: 'GET',\n      url: '/health'\n    });\n    \n    const responseTime = Date.now() - startTime;\n    \n    t.equal(res.statusCode, 200);\n    t.ok(responseTime < 10, `Health check took ${responseTime}ms, should be < 10ms`);\n  });\n\n  t.test('concurrent requests handling', async (t) => {\n    const concurrentRequests = 10;\n    const mockResponse = { success: true, id: 'test' };\n    \n    // Mock multiple responses\n    const scope = nock(process.env.AI_ENGINE_URL)\n      .post('/api/v1/process-question')\n      .times(concurrentRequests)\n      .reply(200, mockResponse);\n\n    const requests = Array(concurrentRequests).fill().map((_, i) => \n      app.inject({\n        method: 'POST',\n        url: '/api/ai/process-question',\n        payload: {\n          question: `Test question ${i}`,\n          subject: 'test'\n        }\n      })\n    );\n\n    const startTime = Date.now();\n    const responses = await Promise.all(requests);\n    const totalTime = Date.now() - startTime;\n\n    // All requests should succeed\n    responses.forEach((res, i) => {\n      t.equal(res.statusCode, 200, `Request ${i} should succeed`);\n    });\n\n    // Should handle concurrent requests efficiently\n    const avgResponseTime = totalTime / concurrentRequests;\n    t.ok(avgResponseTime < PERFORMANCE_THRESHOLDS.maxResponseTime,\n         `Average response time ${avgResponseTime}ms should be < ${PERFORMANCE_THRESHOLDS.maxResponseTime}ms`);\n    \n    scope.done();\n  });\n});\n\ntest('Memory Usage Tests', async (t) => {\n  const app = await build(t);\n\n  t.test('memory usage stays stable under load', async (t) => {\n    const initialMemory = process.memoryUsage().heapUsed;\n    \n    // Mock responses for load test\n    const scope = nock(process.env.AI_ENGINE_URL)\n      .post('/api/v1/process-question')\n      .times(50)\n      .reply(200, { success: true });\n\n    // Generate load\n    const requests = Array(50).fill().map((_, i) => \n      app.inject({\n        method: 'POST',\n        url: '/api/ai/process-question',\n        payload: {\n          question: `Load test question ${i}`,\n          subject: 'test'\n        }\n      })\n    );\n\n    await Promise.all(requests);\n    \n    // Force garbage collection if available\n    if (global.gc) {\n      global.gc();\n    }\n    \n    const finalMemory = process.memoryUsage().heapUsed;\n    const memoryIncrease = finalMemory - initialMemory;\n    const memoryIncreaseMB = memoryIncrease / 1024 / 1024;\n    \n    // Memory increase should be reasonable (< 10MB for this test)\n    t.ok(memoryIncreaseMB < 10, \n         `Memory increase ${memoryIncreaseMB.toFixed(2)}MB should be < 10MB`);\n    \n    scope.done();\n  });\n});\n\ntest('Error Handling Performance', async (t) => {\n  const app = await build(t);\n\n  t.test('error responses are fast', async (t) => {\n    const scope = nock(process.env.AI_ENGINE_URL)\n      .post('/api/v1/process-question')\n      .reply(500, { error: 'Internal server error' });\n\n    const startTime = Date.now();\n    \n    const res = await app.inject({\n      method: 'POST',\n      url: '/api/ai/process-question',\n      payload: {\n        question: 'Test question',\n        subject: 'test'\n      }\n    });\n    \n    const responseTime = Date.now() - startTime;\n    \n    t.equal(res.statusCode, 500);\n    t.ok(responseTime < 100, `Error response took ${responseTime}ms, should be < 100ms`);\n    \n    scope.done();\n  });\n\n  t.test('connection errors are handled quickly', async (t) => {\n    const scope = nock(process.env.AI_ENGINE_URL)\n      .post('/api/v1/process-question')\n      .replyWithError('ECONNREFUSED');\n\n    const startTime = Date.now();\n    \n    const res = await app.inject({\n      method: 'POST',\n      url: '/api/ai/process-question',\n      payload: {\n        question: 'Test question',\n        subject: 'test'\n      }\n    });\n    \n    const responseTime = Date.now() - startTime;\n    \n    t.equal(res.statusCode, 503);\n    t.ok(responseTime < 1000, `Connection error response took ${responseTime}ms, should be < 1000ms`);\n    \n    scope.done();\n  });\n});
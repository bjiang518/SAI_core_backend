/**
 * Optimized Railway PostgreSQL Database Configuration
 * High-performance connection management with advanced caching
 */

const { Pool } = require('pg');
const crypto = require('crypto');
const NodeCache = require('node-cache');
const { promisify } = require('util');
const InputValidation = require('./input-validation');  // SECURITY: Input validation
const encryptionService = require('./encryption-service');  // PRIVACY: Encryption at rest
const logger = require('./logger');  // PRODUCTION: Structured logging with environment-aware levels

// PHASE 1 OPTIMIZATION: Enhanced connection pool configuration
// Optimized for Railway's PostgreSQL limits (20 connections max)
const pool = new Pool({
  connectionString: process.env.DATABASE_URL,
  ssl: process.env.NODE_ENV === 'production' ? { rejectUnauthorized: false } : false,

  // OPTIMIZED: Connection limits for Railway
  max: 20,  // Railway max connections (reduced from 30 for safety)
  min: 2,   // Keep 2 warm connections (reduced from 5 to save resources)

  // OPTIMIZED: Timeout configuration
  idleTimeoutMillis: 30000, // Close idle connections after 30s (was 60s)
  connectionTimeoutMillis: 2000, // Fail fast if no connection (was 5s)

  // OPTIMIZED: Query timeouts
  statement_timeout: 10000,  // 10s statement timeout (was 30s)
  query_timeout: 10000,      // 10s query timeout (was 30s)

  // Metadata
  application_name: 'StudyAI_Backend',

  // PHASE 1: Prevent connection leaks
  allowExitOnIdle: false  // Don't exit on idle connections
});

// Multi-level caching system
const queryCache = new NodeCache({ 
  stdTTL: 600, // 10 minutes default TTL
  checkperiod: 120, // Check for expired keys every 2 minutes
  maxKeys: 10000 // Maximum cached items
});

const sessionCache = new NodeCache({ 
  stdTTL: 1800, // 30 minutes for sessions
  checkperiod: 300 // Check every 5 minutes
});

const userCache = new NodeCache({ 
  stdTTL: 3600, // 1 hour for user data
  checkperiod: 600 // Check every 10 minutes
});

// Performance monitoring and metrics
let queryMetrics = {
  totalQueries: 0,
  cacheHits: 0,
  cacheMisses: 0,
  averageQueryTime: 0,
  slowQueries: [],
  // PHASE 1: Pool health tracking
  poolHealthChecks: 0,
  connectionTimeouts: 0,
  poolExhaustion: 0
};

// PHASE 1 OPTIMIZATION: Improved connection monitoring (less verbose)
pool.on('connect', (client) => {
  logger.debug('‚úÖ PostgreSQL client connected - Pool: total=' + pool.totalCount + ', idle=' + pool.idleCount);
});

pool.on('acquire', (client) => {
  // OPTIMIZED: Only log if pool is getting full (potential bottleneck)
  const activeConnections = pool.totalCount - pool.idleCount;
  if (activeConnections > 15) {  // Alert when > 75% of pool is active
    logger.warn(`‚ö†Ô∏è High pool usage: ${activeConnections}/20 active, ${pool.waitingCount} waiting`);
  }

  // Track pool exhaustion for metrics
  if (pool.waitingCount > 0) {
    queryMetrics.poolExhaustion++;
  }
});

pool.on('error', (err, client) => {
  logger.error('‚ùå Unexpected PostgreSQL error:', err.message);
  // Log connection timeouts separately
  if (err.message && err.message.includes('timeout')) {
    queryMetrics.connectionTimeouts++;
    logger.error('‚ö†Ô∏è Connection timeout count:', queryMetrics.connectionTimeouts);
  }
  // Don't exit - let the pool handle reconnection
});

// Query result preparation helper
function generateCacheKey(text, params) {
  const combined = text + (params ? JSON.stringify(params) : '');
  return crypto.createHash('sha256').update(combined).digest('hex').substring(0, 16);
}

// Batch query processor for bulk operations
class BatchProcessor {
  constructor() {
    this.batches = new Map();
    this.batchSize = 100;
    this.flushInterval = 1000; // 1 second
    
    // Auto-flush batches periodically
    setInterval(() => this.flushAllBatches(), this.flushInterval);
  }
  
  addToBatch(operation, query, params) {
    if (!this.batches.has(operation)) {
      this.batches.set(operation, []);
    }
    
    this.batches.get(operation).push({ query, params, timestamp: Date.now() });
    
    // Auto-flush if batch is full
    if (this.batches.get(operation).length >= this.batchSize) {
      this.flushBatch(operation);
    }
  }
  
  async flushBatch(operation) {
    const batch = this.batches.get(operation);
    if (!batch || batch.length === 0) return;
    
    this.batches.set(operation, []); // Clear batch
    
    try {
      await db.transaction(async (client) => {
        for (const item of batch) {
          await client.query(item.query, item.params);
        }
      });
      
      logger.debug(`üì¶ Flushed batch of ${batch.length} ${operation} operations`);
    } catch (error) {
      logger.error(`‚ùå Batch flush error for ${operation}:`, error);
    }
  }
  
  async flushAllBatches() {
    for (const operation of this.batches.keys()) {
      await this.flushBatch(operation);
    }
  }
}

const batchProcessor = new BatchProcessor();

// Enhanced database utility functions with caching and optimization
const db = {
  /**
   * Execute a cached query with performance monitoring, retry logic, and connection safety
   */
  async query(text, params = [], options = {}) {
    const start = Date.now();
    const cacheKey = options.cache !== false ? generateCacheKey(text, params) : null;
    const maxRetries = options.maxRetries || 2;  // NEW: Configurable retries
    const retryDelay = options.retryDelay || 100; // NEW: Delay between retries (ms)

    // Check cache first (for SELECT queries)
    if (cacheKey && text.trim().toLowerCase().startsWith('select')) {
      const cached = queryCache.get(cacheKey);
      if (cached) {
        queryMetrics.cacheHits++;
        const duration = Date.now() - start;
        logger.debug(`‚ö° Cache hit in ${duration}ms: ${text.substring(0, 50)}...`);
        return cached;
      }
      queryMetrics.cacheMisses++;
    }

    // NEW: Retry logic for transient errors
    let lastError;
    for (let attempt = 0; attempt <= maxRetries; attempt++) {
      try {
        queryMetrics.totalQueries++;
        const result = await pool.query(text, params);
        const duration = Date.now() - start;

        // Update performance metrics
        queryMetrics.averageQueryTime =
          (queryMetrics.averageQueryTime * (queryMetrics.totalQueries - 1) + duration) / queryMetrics.totalQueries;

        // OPTIMIZED: Track slow queries with more detail
        if (duration > 500) {  // Changed from 1000ms to 500ms for earlier detection
          const slowQuery = {
            query: text.substring(0, 150),
            params: params?.length > 0 ? JSON.stringify(params).substring(0, 100) : 'none',
            duration,
            timestamp: new Date().toISOString()
          };

          queryMetrics.slowQueries.push(slowQuery);

          // Log slow queries immediately for monitoring
          logger.warn(`‚ö†Ô∏è SLOW QUERY (${duration}ms): ${slowQuery.query}...`);
          if (params?.length > 0) {
            logger.warn(`   Parameters: ${slowQuery.params}`);
          }

          // Keep only last 100 slow queries
          if (queryMetrics.slowQueries.length > 100) {
            queryMetrics.slowQueries = queryMetrics.slowQueries.slice(-100);
          }
        }

        // OPTIMIZED: Log all queries in development for debugging
        if (process.env.NODE_ENV !== 'production') {
          logger.debug(`üìä Query executed in ${duration}ms: ${text.substring(0, 80)}...`);
        } else if (duration > 200) {
          // In production, only log queries taking >200ms
          logger.debug(`üìä Query executed in ${duration}ms: ${text.substring(0, 80)}...`);
        }

        // Cache SELECT results
        if (cacheKey && text.trim().toLowerCase().startsWith('select') && result.rows.length > 0) {
          const ttl = options.cacheTTL || 600; // 10 minutes default
          queryCache.set(cacheKey, result, ttl);
        }

        return result;
      } catch (error) {
        lastError = error;

        // NEW: Check if error is retryable
        const isRetryable = this.isRetryableError(error);
        const shouldRetry = attempt < maxRetries && isRetryable;

        if (shouldRetry) {
          logger.warn(`‚ö†Ô∏è Database query failed (attempt ${attempt + 1}/${maxRetries + 1}), retrying in ${retryDelay}ms...`);
          logger.warn(`   Error: ${error.message}`);
          logger.warn(`   Query: ${text.substring(0, 100)}...`);

          // Wait before retrying with exponential backoff
          await new Promise(resolve => setTimeout(resolve, retryDelay * Math.pow(2, attempt)));
          continue;
        }

        // Not retryable or max retries exceeded
        logger.error('‚ùå Database query error:', error);
        logger.error('Query:', text.substring(0, 200));
        logger.error('Params:', params);
        throw error;
      }
    }

    // Should never reach here, but TypeScript needs it
    throw lastError;
  },

  /**
   * NEW: Check if database error is retryable
   * Retries connection errors, timeouts, and deadlocks
   */
  isRetryableError(error) {
    if (!error) return false;

    // Connection-related errors (retryable)
    if (error.code === 'ECONNREFUSED' || error.code === 'ECONNRESET') return true;
    if (error.code === 'ETIMEDOUT' || error.message?.includes('timeout')) return true;
    if (error.message?.includes('Connection terminated') ||
        error.message?.includes('Connection lost')) return true;

    // PostgreSQL specific errors (retryable)
    if (error.code === '40P01') return true; // Deadlock detected
    if (error.code === '08000' || error.code === '08006') return true; // Connection errors
    if (error.code === '53300') return true; // Too many connections
    if (error.code === '57P01') return true; // Admin shutdown

    // Not retryable (data errors, constraint violations, etc.)
    return false;
  },

  /**
   * Get a client from the pool for transactions
   */
  async getClient() {
    return await pool.connect();
  },

  /**
   * Execute multiple queries in a transaction
   */
  async transaction(callback) {
    const client = await pool.connect();
    try {
      await client.query('BEGIN');
      const result = await callback(client);
      await client.query('COMMIT');
      return result;
    } catch (error) {
      await client.query('ROLLBACK');
      throw error;
    } finally {
      client.release();
    }
  },

  /**
   * User Management Functions
   */

  /**
   * Create or update user (for Google/Apple OAuth)
   */
  async createOrUpdateUser(userData) {
    const {
      email,
      name,
      profileImageUrl,
      authProvider,
      googleId,
      appleId
    } = userData;

    const query = `
      INSERT INTO users (
        email, 
        name, 
        profile_image_url, 
        auth_provider, 
        google_id, 
        apple_id,
        email_verified,
        last_login_at
      ) VALUES ($1, $2, $3, $4, $5, $6, $7, NOW())
      ON CONFLICT (email) 
      DO UPDATE SET 
        name = EXCLUDED.name,
        profile_image_url = EXCLUDED.profile_image_url,
        google_id = COALESCE(EXCLUDED.google_id, users.google_id),
        apple_id = COALESCE(EXCLUDED.apple_id, users.apple_id),
        last_login_at = NOW(),
        updated_at = NOW()
      RETURNING *
    `;

    const values = [
      email,
      name,
      profileImageUrl,
      authProvider,
      googleId,
      appleId,
      true // email_verified for OAuth providers
    ];

    const result = await this.query(query, values);
    return result.rows[0];
  },

  /**
   * Create user session token
   */
  async createUserSession(userId, deviceInfo = null, ipAddress = null) {
    // Generate secure token
    const token = crypto.randomBytes(32).toString('hex');
    const tokenHash = crypto.createHash('sha256').update(token).digest('hex');
    
    // Token expires in 30 days
    const expiresAt = new Date(Date.now() + 30 * 24 * 60 * 60 * 1000);

    const query = `
      INSERT INTO user_sessions (
        user_id, 
        token_hash, 
        expires_at, 
        device_info, 
        ip_address
      ) VALUES ($1, $2, $3, $4, $5)
      RETURNING id
    `;

    const values = [
      userId,
      tokenHash,
      expiresAt,
      deviceInfo ? JSON.stringify(deviceInfo) : null,
      ipAddress
    ];

    const result = await this.query(query, values);
    return {
      sessionId: result.rows[0].id,
      token: token,
      expiresAt: expiresAt
    };
  },

  /**
   * Verify user session token
   */
  async verifyUserSession(token) {
    const startTime = Date.now();
    try {
      const tokenHash = crypto.createHash('sha256').update(token).digest('hex');
      
      const query = `
        SELECT 
          us.id as session_id,
          us.user_id,
          us.expires_at,
          u.email,
          u.name,
          u.profile_image_url,
          u.auth_provider
        FROM user_sessions us
        JOIN users u ON us.user_id = u.id
        WHERE us.token_hash = $1 
          AND us.expires_at > NOW()
          AND u.is_active = true
      `;

      logger.debug(`üîç Starting token verification for hash: ${tokenHash.substring(0, 8)}...`);
      const result = await this.query(query, [tokenHash]);
      const duration = Date.now() - startTime;
      
      if (result.rows.length > 0) {
        logger.debug(`‚úÖ Token verification successful in ${duration}ms for user: ${result.rows[0].user_id}`);
        return result.rows[0];
      } else {
        logger.debug(`‚ùå Token verification failed in ${duration}ms - no matching session found`);
        return null;
      }
    } catch (error) {
      const duration = Date.now() - startTime;
      logger.error(`‚ùå Token verification error after ${duration}ms:`, error);
      throw error;
    }
  },

  /**
   * Get user by email
   */
  async getUserByEmail(email) {
    const query = `
      SELECT * FROM users 
      WHERE email = $1 AND is_active = true
    `;
    
    const result = await this.query(query, [email]);
    return result.rows[0];
  },

  /**
   * Create new user (for email/password registration)
   */
  async createUser(userData) {
    const {
      email,
      name,
      password,
      authProvider = 'email',
      emailVerified = false  // Default to false, but allow override for verified registrations
    } = userData;

    // Hash the password using bcryptjs
    const bcrypt = require('bcryptjs');
    const saltRounds = 12;
    const passwordHash = await bcrypt.hash(password, saltRounds);

    const query = `
      INSERT INTO users (
        email,
        name,
        password_hash,
        auth_provider,
        email_verified,
        last_login_at
      ) VALUES ($1, $2, $3, $4, $5, NOW())
      RETURNING *
    `;

    const values = [
      email,
      name,
      passwordHash,
      authProvider,
      emailVerified  // Use the provided value (true for email verification flow)
    ];

    const result = await this.query(query, values);
    return result.rows[0];
  },

  /**
   * Verify user credentials (for email/password login)
   */
  async verifyUserCredentials(email, password) {
    const user = await this.getUserByEmail(email);
    if (!user || !user.password_hash) {
      return null; // User not found or no password set (OAuth user)
    }

    // Verify password using bcryptjs
    const bcrypt = require('bcryptjs');
    const isValidPassword = await bcrypt.compare(password, user.password_hash);
    
    if (isValidPassword) {
      // Update last login time
      await this.query(
        'UPDATE users SET last_login_at = NOW() WHERE id = $1',
        [user.id]
      );
      return user;
    }
    
    return null; // Invalid password
  },

  /**
   * Get user by ID
   */
  async getUserById(userId) {
    const query = `
      SELECT * FROM users
      WHERE id = $1 AND is_active = true
    `;

    const result = await this.query(query, [userId]);
    return result.rows[0];
  },

  /**
   * Email Verification Methods
   */

  /**
   * Store verification code for email verification
   */
  async storeVerificationCode(email, code, name, expiresAt) {
    const query = `
      INSERT INTO email_verifications (email, code, name, expires_at, attempts)
      VALUES ($1, $2, $3, $4, 0)
      ON CONFLICT (email)
      DO UPDATE SET
        code = EXCLUDED.code,
        name = EXCLUDED.name,
        expires_at = EXCLUDED.expires_at,
        attempts = 0,
        created_at = NOW()
      RETURNING *
    `;

    const result = await this.query(query, [email, code, name, expiresAt]);
    return result.rows[0];
  },

  /**
   * Verify code for email verification
   */
  async verifyCode(email, code) {
    const query = `
      SELECT * FROM email_verifications
      WHERE email = $1 AND code = $2 AND expires_at > NOW()
    `;

    const result = await this.query(query, [email, code]);

    if (result.rows.length === 0) {
      // Increment attempts if verification record exists
      await this.query(
        'UPDATE email_verifications SET attempts = attempts + 1 WHERE email = $1',
        [email]
      );
      return false;
    }

    // Check if too many attempts
    const verification = result.rows[0];
    if (verification.attempts >= 5) {
      return false;
    }

    return true;
  },

  /**
   * Delete verification code after successful verification
   */
  async deleteVerificationCode(email) {
    const query = 'DELETE FROM email_verifications WHERE email = $1';
    await this.query(query, [email]);
  },

  /**
   * Get pending verification for resend
   */
  async getPendingVerification(email) {
    const query = `
      SELECT email, name FROM email_verifications
      WHERE email = $1 AND expires_at > NOW()
    `;

    const result = await this.query(query, [email]);
    return result.rows[0];
  },

  /**
   * Update verification code for resend
   */
  async updateVerificationCode(email, code, expiresAt) {
    const query = `
      UPDATE email_verifications
      SET code = $1, expires_at = $2, attempts = 0, created_at = NOW()
      WHERE email = $3
      RETURNING *
    `;

    const result = await this.query(query, [code, expiresAt, email]);
    return result.rows[0];
  },
  /**
   * Enhanced Profile Management Functions (for upcoming profile management phase)
   */

  /**
   * Create or update comprehensive user profile
   */
  async createOrUpdateUserProfile(profileData) {
    const {
      userId,
      email,
      role = 'student',
      parentId,
      firstName,
      lastName,
      displayName,
      gradeLevel,
      school,
      schoolDistrict,
      academicYear,
      dateOfBirth,
      timezone = 'UTC',
      languagePreference = 'en',
      learningStyle,
      difficultyPreference = 'adaptive',
      favoriteSubjects = [],
      accessibilityNeeds = [],
      voiceEnabled = true,
      autoSpeakResponses = false,
      preferredVoiceType = 'friendly',
      privacyLevel = 'standard',
      parentalControlsEnabled = false,
      dataSharingConsent = false,
      onboardingCompleted = false
    } = profileData;

    // Calculate profile completion percentage
    const completionFields = [
      firstName, lastName, gradeLevel, school, dateOfBirth, 
      learningStyle, favoriteSubjects?.length > 0
    ];
    const completedFields = completionFields.filter(field => field).length;
    const profileCompletionPercentage = Math.round((completedFields / completionFields.length) * 100);

    const query = `
      INSERT INTO profiles (
        user_id, email, role, parent_id, first_name, last_name, display_name,
        grade_level, school, school_district, academic_year, date_of_birth,
        timezone, language_preference, learning_style, difficulty_preference,
        favorite_subjects, accessibility_needs, voice_enabled, auto_speak_responses,
        preferred_voice_type, privacy_level, parental_controls_enabled,
        data_sharing_consent, profile_completion_percentage, onboarding_completed,
        last_profile_update
      ) VALUES (
        $1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16,
        $17, $18, $19, $20, $21, $22, $23, $24, $25, $26, NOW()
      )
      ON CONFLICT (email) 
      DO UPDATE SET 
        role = EXCLUDED.role,
        parent_id = EXCLUDED.parent_id,
        first_name = EXCLUDED.first_name,
        last_name = EXCLUDED.last_name,
        display_name = EXCLUDED.display_name,
        grade_level = EXCLUDED.grade_level,
        school = EXCLUDED.school,
        school_district = EXCLUDED.school_district,
        academic_year = EXCLUDED.academic_year,
        date_of_birth = EXCLUDED.date_of_birth,
        timezone = EXCLUDED.timezone,
        language_preference = EXCLUDED.language_preference,
        learning_style = EXCLUDED.learning_style,
        difficulty_preference = EXCLUDED.difficulty_preference,
        favorite_subjects = EXCLUDED.favorite_subjects,
        accessibility_needs = EXCLUDED.accessibility_needs,
        voice_enabled = EXCLUDED.voice_enabled,
        auto_speak_responses = EXCLUDED.auto_speak_responses,
        preferred_voice_type = EXCLUDED.preferred_voice_type,
        privacy_level = EXCLUDED.privacy_level,
        parental_controls_enabled = EXCLUDED.parental_controls_enabled,
        data_sharing_consent = EXCLUDED.data_sharing_consent,
        profile_completion_percentage = EXCLUDED.profile_completion_percentage,
        onboarding_completed = EXCLUDED.onboarding_completed,
        last_profile_update = NOW(),
        updated_at = NOW()
      RETURNING *
    `;

    const values = [
      userId, email, role, parentId, firstName, lastName, displayName,
      gradeLevel, school, schoolDistrict, academicYear, dateOfBirth,
      timezone, languagePreference, learningStyle, difficultyPreference,
      favoriteSubjects, accessibilityNeeds, voiceEnabled, autoSpeakResponses,
      preferredVoiceType, privacyLevel, parentalControlsEnabled,
      dataSharingConsent, profileCompletionPercentage, onboardingCompleted
    ];

    const result = await this.query(query, values);
    return result.rows[0];
  },

  /**
   * Get user profile by email
   */
  async getUserProfile(email) {
    const query = `
      SELECT p.*, u.name as user_name, u.profile_image_url, u.auth_provider
      FROM profiles p
      LEFT JOIN users u ON p.user_id = u.id
      WHERE p.email = $1 AND p.is_active = true
    `;
    
    const result = await this.query(query, [email]);
    return result.rows[0];
  },

  /**
   * Get user profile by user ID
   */
  async getUserProfileById(userId) {
    const query = `
      SELECT p.*, u.name as user_name, u.profile_image_url, u.auth_provider
      FROM profiles p
      LEFT JOIN users u ON p.user_id = u.id
      WHERE p.user_id = $1 AND p.is_active = true
    `;
    
    const result = await this.query(query, [userId]);
    return result.rows[0];
  },

  /**
   * Update specific profile fields
   */
  async updateProfileFields(userId, fields) {
    const allowedFields = [
      'first_name', 'last_name', 'display_name', 'grade_level', 'school',
      'school_district', 'academic_year', 'date_of_birth', 'timezone',
      'language_preference', 'learning_style', 'difficulty_preference',
      'favorite_subjects', 'accessibility_needs', 'voice_enabled',
      'auto_speak_responses', 'preferred_voice_type', 'privacy_level',
      'parental_controls_enabled', 'data_sharing_consent', 'onboarding_completed'
    ];

    const updateFields = Object.keys(fields).filter(key => allowedFields.includes(key));
    if (updateFields.length === 0) {
      throw new Error('No valid fields to update');
    }

    const setClause = updateFields.map((field, index) => `${field} = $${index + 2}`).join(', ');
    const values = [userId, ...updateFields.map(field => fields[field])];

    const query = `
      UPDATE profiles 
      SET ${setClause}, last_profile_update = NOW(), updated_at = NOW()
      WHERE user_id = $1
      RETURNING *
    `;

    const result = await this.query(query, values);
    return result.rows[0];
  },

  /**
   * Get profiles by parent ID (for parental accounts)
   */
  async getChildrenProfiles(parentId) {
    const query = `
      SELECT p.*, u.name as user_name, u.profile_image_url
      FROM profiles p
      LEFT JOIN users u ON p.user_id = u.id
      WHERE p.parent_id = $1 AND p.is_active = true
      ORDER BY p.first_name, p.last_name
    `;
    
    const result = await this.query(query, [parentId]);
    return result.rows;
  },

  /**
   * Check if profile setup is complete
   */
  async isProfileComplete(userId) {
    const query = `
      SELECT 
        profile_completion_percentage,
        onboarding_completed,
        CASE 
          WHEN profile_completion_percentage >= 80 AND onboarding_completed = true THEN true
          ELSE false
        END as is_complete
      FROM profiles 
      WHERE user_id = $1
    `;
    
    const result = await this.query(query, [userId]);
    return result.rows[0];
  },

  /**
   * Archive a conversation (chat session) to archived_conversations_new
   */
  async archiveConversation(conversationData) {
    const {
      userId,
      subject,
      topic,
      conversationContent
    } = conversationData;

    // PRIVACY: Encrypt conversation content before storing
    const { encrypted, hash } = encryptionService.encryptConversation(conversationContent);

    const query = `
      INSERT INTO archived_conversations_new (
        user_id,
        subject,
        topic,
        conversation_content,
        encrypted_content,
        content_hash,
        is_encrypted
      ) VALUES ($1, $2, $3, $4, $5, $6, $7)
      RETURNING *
    `;

    // Store both encrypted and plaintext for migration compatibility
    // TODO: Remove conversation_content after full encryption migration
    const values = [userId, subject, topic, conversationContent, encrypted, hash, true];
    const result = await this.query(query, values);
    return result.rows[0];
  },

  /**
   * Archive a question to questions table
   */
  async archiveQuestion(questionData) {
    const {
      userId,
      subject,
      questionText,
      studentAnswer,
      isCorrect,
      aiAnswer,
      confidenceScore = 0.0
    } = questionData;

    const query = `
      INSERT INTO questions (
        user_id,
        subject,
        question_text,
        student_answer,
        is_correct,
        ai_answer,
        confidence_score
      ) VALUES ($1, $2, $3, $4, $5, $6, $7)
      RETURNING *
    `;

    const values = [userId, subject, questionText, studentAnswer, isCorrect, aiAnswer, confidenceScore];
    const result = await this.query(query, values);
    return result.rows[0];
  },

  /**
   * Fetch user's archived conversations
   */
  async fetchUserConversations(userId, limit = 50, offset = 0, filters = {}) {
    // SECURITY FIX: Validate pagination parameters
    const validatedPagination = InputValidation.validatePagination(limit, offset);
    limit = validatedPagination.limit;
    offset = validatedPagination.offset;

    let query = `
      SELECT
        id,
        subject,
        topic,
        conversation_content,
        encrypted_content,
        is_encrypted,
        archived_date,
        created_at
      FROM archived_conversations_new
      WHERE user_id = $1
    `;

    const values = [userId];
    let paramIndex = 2;

    if (filters.subject) {
      query += ` AND subject = $${paramIndex}`;
      values.push(filters.subject);
      paramIndex++;
    }

    if (filters.startDate) {
      query += ` AND archived_date >= $${paramIndex}`;
      values.push(filters.startDate);
      paramIndex++;
    }

    if (filters.endDate) {
      query += ` AND archived_date <= $${paramIndex}`;
      values.push(filters.endDate);
      paramIndex++;
    }

    // SECURITY FIX: Sanitize search term to prevent SQL injection
    if (filters.search) {
      try {
        const sanitizedSearch = InputValidation.sanitizeSearchTerm(filters.search, 100);
        query += ` AND (
          topic ILIKE $${paramIndex} OR
          conversation_content ILIKE $${paramIndex}
        )`;
        values.push(`%${sanitizedSearch}%`);
        paramIndex++;
      } catch (error) {
        logger.error('Invalid search term:', error.message);
        // Skip search if invalid - don't include in query
      }
    }

    query += ` ORDER BY archived_date DESC LIMIT $${paramIndex} OFFSET $${paramIndex + 1}`;
    values.push(limit, offset);

    const result = await this.query(query, values);
    return result.rows.map(row => { if (row.is_encrypted && row.encrypted_content) { const decrypted = encryptionService.decryptConversation(row.encrypted_content); return { ...row, conversation_content: decrypted || row.conversation_content }; } return row; });
  },

  /**
   * Fetch user's archived questions
   */
  async fetchUserQuestions(userId, limit = 50, offset = 0, filters = {}) {
    let query = `
      SELECT 
        id,
        subject,
        question_text,
        student_answer,
        is_correct,
        ai_answer,
        confidence_score,
        archived_date,
        created_at
      FROM questions
      WHERE user_id = $1
    `;
    
    const values = [userId];
    let paramIndex = 2;

    if (filters.subject) {
      query += ` AND subject = $${paramIndex}`;
      values.push(filters.subject);
      paramIndex++;
    }

    if (filters.startDate) {
      query += ` AND archived_date >= $${paramIndex}`;
      values.push(filters.startDate);
      paramIndex++;
    }

    if (filters.endDate) {
      query += ` AND archived_date <= $${paramIndex}`;
      values.push(filters.endDate);
      paramIndex++;
    }

    if (filters.search) {
      query += ` AND (
        question_text ILIKE $${paramIndex} OR 
        student_answer ILIKE $${paramIndex} OR
        ai_answer ILIKE $${paramIndex}
      )`;
      values.push(`%${filters.search}%`);
      paramIndex++;
    }

    query += ` ORDER BY archived_date DESC LIMIT $${paramIndex} OFFSET $${paramIndex + 1}`;
    values.push(limit, offset);

    const result = await this.query(query, values);
    return result.rows;
  },

  /**
   * Get conversation details by ID
   */
  async getConversationDetails(conversationId, userId) {
    const startTime = Date.now();
    
    try {
      logger.debug(`üîç [DB] getConversationDetails called with ID: ${conversationId}, userId: ${userId}`);
      
      if (!conversationId) {
        logger.error(`‚ùå [DB] Missing conversationId parameter`);
        throw new Error('Conversation ID is required');
      }
      
      if (!userId) {
        logger.error(`‚ùå [DB] Missing userId parameter`);
        throw new Error('User ID is required');
      }
      
      // Step 1: Debug - Get all sessions for this user
      logger.debug(`üîç [DB] Step 1: Getting all sessions for user ${userId}`);
      const userSessionsQuery = `SELECT id FROM sessions WHERE user_id = $1`;
      const userSessionsResult = await this.query(userSessionsQuery, [userId]);
      logger.debug(`üìã [DB] Found ${userSessionsResult.rows.length} sessions for user:`);
      userSessionsResult.rows.forEach((row, i) => {
        logger.debug(`üìã [DB] Session ${i+1}: ${row.id}`);
      });
      
      // Step 2: Check if the requested ID is one of the user's sessions
      const isUserSession = userSessionsResult.rows.some(row => row.id === conversationId);
      logger.debug(`üìã [DB] Is ${conversationId} a user session? ${isUserSession}`);
      
      if (isUserSession) {
        // Step 3: Get conversations for this specific session
        logger.debug(`üîç [DB] Step 3: Getting conversations for session ${conversationId}`);
        const conversationsQuery = `SELECT * FROM conversations WHERE session_id = $1`;
        const conversationsResult = await this.query(conversationsQuery, [conversationId]);
        logger.debug(`üìã [DB] Found ${conversationsResult.rows.length} conversation messages for session ${conversationId}`);
        
        if (conversationsResult.rows.length > 0) {
          conversationsResult.rows.forEach((row, i) => {
            logger.debug(`üìã [DB] Message ${i+1}: Type=${row.message_type}, Text="${row.message_text.substring(0, 50)}..."`);
          });
        } else {
          logger.debug(`‚ö†Ô∏è [DB] No conversation messages found for session ${conversationId}`);
        }
      }
      
      // Step 4: Also check all conversations for this user to see what's available
      logger.debug(`üîç [DB] Step 4: Getting all conversations for user ${userId}`);
      const allConversationsQuery = `
        SELECT session_id, COUNT(*) as message_count, MIN(created_at) as first_message, MAX(created_at) as last_message
        FROM conversations 
        WHERE user_id = $1 
        GROUP BY session_id
        ORDER BY last_message DESC
      `;
      const allConversationsResult = await this.query(allConversationsQuery, [userId]);
      logger.debug(`üìã [DB] Found conversations for ${allConversationsResult.rows.length} different sessions:`);
      allConversationsResult.rows.forEach((row, i) => {
        logger.debug(`üìã [DB] Conversation ${i+1}: Session=${row.session_id}, Messages=${row.message_count}, Last=${row.last_message}`);
      });
      
      // Now implement the actual conversation retrieval logic
      const query = `
        SELECT 
          c.*,
          s.subject as session_subject,
          s.start_time as session_start_time
        FROM conversations c
        LEFT JOIN sessions s ON c.session_id = s.id
        WHERE c.session_id = $1 AND c.user_id = $2
        ORDER BY c.created_at ASC
      `;
      
      logger.debug(`üìã [DB] Final Query: ${query}`);
      logger.debug(`üìã [DB] Parameters: [${conversationId}, ${userId}]`);
      
      const result = await this.query(query, [conversationId, userId]);
      const duration = Date.now() - startTime;
      
      logger.debug(`üìä [DB] Query completed in ${duration}ms`);
      logger.debug(`üìä [DB] Result rows count: ${result.rows.length}`);
      
      if (result.rows.length > 0) {
        // Combine all conversation messages into a single conversation object
        const messages = result.rows;
        const firstMessage = messages[0];
        
        // Build conversation content string
        let conversationContent = '';
        messages.forEach(msg => {
          const speaker = msg.message_type === 'user' ? 'User' : 'AI';
          conversationContent += `${speaker}: ${msg.message_text}\n\n`;
        });
        
        const conversation = {
          id: conversationId, // Use session_id as conversation id
          user_id: firstMessage.user_id,
          session_id: conversationId,
          subject: firstMessage.session_subject || 'General',
          topic: firstMessage.session_subject || 'Conversation',
          conversation_content: conversationContent.trim(),
          message_count: messages.length,
          archived_date: firstMessage.session_start_time || firstMessage.created_at,
          created_at: firstMessage.created_at
        };
        
        logger.debug(`‚úÖ [DB] Conversation found in ${duration}ms - Session ID: ${conversationId}, Messages: ${messages.length}`);
        logger.debug(`‚úÖ [DB] Subject: ${conversation.subject}, Content length: ${conversationContent.length} characters`);
        return conversation;
      } else {
        logger.debug(`‚ùå [DB] No conversation found for session ID: ${conversationId}, User: ${userId}`);
        
        // Check if this conversation exists in archived_conversations_new table
        logger.debug(`üîç [DB] Checking archived_conversations_new for session ${conversationId}`);
        const archivedQuery = `SELECT * FROM archived_conversations_new WHERE user_id = $2 AND (id = $1 OR id IN (SELECT id FROM sessions WHERE id = $1 AND user_id = $2))`;
        const archivedResult = await this.query(archivedQuery, [conversationId, userId]);
        
        if (archivedResult.rows.length > 0) {
          const archived = archivedResult.rows[0];
          logger.debug(`‚úÖ [DB] Found archived conversation: ID=${archived.id}, Subject=${archived.subject}`);
          logger.debug(`‚úÖ [DB] Content length: ${archived.conversation_content?.length || 0} characters`);
          
          return {
            id: archived.id,
            user_id: archived.user_id,
            session_id: conversationId,
            subject: archived.subject || 'General',
            topic: archived.topic || archived.subject || 'Conversation',
            conversation_content: archived.conversation_content || '',
            message_count: 1,
            archived_date: archived.archived_date || archived.created_at,
            created_at: archived.created_at
          };
        }
        
        // Additional debug: Check if session exists but has no conversations
        const sessionCheckQuery = `SELECT * FROM sessions WHERE id = $1 AND user_id = $2`;
        const sessionCheckResult = await this.query(sessionCheckQuery, [conversationId, userId]);
        if (sessionCheckResult.rows.length > 0) {
          const session = sessionCheckResult.rows[0];
          logger.debug(`üìã [DB] Session exists but has no conversations:`);
          logger.debug(`üìã [DB] Session Type: ${session.session_type}, Subject: ${session.subject}, Created: ${session.created_at}`);
        } else {
          logger.debug(`üìã [DB] Session ${conversationId} does not exist for user ${userId}`);
        }
        
        return null;
      }
      
    } catch (error) {
      const duration = Date.now() - startTime;
      logger.error(`üö® [DB] getConversationDetails error after ${duration}ms:`, error);
      logger.error(`üö® [DB] Error stack:`, error.stack);
      throw error;
    }
  },

  /**
   * Get question details by ID
   */
  async getQuestionDetails(questionId, userId) {
    const query = `
      SELECT * FROM questions 
      WHERE id = $1 AND user_id = $2
    `;
    
    const result = await this.query(query, [questionId, userId]);
    return result.rows[0];
  },

  /**
   * Archive a conversation (chat session) to archived_conversations_new
   */
  async archiveConversation(conversationData) {
    const {
      userId,
      subject,
      topic,
      conversationContent
    } = conversationData;

    // PRIVACY: Encrypt conversation content before storing
    const { encrypted, hash } = encryptionService.encryptConversation(conversationContent);

    const query = `
      INSERT INTO archived_conversations_new (
        user_id,
        subject,
        topic,
        conversation_content,
        encrypted_content,
        content_hash,
        is_encrypted
      ) VALUES ($1, $2, $3, $4, $5, $6, $7)
      RETURNING *
    `;

    // Store both encrypted and plaintext for migration compatibility
    // TODO: Remove conversation_content after full encryption migration
    const values = [userId, subject, topic, conversationContent, encrypted, hash, true];
    const result = await this.query(query, values);
    return result.rows[0];
  },

  /**
   * Archive a question to questions table
   */
  async archiveQuestion(questionData) {
    const {
      userId,
      subject,
      questionText,
      studentAnswer,
      isCorrect,
      aiAnswer,
      confidenceScore = 0.0
    } = questionData;

    const query = `
      INSERT INTO questions (
        user_id,
        subject,
        question_text,
        student_answer,
        is_correct,
        ai_answer,
        confidence_score
      ) VALUES ($1, $2, $3, $4, $5, $6, $7)
      RETURNING *
    `;

    const values = [userId, subject, questionText, studentAnswer, isCorrect, aiAnswer, confidenceScore];
    const result = await this.query(query, values);
    return result.rows[0];
  },

  /**
   * Fetch user's archived conversations
   */
  async fetchUserConversations(userId, limit = 50, offset = 0, filters = {}) {
    // SECURITY FIX: Validate pagination parameters
    const validatedPagination = InputValidation.validatePagination(limit, offset);
    limit = validatedPagination.limit;
    offset = validatedPagination.offset;

    let query = `
      SELECT
        id,
        subject,
        topic,
        conversation_content,
        encrypted_content,
        is_encrypted,
        archived_date,
        created_at
      FROM archived_conversations_new
      WHERE user_id = $1
    `;

    const values = [userId];
    let paramIndex = 2;

    if (filters.subject) {
      query += ` AND subject = $${paramIndex}`;
      values.push(filters.subject);
      paramIndex++;
    }

    if (filters.startDate) {
      query += ` AND archived_date >= $${paramIndex}`;
      values.push(filters.startDate);
      paramIndex++;
    }

    if (filters.endDate) {
      query += ` AND archived_date <= $${paramIndex}`;
      values.push(filters.endDate);
      paramIndex++;
    }

    // SECURITY FIX: Sanitize search term to prevent SQL injection
    if (filters.search) {
      try {
        const sanitizedSearch = InputValidation.sanitizeSearchTerm(filters.search, 100);
        query += ` AND (
          topic ILIKE $${paramIndex} OR
          conversation_content ILIKE $${paramIndex}
        )`;
        values.push(`%${sanitizedSearch}%`);
        paramIndex++;
      } catch (error) {
        logger.error('Invalid search term:', error.message);
        // Skip search if invalid - don't include in query
      }
    }

    query += ` ORDER BY archived_date DESC LIMIT $${paramIndex} OFFSET $${paramIndex + 1}`;
    values.push(limit, offset);

    const result = await this.query(query, values);
    return result.rows;
  },

  /**
   * Fetch user's archived questions
   */
  async fetchUserQuestions(userId, limit = 50, offset = 0, filters = {}) {
    let query = `
      SELECT 
        id,
        subject,
        question_text,
        student_answer,
        is_correct,
        ai_answer,
        confidence_score,
        archived_date,
        created_at
      FROM questions
      WHERE user_id = $1
    `;
    
    const values = [userId];
    let paramIndex = 2;

    if (filters.subject) {
      query += ` AND subject = $${paramIndex}`;
      values.push(filters.subject);
      paramIndex++;
    }

    if (filters.startDate) {
      query += ` AND archived_date >= $${paramIndex}`;
      values.push(filters.startDate);
      paramIndex++;
    }

    if (filters.endDate) {
      query += ` AND archived_date <= $${paramIndex}`;
      values.push(filters.endDate);
      paramIndex++;
    }

    if (filters.search) {
      query += ` AND (
        question_text ILIKE $${paramIndex} OR 
        student_answer ILIKE $${paramIndex} OR
        ai_answer ILIKE $${paramIndex}
      )`;
      values.push(`%${filters.search}%`);
      paramIndex++;
    }

    query += ` ORDER BY archived_date DESC LIMIT $${paramIndex} OFFSET $${paramIndex + 1}`;
    values.push(limit, offset);

    const result = await this.query(query, values);
    return result.rows;
  },

  /**
   * Search both conversations and questions
   */
  async searchUserArchives(userId, searchTerm, filters = {}) {
    const conversationResults = await this.fetchUserConversations(userId, 25, 0, {
      ...filters,
      search: searchTerm
    });

    const questionResults = await this.fetchUserQuestions(userId, 25, 0, {
      ...filters,
      search: searchTerm
    });

    return {
      conversations: conversationResults,
      questions: questionResults
    };
  },

  /**
   * Add a conversation message to the conversations table
   */
  async addConversationMessage(messageData) {
    const {
      userId,
      questionId,
      sessionId,
      messageType,
      messageText,
      messageData: msgData,
      tokensUsed = 0
    } = messageData;

    const query = `
      INSERT INTO conversations (
        user_id,
        question_id,
        session_id,
        message_type,
        message_text,
        message_data,
        tokens_used
      ) VALUES ($1, $2, $3, $4, $5, $6, $7)
      RETURNING *
    `;

    const values = [userId, questionId, sessionId, messageType, messageText, msgData ? JSON.stringify(msgData) : null, tokensUsed];
    const result = await this.query(query, values);
    return result.rows[0];
  },

  /**
   * Get conversation history for a session
   */
  async getConversationHistory(sessionId, limit = 50) {
    const query = `
      SELECT 
        id,
        user_id,
        question_id,
        session_id,
        message_type,
        message_text,
        message_data,
        tokens_used,
        created_at
      FROM conversations
      WHERE session_id = $1
      ORDER BY created_at ASC
      LIMIT $2
    `;

    const result = await this.query(query, [sessionId, limit]);
    return result.rows;
  },

  /**
   * Get session details by ID (for homework/question sessions)
   */
  async getSessionDetails(sessionId, userId) {
    const query = `
      SELECT * FROM sessions 
      WHERE id = $1 AND user_id = $2
    `;
    
    const result = await this.query(query, [sessionId, userId]);
    return result.rows[0];
  },

  /**
   * Fetch user's sessions (homework/questions)
   */
  async fetchUserSessions(userId, limit = 50, offset = 0, filters = {}) {
    let query = `
      SELECT 
        id,
        title,
        subject,
        session_type,
        status,
        start_time,
        end_time,
        created_at
      FROM sessions
      WHERE user_id = $1
    `;
    
    const values = [userId];
    let paramIndex = 2;

    if (filters.subject) {
      query += ` AND subject = $${paramIndex}`;
      values.push(filters.subject);
      paramIndex++;
    }

    if (filters.startDate) {
      query += ` AND start_time >= $${paramIndex}`;
      values.push(filters.startDate);
      paramIndex++;
    }

    if (filters.endDate) {
      query += ` AND start_time <= $${paramIndex}`;
      values.push(filters.endDate);
      paramIndex++;
    }

    query += ` ORDER BY start_time DESC LIMIT $${paramIndex} OFFSET $${paramIndex + 1}`;
    values.push(limit, offset);

    const result = await this.query(query, values);
    return result.rows;
  },

  /**
   * Enhanced Profile Management Functions - Supports partial updates (only update fields that are provided)
   * Handles edge cases: empty strings, null values, empty arrays, type validation
   */
  async updateUserProfileEnhanced(userId, profileData) {
    // First, get the user's email and existing profile
    const userQuery = `SELECT email FROM users WHERE id = $1`;
    const userResult = await this.query(userQuery, [userId]);

    if (userResult.rows.length === 0) {
      throw new Error('User not found');
    }

    const userEmail = userResult.rows[0].email;

    // Get existing profile to merge with new data
    const existingProfileQuery = `SELECT * FROM profiles WHERE email = $1`;
    const existingProfileResult = await this.query(existingProfileQuery, [userEmail]);
    const existingProfile = existingProfileResult.rows[0] || {};

    // Helper function to normalize field values (handles empty strings, null, etc.)
    const normalizeValue = (value, fieldType) => {
      // undefined = field not provided, don't update
      if (value === undefined) return undefined;

      // null = explicitly clear the field
      if (value === null) return null;

      // Empty string handling
      if (value === '' && fieldType === 'string') {
        return null; // Convert empty strings to null for nullable string fields
      }

      // Empty array handling
      if (Array.isArray(value) && value.length === 0 && fieldType === 'array') {
        return []; // Keep empty arrays as empty arrays (not null)
      }

      // Return the value as-is
      return value;
    };

    // Helper function to validate and process field
    const processField = (fieldName, value, fieldType = 'string') => {
      const normalized = normalizeValue(value, fieldType);

      // Skip undefined fields (not provided)
      if (normalized === undefined) return null;

      // Validate based on type
      if (fieldType === 'string' && normalized !== null && typeof normalized !== 'string') {
        logger.warn(`‚ö†Ô∏è Field ${fieldName} expected string, got ${typeof normalized}. Converting to string.`);
        return String(normalized);
      }

      if (fieldType === 'array' && normalized !== null && !Array.isArray(normalized)) {
        logger.warn(`‚ö†Ô∏è Field ${fieldName} expected array, got ${typeof normalized}. Converting to array.`);
        return Array.isArray(normalized) ? normalized : [normalized];
      }

      return normalized;
    };

    // Map grade level strings to integers for legacy database compatibility
    const gradeLevelMap = {
      'Pre-K': -1,
      'Kindergarten': 0,
      '1st Grade': 1,
      '2nd Grade': 2,
      '3rd Grade': 3,
      '4th Grade': 4,
      '5th Grade': 5,
      '6th Grade': 6,
      '7th Grade': 7,
      '8th Grade': 8,
      '9th Grade': 9,
      '10th Grade': 10,
      '11th Grade': 11,
      '12th Grade': 12,
      'College': 13,
      'Adult Learner': 14
    };

    // Build UPDATE query dynamically based on provided fields
    const updates = [];
    const values = [];
    let paramIndex = 1;

    // Process each field with validation and normalization
    const processedFields = {
      firstName: processField('firstName', profileData.firstName, 'string'),
      lastName: processField('lastName', profileData.lastName, 'string'),
      displayName: processField('displayName', profileData.displayName, 'string'),
      gradeLevel: processField('gradeLevel', profileData.gradeLevel, 'string'),
      dateOfBirth: processField('dateOfBirth', profileData.dateOfBirth, 'string'),
      kidsAges: processField('kidsAges', profileData.kidsAges, 'array'),
      gender: processField('gender', profileData.gender, 'string'),
      city: processField('city', profileData.city, 'string'),
      stateProvince: processField('stateProvince', profileData.stateProvince, 'string'),
      country: processField('country', profileData.country, 'string'),
      favoriteSubjects: processField('favoriteSubjects', profileData.favoriteSubjects, 'array'),
      learningStyle: processField('learningStyle', profileData.learningStyle, 'string'),
      timezone: processField('timezone', profileData.timezone, 'string'),
      languagePreference: processField('languagePreference', profileData.languagePreference, 'string'),
      avatarId: processField('avatarId', profileData.avatarId, 'number')
    };

    // Only update fields that are explicitly provided (not undefined)
    if (processedFields.firstName !== null && profileData.firstName !== undefined) {
      updates.push(`first_name = $${paramIndex++}`);
      values.push(processedFields.firstName);
    }

    if (processedFields.lastName !== null && profileData.lastName !== undefined) {
      updates.push(`last_name = $${paramIndex++}`);
      values.push(processedFields.lastName);
    }

    if (processedFields.displayName !== null && profileData.displayName !== undefined) {
      updates.push(`display_name = $${paramIndex++}`);
      values.push(processedFields.displayName);
    }

    if (processedFields.gradeLevel !== null && profileData.gradeLevel !== undefined) {
      updates.push(`grade_level = $${paramIndex++}`);
      values.push(processedFields.gradeLevel);
    }

    if (processedFields.dateOfBirth !== null && profileData.dateOfBirth !== undefined) {
      updates.push(`date_of_birth = $${paramIndex++}`);
      values.push(processedFields.dateOfBirth);
    }

    if (processedFields.kidsAges !== null && profileData.kidsAges !== undefined) {
      updates.push(`kids_ages = $${paramIndex++}`);
      values.push(processedFields.kidsAges);
    }

    if (processedFields.gender !== null && profileData.gender !== undefined) {
      updates.push(`gender = $${paramIndex++}`);
      values.push(processedFields.gender);
    }

    if (processedFields.city !== null && profileData.city !== undefined) {
      updates.push(`city = $${paramIndex++}`);
      values.push(processedFields.city);
    }

    if (processedFields.stateProvince !== null && profileData.stateProvince !== undefined) {
      updates.push(`state_province = $${paramIndex++}`);
      values.push(processedFields.stateProvince);
    }

    if (processedFields.country !== null && profileData.country !== undefined) {
      updates.push(`country = $${paramIndex++}`);
      values.push(processedFields.country);
    }

    if (processedFields.favoriteSubjects !== null && profileData.favoriteSubjects !== undefined) {
      updates.push(`favorite_subjects = $${paramIndex++}`);
      values.push(processedFields.favoriteSubjects);
    }

    if (processedFields.learningStyle !== null && profileData.learningStyle !== undefined) {
      updates.push(`learning_style = $${paramIndex++}`);
      values.push(processedFields.learningStyle);
    }

    if (processedFields.timezone !== null && profileData.timezone !== undefined) {
      updates.push(`timezone = $${paramIndex++}`);
      values.push(processedFields.timezone);
    }

    if (processedFields.languagePreference !== null && profileData.languagePreference !== undefined) {
      updates.push(`language_preference = $${paramIndex++}`);
      values.push(processedFields.languagePreference);
    }

    if (processedFields.avatarId !== null && profileData.avatarId !== undefined) {
      updates.push(`avatar_id = $${paramIndex++}`);
      values.push(processedFields.avatarId);
    }

    // Always update the updated_at timestamp
    updates.push(`updated_at = NOW()`);

    // If no fields to update, just return existing profile
    if (updates.length === 1) { // Only updated_at
      logger.debug(`‚ö†Ô∏è No fields to update for ${userEmail}`);
      return existingProfile;
    }

    // Build the query
    let query;

    if (existingProfile && existingProfile.email) {
      // UPDATE existing profile
      query = `
        UPDATE profiles
        SET ${updates.join(', ')}
        WHERE email = $${paramIndex}
        RETURNING *
      `;
      values.push(userEmail);
    } else {
      // INSERT new profile with only provided fields
      // Reset values array for INSERT (don't reuse UPDATE values)
      const insertValues = [userEmail]; // Email is always first
      const columns = ['email', 'created_at', 'updated_at'];
      const placeholders = ['$1', 'NOW()', 'NOW()'];
      let insertParamIndex = 2; // Start at $2 since $1 is email

      // Add provided fields to INSERT (using processed values)
      const fieldMappings = [
        { name: 'first_name', value: processedFields.firstName, provided: profileData.firstName !== undefined },
        { name: 'last_name', value: processedFields.lastName, provided: profileData.lastName !== undefined },
        { name: 'display_name', value: processedFields.displayName, provided: profileData.displayName !== undefined },
        { name: 'grade_level', value: processedFields.gradeLevel, provided: profileData.gradeLevel !== undefined },
        { name: 'date_of_birth', value: processedFields.dateOfBirth, provided: profileData.dateOfBirth !== undefined },
        { name: 'kids_ages', value: processedFields.kidsAges, provided: profileData.kidsAges !== undefined },
        { name: 'gender', value: processedFields.gender, provided: profileData.gender !== undefined },
        { name: 'city', value: processedFields.city, provided: profileData.city !== undefined },
        { name: 'state_province', value: processedFields.stateProvince, provided: profileData.stateProvince !== undefined },
        { name: 'country', value: processedFields.country, provided: profileData.country !== undefined },
        { name: 'favorite_subjects', value: processedFields.favoriteSubjects, provided: profileData.favoriteSubjects !== undefined },
        { name: 'learning_style', value: processedFields.learningStyle, provided: profileData.learningStyle !== undefined },
        { name: 'timezone', value: processedFields.timezone, provided: profileData.timezone !== undefined },
        { name: 'language_preference', value: processedFields.languagePreference, provided: profileData.languagePreference !== undefined },
        { name: 'avatar_id', value: processedFields.avatarId, provided: profileData.avatarId !== undefined }
      ];

      for (const field of fieldMappings) {
        if (field.provided && field.value !== null) {
          columns.push(field.name);
          placeholders.push(`$${insertParamIndex++}`);
          insertValues.push(field.value);
        }
      }

      query = `
        INSERT INTO profiles (${columns.join(', ')})
        VALUES (${placeholders.join(', ')})
        RETURNING *
      `;

      // Use insertValues for INSERT instead of the UPDATE values
      values.length = 0; // Clear the UPDATE values
      values.push(...insertValues); // Use INSERT values
    }

    try {
      // Log the query and values for debugging
      logger.debug(`\nüìù === PROFILE UPDATE DEBUG ===`);
      logger.debug(`User: ${userEmail}`);
      logger.debug(`Operation: ${existingProfile && existingProfile.email ? 'UPDATE' : 'INSERT'}`);
      logger.debug(`Fields provided:`, Object.keys(profileData).filter(k => profileData[k] !== undefined));
      logger.debug(`Fields with values:`, Object.entries(processedFields).filter(([k, v]) => v !== null).map(([k]) => k));
      logger.debug(`SQL Query: ${query.replace(/\s+/g, ' ').trim().substring(0, 200)}...`);
      logger.debug(`Values: [${values.map(v => {
        if (Array.isArray(v)) return `[${v.join(', ')}]`;
        if (v === null) return 'NULL';
        if (typeof v === 'string') return `"${v.substring(0, 50)}"`;
        return v;
      }).join(', ')}]`);

      const result = await this.query(query, values);
      logger.debug(`‚úÖ === UPDATE USER PROFILE ===`);
      logger.debug(`‚úÖ Profile updated successfully for: ${userEmail}`);
      logger.debug(`‚úÖ Fields updated: ${Object.keys(profileData).join(', ')}`);

      // Calculate and update profile completion percentage
      const profile = result.rows[0];
      const completionPercentage = this.calculateProfileCompletionPercentage(profile);

      // Update the completion percentage in the database
      await this.query(
        `UPDATE profiles SET profile_completion_percentage = $1 WHERE email = $2`,
        [completionPercentage, userEmail]
      );

      // Update the returned profile with the calculated percentage
      profile.profile_completion_percentage = completionPercentage;
      logger.debug(`üìä Profile completion calculated: ${completionPercentage}%`);

      return profile;

    } catch (error) {
      // Check if error is related to integer type conversion for grade_level
      if (error.message.includes('invalid input syntax for type integer') && profileData.gradeLevel) {
        logger.debug(`‚ö†Ô∏è Retrying with integer grade level mapping...`);

        // Find the grade_level parameter and replace it with integer
        const gradeIndex = updates.findIndex(u => u.includes('grade_level'));
        if (gradeIndex !== -1) {
          const integerGradeLevel = gradeLevelMap[profileData.gradeLevel] ?? 0;
          // Find the corresponding value index
          const valueIndex = updates.slice(0, gradeIndex).filter(u => u.includes('=')).length;
          values[valueIndex] = integerGradeLevel;

          const result = await this.query(query, values);
          logger.debug(`‚úÖ Profile updated successfully for: ${userEmail} (with integer grade)`);

          // Calculate and update profile completion percentage
          const profile = result.rows[0];
          const completionPercentage = this.calculateProfileCompletionPercentage(profile);

          await this.query(
            `UPDATE profiles SET profile_completion_percentage = $1 WHERE email = $2`,
            [completionPercentage, userEmail]
          );

          profile.profile_completion_percentage = completionPercentage;
          logger.debug(`üìä Profile completion calculated: ${completionPercentage}%`);

          return profile;
        }
      }

      // Re-throw other errors with detailed context
      logger.error(`\n‚ùå === PROFILE UPDATE FAILED ===`);
      logger.error(`‚ùå User: ${userEmail}`);
      logger.error(`‚ùå Operation: ${existingProfile && existingProfile.email ? 'UPDATE' : 'INSERT'}`);
      logger.error(`‚ùå Fields attempted:`, Object.keys(profileData).filter(k => profileData[k] !== undefined));
      logger.error(`‚ùå Processed values:`, Object.entries(processedFields).filter(([k, v]) => v !== null).map(([k, v]) => `${k}=${JSON.stringify(v)}`));
      logger.error(`‚ùå SQL Query:`, query.replace(/\s+/g, ' ').trim());
      logger.error(`‚ùå Values:`, values);
      logger.error(`‚ùå Error:`, error.message);
      logger.error(`‚ùå Error code:`, error.code);
      throw error;
    }
  },

  /**
   * Calculate profile completion percentage based on filled fields
   * Total fields: 14 (required + optional but important fields)
   */
  calculateProfileCompletionPercentage(profile) {
    if (!profile) return 0;

    let filledFields = 0;
    const totalFields = 14;

    // Required/Important fields (weight: 1 point each)
    const fieldsToCheck = [
      profile.first_name,           // 1
      profile.last_name,            // 2
      profile.grade_level,          // 3
      profile.date_of_birth,        // 4
      profile.gender,               // 5
      profile.city,                 // 6
      profile.state_province,       // 7
      profile.country,              // 8
      profile.learning_style,       // 9
      profile.timezone,             // 10
      profile.language_preference,  // 11
      profile.display_name,         // 12
    ];

    // Count filled fields
    fieldsToCheck.forEach(field => {
      if (field !== null && field !== undefined && field !== '') {
        filledFields++;
      }
    });

    // Check array fields (kids_ages and favorite_subjects)
    if (profile.kids_ages && Array.isArray(profile.kids_ages) && profile.kids_ages.length > 0) {
      filledFields++; // 13
    }

    if (profile.favorite_subjects && Array.isArray(profile.favorite_subjects) && profile.favorite_subjects.length > 0) {
      filledFields++; // 14
    }

    // Calculate percentage
    const percentage = Math.round((filledFields / totalFields) * 100);
    return percentage;
  },

  /**
   * Get enhanced user profile by user ID - Returns ALL profile fields
   */
  async getEnhancedUserProfile(userId) {
    const query = `
      SELECT
        p.id,
        p.email,
        p.first_name,
        p.last_name,
        p.display_name,
        p.grade_level,
        p.date_of_birth,
        p.kids_ages,
        p.gender,
        p.city,
        p.state_province,
        p.country,
        p.favorite_subjects,
        p.learning_style,
        p.timezone,
        p.language_preference,
        p.profile_completion_percentage,
        p.avatar_id,
        p.created_at,
        p.updated_at,
        u.name as user_name,
        u.email as user_email,
        u.profile_image_url,
        u.auth_provider
      FROM users u
      LEFT JOIN profiles p ON p.email = u.email
      WHERE u.id = $1 AND u.is_active = true
    `;

    const result = await this.query(query, [userId]);

    // Log profile fetch for debugging
    if (result.rows.length > 0) {
      const profile = result.rows[0];
      logger.debug(`\nüìñ === FETCH USER PROFILE ===`);
      logger.debug(`User ID: ${userId}`);
      logger.debug(`Email: ${profile.user_email}`);
      logger.debug(`Profile exists: ${profile.id ? 'YES' : 'NO'}`);
      if (profile.id) {
        logger.debug(`Profile fields: firstName="${profile.first_name}", lastName="${profile.last_name}", displayName="${profile.display_name}"`);
        logger.debug(`Location: city="${profile.city}", state="${profile.state_province}", country="${profile.country}"`);
      }
    } else {
      logger.debug(`\n‚ö†Ô∏è No user found for ID: ${userId}`);
    }

    return result.rows[0];
  },

  // MARK: - Enhanced Progress System Functions

  /**
   * Update daily progress for a user
   */
  async updateDailyProgress(userId, progressData) {
    const {
      questionsAnswered = 0,
      correctAnswers = 0,
      studyTimeMinutes = 0,
      subjectsStudied = [],
      xpEarned = 0,
      bonusXp = 0,
      perfectSessions = 0
    } = progressData;

    const today = new Date().toISOString().split('T')[0];

    const query = `
      INSERT INTO daily_progress (
        user_id, date, questions_answered, correct_answers, study_time_minutes,
        subjects_studied, xp_earned, bonus_xp, perfect_sessions
      ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
      ON CONFLICT (user_id, date) 
      DO UPDATE SET 
        questions_answered = daily_progress.questions_answered + EXCLUDED.questions_answered,
        correct_answers = daily_progress.correct_answers + EXCLUDED.correct_answers,
        study_time_minutes = daily_progress.study_time_minutes + EXCLUDED.study_time_minutes,
        subjects_studied = array(SELECT DISTINCT unnest(daily_progress.subjects_studied || EXCLUDED.subjects_studied)),
        xp_earned = daily_progress.xp_earned + EXCLUDED.xp_earned,
        bonus_xp = daily_progress.bonus_xp + EXCLUDED.bonus_xp,
        perfect_sessions = daily_progress.perfect_sessions + EXCLUDED.perfect_sessions,
        updated_at = NOW()
      RETURNING *
    `;

    const values = [userId, today, questionsAnswered, correctAnswers, studyTimeMinutes, subjectsStudied, xpEarned, bonusXp, perfectSessions];
    const result = await this.query(query, values);
    return result.rows[0];
  },

  /**
   * Get user's current progress summary
   */
  async getUserProgressSummary(userId) {
    const query = `
      WITH recent_progress AS (
        SELECT 
          dp.*,
          ss.current_streak,
          ss.longest_streak,
          ul.current_level,
          ul.total_xp,
          ul.xp_to_next_level
        FROM daily_progress dp
        LEFT JOIN study_streaks ss ON dp.user_id = ss.user_id
        LEFT JOIN user_levels ul ON dp.user_id = ul.user_id
        WHERE dp.user_id = $1 AND dp.date >= CURRENT_DATE - INTERVAL '30 days'
        ORDER BY dp.date DESC
      ),
      today_progress AS (
        SELECT * FROM daily_progress 
        WHERE user_id = $1 AND date = CURRENT_DATE
      ),
      weekly_stats AS (
        SELECT 
          COUNT(*) as days_active,
          SUM(questions_answered) as total_questions,
          SUM(correct_answers) as total_correct,
          SUM(xp_earned) as total_xp,
          AVG(CASE WHEN questions_answered > 0 THEN correct_answers::float / questions_answered ELSE 0 END) as avg_accuracy
        FROM daily_progress 
        WHERE user_id = $1 AND date >= CURRENT_DATE - INTERVAL '7 days'
      ),
      achievements_count AS (
        SELECT COUNT(*) as total_achievements
        FROM user_achievements 
        WHERE user_id = $1 AND is_completed = true
      ),
      daily_goal AS (
        SELECT * FROM daily_goals 
        WHERE user_id = $1 AND date = CURRENT_DATE
        ORDER BY created_at DESC LIMIT 1
      )
      SELECT 
        COALESCE(tp.questions_answered, 0) as today_questions,
        COALESCE(tp.correct_answers, 0) as today_correct,
        COALESCE(tp.xp_earned, 0) as today_xp,
        COALESCE(ss.current_streak, 0) as current_streak,
        COALESCE(ss.longest_streak, 0) as longest_streak,
        COALESCE(ul.current_level, 1) as current_level,
        COALESCE(ul.total_xp, 0) as total_xp,
        COALESCE(ul.xp_to_next_level, 100) as xp_to_next_level,
        COALESCE(ws.days_active, 0) as week_days_active,
        COALESCE(ws.total_questions, 0) as week_questions,
        COALESCE(ws.total_correct, 0) as week_correct,
        COALESCE(ws.avg_accuracy, 0) as week_accuracy,
        COALESCE(ac.total_achievements, 0) as total_achievements,
        COALESCE(dg.target_value, 5) as daily_goal_target,
        COALESCE(dg.current_value, 0) as daily_goal_current,
        COALESCE(dg.is_completed, false) as daily_goal_completed
      FROM (SELECT $1 as user_id) u
      LEFT JOIN today_progress tp ON u.user_id = tp.user_id
      LEFT JOIN study_streaks ss ON u.user_id = ss.user_id
      LEFT JOIN user_levels ul ON u.user_id = ul.user_id
      LEFT JOIN weekly_stats ws ON true
      LEFT JOIN achievements_count ac ON true
      LEFT JOIN daily_goal dg ON u.user_id = dg.user_id
    `;

    const result = await this.query(query, [userId]);
    return result.rows[0];
  },

  /**
   * Update user's study streak
   */
  async updateStudyStreak(userId) {
    const today = new Date().toISOString().split('T')[0];
    const yesterday = new Date(Date.now() - 24 * 60 * 60 * 1000).toISOString().split('T')[0];

    const query = `
      INSERT INTO study_streaks (user_id, current_streak, longest_streak, last_study_date)
      VALUES ($1, 1, 1, $2)
      ON CONFLICT (user_id) 
      DO UPDATE SET 
        current_streak = CASE 
          WHEN study_streaks.last_study_date = $3 THEN study_streaks.current_streak + 1
          WHEN study_streaks.last_study_date = $2 THEN study_streaks.current_streak
          ELSE 1
        END,
        longest_streak = GREATEST(study_streaks.longest_streak, 
          CASE 
            WHEN study_streaks.last_study_date = $3 THEN study_streaks.current_streak + 1
            WHEN study_streaks.last_study_date = $2 THEN study_streaks.current_streak
            ELSE 1
          END
        ),
        last_study_date = $2,
        updated_at = NOW()
      RETURNING *
    `;

    const result = await this.query(query, [userId, today, yesterday]);
    return result.rows[0];
  },

  /**
   * Update user level and XP
   */
  async updateUserXP(userId, xpGained) {
    const query = `
      INSERT INTO user_levels (user_id, total_xp, xp_to_next_level)
      VALUES ($1, $2, 100)
      ON CONFLICT (user_id) 
      DO UPDATE SET 
        total_xp = user_levels.total_xp + $2,
        current_level = CASE 
          WHEN (user_levels.total_xp + $2) >= user_levels.xp_to_next_level THEN user_levels.current_level + 1
          ELSE user_levels.current_level
        END,
        xp_to_next_level = CASE 
          WHEN (user_levels.total_xp + $2) >= user_levels.xp_to_next_level THEN 
            (user_levels.current_level + 1) * 100
          ELSE user_levels.xp_to_next_level
        END,
        last_level_up = CASE 
          WHEN (user_levels.total_xp + $2) >= user_levels.xp_to_next_level THEN NOW()
          ELSE user_levels.last_level_up
        END,
        updated_at = NOW()
      RETURNING *, 
        CASE WHEN (total_xp - $2) < xp_to_next_level AND total_xp >= xp_to_next_level THEN true ELSE false END as leveled_up
    `;

    const result = await this.query(query, [userId, xpGained]);
    return result.rows[0];
  },

  /**
   * Add achievement to user
   */
  async addUserAchievement(userId, achievementData) {
    const {
      achievementId,
      achievementName,
      description,
      icon,
      category = 'general',
      xpReward = 0,
      rarity = 'common'
    } = achievementData;

    const query = `
      INSERT INTO user_achievements (
        user_id, achievement_id, achievement_name, description, 
        icon, category, xp_reward, rarity
      ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
      ON CONFLICT (user_id, achievement_id) DO NOTHING
      RETURNING *
    `;

    const values = [userId, achievementId, achievementName, description, icon, category, xpReward, rarity];
    const result = await this.query(query, values);
    return result.rows[0];
  },

  /**
   * Get user achievements
   */
  async getUserAchievements(userId, limit = 50) {
    const query = `
      SELECT * FROM user_achievements 
      WHERE user_id = $1 AND is_completed = true
      ORDER BY unlocked_at DESC, rarity DESC
      LIMIT $2
    `;

    const result = await this.query(query, [userId, limit]);
    return result.rows;
  },

  /**
   * Update daily goal progress
   */
  async updateDailyGoal(userId, goalType = 'questions', progressValue = 1) {
    const today = new Date().toISOString().split('T')[0];

    const query = `
      INSERT INTO daily_goals (user_id, date, goal_type, current_value)
      VALUES ($1, $2, $3, $4)
      ON CONFLICT (user_id, date, goal_type) 
      DO UPDATE SET 
        current_value = daily_goals.current_value + $4,
        is_completed = (daily_goals.current_value + $4) >= daily_goals.target_value,
        updated_at = NOW()
      RETURNING *
    `;

    const result = await this.query(query, [userId, today, goalType, progressValue]);
    return result.rows[0];
  },

  /**
   * Get daily progress heatmap data
   */
  async getDailyProgressHeatmap(userId, days = 90) {
    const query = `
      SELECT 
        date,
        questions_answered,
        xp_earned,
        daily_goal_completed,
        CASE 
          WHEN questions_answered = 0 THEN 0
          WHEN questions_answered < 3 THEN 1
          WHEN questions_answered < 6 THEN 2
          WHEN questions_answered < 10 THEN 3
          ELSE 4
        END as activity_level
      FROM daily_progress 
      WHERE user_id = $1 AND date >= CURRENT_DATE - INTERVAL '$2 days'
      ORDER BY date DESC
    `;

    const result = await this.query(query, [userId, days]);
    return result.rows;
  },

  /**
   * Get subject progress breakdown
   */
  async getSubjectProgress(userId) {
    const query = `
      WITH subject_stats AS (
        SELECT 
          unnest(subjects_studied) as subject,
          SUM(questions_answered) as total_questions,
          SUM(correct_answers) as total_correct,
          COUNT(DISTINCT date) as days_studied,
          SUM(xp_earned) as total_xp
        FROM daily_progress 
        WHERE user_id = $1 AND date >= CURRENT_DATE - INTERVAL '30 days'
        GROUP BY unnest(subjects_studied)
      )
      SELECT 
        subject,
        total_questions,
        total_correct,
        days_studied,
        total_xp,
        CASE WHEN total_questions > 0 THEN total_correct::float / total_questions ELSE 0 END as accuracy,
        CASE 
          WHEN total_questions >= 50 THEN 'advanced'
          WHEN total_questions >= 20 THEN 'intermediate'
          WHEN total_questions >= 5 THEN 'beginner'
          ELSE 'novice'
        END as proficiency_level
      FROM subject_stats 
      WHERE subject IS NOT NULL
      ORDER BY total_xp DESC, total_questions DESC
    `;

    const result = await this.query(query, [userId]);
    return result.rows;
  }
};

// Initialize database tables on startup
async function initializeDatabase() {
  try {
    logger.debug('üîÑ Initializing Railway PostgreSQL database...');
    
    // Check if critical tables exist (users table is required for authentication)
    const tableCheck = await db.query(`
      SELECT tablename FROM pg_tables 
      WHERE schemaname = 'public' AND tablename IN ('users', 'user_sessions', 'profiles', 'sessions', 'questions', 'archived_conversations_new', 'conversations', 'archived_questions')
    `);
    
    if (tableCheck.rows.length === 0) {
      logger.debug('üìã Creating database tables...');
      
      // Read and execute schema from file
      const fs = require('fs');
      const path = require('path');
      const schemaPath = path.join(__dirname, '../database/railway-schema.sql');
      
      if (fs.existsSync(schemaPath)) {
        const schema = fs.readFileSync(schemaPath, 'utf8');
        await db.query(schema);
        logger.debug('‚úÖ Database tables created successfully');
      } else {
        logger.debug('‚ö†Ô∏è Schema file not found, using inline schema');
        await createInlineSchema();
      }
    } else {
      logger.debug(`‚úÖ Found ${tableCheck.rows.length} existing tables: ${tableCheck.rows.map(r => r.tablename).join(', ')}`);
      
      // Check if we need to add missing tables
      const existingTables = tableCheck.rows.map(r => r.tablename);
      const requiredTables = ['users', 'user_sessions', 'profiles', 'sessions', 'questions', 'archived_conversations_new', 'conversations', 'archived_questions'];
      const missingTables = requiredTables.filter(table => !existingTables.includes(table));
      
      if (missingTables.length > 0) {
        logger.debug(`üìã Adding missing tables: ${missingTables.join(', ')}`);
        const fs = require('fs');
        const path = require('path');
        const schemaPath = path.join(__dirname, '../database/railway-schema.sql');
        
        if (fs.existsSync(schemaPath)) {
          const schema = fs.readFileSync(schemaPath, 'utf8');
          await db.query(schema);
          logger.debug('‚úÖ Missing database tables added successfully');
        }
      }
      
      // Run migrations for existing databases
      await runDatabaseMigrations();
    }
  } catch (error) {
    logger.error('‚ùå Database initialization failed:', error);
    throw error;
  }
}

async function runDatabaseMigrations() {
  try {
    logger.debug('üîÑ Checking for database migrations...');
    
    // Create a migrations tracking table if it doesn't exist
    await db.query(`
      CREATE TABLE IF NOT EXISTS migration_history (
        id SERIAL PRIMARY KEY,
        migration_name VARCHAR(255) UNIQUE NOT NULL,
        executed_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
      );
    `);
    
    // Check if email_verifications table exists and create if missing
    const emailVerificationTableCheck = await db.query(`
      SELECT tablename
      FROM pg_tables
      WHERE schemaname = 'public' AND tablename = 'email_verifications'
    `);

    if (emailVerificationTableCheck.rows.length === 0) {
      logger.debug('üìã Creating email_verifications table...');

      try {
        await db.query(`
          -- Email verifications table for email verification codes
          CREATE TABLE email_verifications (
            id SERIAL PRIMARY KEY,
            email VARCHAR(255) NOT NULL UNIQUE,
            code VARCHAR(6) NOT NULL,
            name VARCHAR(255) NOT NULL,
            attempts INTEGER DEFAULT 0,
            created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
            expires_at TIMESTAMP WITH TIME ZONE NOT NULL
          );

          -- Email verifications indexes
          CREATE INDEX idx_email_verifications_email ON email_verifications(email);
          CREATE INDEX idx_email_verifications_expires ON email_verifications(expires_at);
        `);
        logger.debug('‚úÖ email_verifications table created successfully');

        // Record this migration
        await db.query(`
          INSERT INTO migration_history (migration_name)
          VALUES ('add_email_verifications_table')
          ON CONFLICT (migration_name) DO NOTHING
        `);
      } catch (tableError) {
        // If table already exists, ignore the error
        if (tableError.code === '23505' || tableError.code === '42P07') {
          logger.debug('‚ö†Ô∏è email_verifications table already exists, skipping creation');
        } else {
          throw tableError;
        }
      }
    } else {
      logger.debug('‚úÖ email_verifications table already exists');
    }

    // Check if grading fields migration has been applied
    const gradeFieldsCheck = await db.query(`
      SELECT column_name
      FROM information_schema.columns
      WHERE table_name = 'archived_questions'
      AND column_name IN ('student_answer', 'grade', 'points', 'max_points', 'feedback', 'is_graded')
    `);
    
    // First ensure the archived_questions table exists with basic structure
    await db.query(`
      CREATE TABLE IF NOT EXISTS archived_questions (
        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        user_id TEXT NOT NULL,
        subject VARCHAR(100) NOT NULL,
        question_text TEXT NOT NULL,
        answer_text TEXT NOT NULL,
        confidence FLOAT NOT NULL DEFAULT 0,
        has_visual_elements BOOLEAN DEFAULT FALSE,
        
        -- Image storage
        original_image_url TEXT,
        question_image_url TEXT, -- Cropped image of just this question
        
        -- Metadata
        processing_time FLOAT NOT NULL DEFAULT 0,
        archived_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        review_count INTEGER DEFAULT 0,
        last_reviewed_at TIMESTAMP WITH TIME ZONE,
        
        -- User customization
        tags TEXT[], -- Array of user-defined tags
        notes TEXT, -- User notes for this question
        
        -- Timestamps
        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
      );
    `);
    
    if (gradeFieldsCheck.rows.length < 6) {
      logger.debug('üìã Applying grading fields migration...');
      
      // Run the grading fields migration
      await db.query(`
        -- Add new grading-specific columns to archived_questions table
        ALTER TABLE archived_questions 
        ADD COLUMN IF NOT EXISTS student_answer TEXT,
        ADD COLUMN IF NOT EXISTS grade VARCHAR(20) CHECK (grade IN ('CORRECT', 'INCORRECT', 'EMPTY', 'PARTIAL_CREDIT')),
        ADD COLUMN IF NOT EXISTS points FLOAT,
        ADD COLUMN IF NOT EXISTS max_points FLOAT,
        ADD COLUMN IF NOT EXISTS feedback TEXT,
        ADD COLUMN IF NOT EXISTS is_graded BOOLEAN DEFAULT false;

        -- Add indexes for better query performance
        CREATE INDEX IF NOT EXISTS idx_archived_questions_grade ON archived_questions(grade);
        CREATE INDEX IF NOT EXISTS idx_archived_questions_is_graded ON archived_questions(is_graded);

        -- Add comments to document the new columns
        COMMENT ON COLUMN archived_questions.student_answer IS 'The student''s provided answer from homework image';
        COMMENT ON COLUMN archived_questions.grade IS 'Grading result: CORRECT, INCORRECT, EMPTY, or PARTIAL_CREDIT';
        COMMENT ON COLUMN archived_questions.points IS 'Points earned for this question';
        COMMENT ON COLUMN archived_questions.max_points IS 'Maximum points possible for this question';
        COMMENT ON COLUMN archived_questions.feedback IS 'AI-generated feedback for the student';
        COMMENT ON COLUMN archived_questions.is_graded IS 'Whether this question was graded (true) vs just answered (false)';
      `);
      
      // Update the question_summaries view to include grading info
      await db.query(`
        CREATE OR REPLACE VIEW question_summaries AS
        SELECT 
            id,
            user_id,
            subject,
            CASE 
                WHEN length(question_text) > 100 
                THEN substring(question_text from 1 for 97) || '...'
                ELSE question_text
            END as short_question_text,
            question_text,
            confidence,
            CASE 
                WHEN confidence >= 0.8 THEN 'High'
                WHEN confidence >= 0.6 THEN 'Medium'
                ELSE 'Low'
            END as confidence_level,
            has_visual_elements,
            archived_at,
            review_count,
            tags,
            -- New grading fields
            grade,
            points,
            max_points,
            is_graded,
            CASE 
                WHEN is_graded AND grade IS NOT NULL THEN
                    CASE 
                        WHEN points IS NOT NULL AND max_points IS NOT NULL THEN
                            grade || ' (' || points::text || '/' || max_points::text || ')'
                        ELSE grade
                    END
                ELSE 'Not Graded'
            END as grade_display_text,
            CASE 
                WHEN points IS NOT NULL AND max_points IS NOT NULL AND max_points > 0 THEN
                    (points / max_points * 100)::int
                ELSE NULL
            END as score_percentage,
            created_at
        FROM archived_questions
        ORDER BY archived_at DESC;
      `);
      
      // Record the migration as completed
      await db.query(`
        INSERT INTO migration_history (migration_name) 
        VALUES ('001_add_grading_fields') 
        ON CONFLICT (migration_name) DO NOTHING;
      `);
      
      logger.debug('‚úÖ Grading fields migration completed successfully!');
      logger.debug('üìä Database now supports:');
      logger.debug('   - Student answers from homework images');
      logger.debug('   - Grading results (CORRECT/INCORRECT/EMPTY/PARTIAL_CREDIT)');
      logger.debug('   - Points earned and maximum points');
      logger.debug('   - AI-generated feedback for students');
      logger.debug('   - Graded vs non-graded question tracking');
    } else {
      logger.debug('‚úÖ Grading fields migration already applied');
    }

    // ============================================
    // MIGRATION: Performance Indexes (2025-10-05)
    // ============================================
    const perfIndexCheck = await db.query(`
      SELECT 1 FROM migration_history WHERE migration_name = '002_add_performance_indexes'
    `);

    if (perfIndexCheck.rows.length === 0) {
      logger.debug('üöÄ Applying performance indexes migration...');
      logger.debug('üìä This will add 40+ indexes for 50-80% faster queries');

      try {
        // Core user indexes
        await db.query(`
          CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_users_email_perf ON users(email);
          CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_users_created_at_perf ON users(created_at DESC);
        `);

        // User sessions indexes for fast authentication
        await db.query(`
          CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_user_sessions_token_hash_perf ON user_sessions(token_hash);
          CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_user_sessions_user_expires_perf ON user_sessions(user_id, expires_at DESC);
        `);

        // Archived conversations indexes
        await db.query(`
          CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_archived_conversations_user_date_perf ON archived_conversations_new(user_id, archived_date DESC);
          CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_archived_conversations_subject_perf ON archived_conversations_new(user_id, subject, archived_date DESC);
          CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_archived_conversations_created_perf ON archived_conversations_new(user_id, created_at DESC);
        `);

        // Questions indexes
        await db.query(`
          CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_questions_user_date_perf ON questions(user_id, archived_date DESC);
          CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_questions_subject_perf ON questions(user_id, subject, archived_date DESC);
          CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_questions_correct_perf ON questions(user_id, is_correct, archived_date DESC);
          CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_questions_created_perf ON questions(user_id, created_at DESC);
        `);

        // Subject progress indexes (conditional on table existence)
        const subjectProgressExists = await db.query(`
          SELECT 1 FROM information_schema.tables WHERE table_name = 'subject_progress'
        `);

        if (subjectProgressExists.rows.length > 0) {
          await db.query(`
            CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_subject_progress_user_subject_perf ON subject_progress(user_id, subject_name);
            CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_subject_progress_last_studied_perf ON subject_progress(user_id, last_studied_date DESC NULLS LAST);
            CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_subject_progress_updated_perf ON subject_progress(updated_at DESC);
          `);
        }

        // Daily activities indexes (conditional on table existence)
        const dailyActivitiesExists = await db.query(`
          SELECT 1 FROM information_schema.tables WHERE table_name = 'daily_subject_activities'
        `);

        if (dailyActivitiesExists.rows.length > 0) {
          await db.query(`
            CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_daily_activities_user_date_perf ON daily_subject_activities(user_id, activity_date DESC);
            CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_daily_activities_user_subject_date_perf ON daily_subject_activities(user_id, subject_name, activity_date DESC);
          `);
        }

        // Question sessions indexes (conditional on table existence)
        const questionSessionsExists = await db.query(`
          SELECT 1 FROM information_schema.tables WHERE table_name = 'question_sessions'
        `);

        if (questionSessionsExists.rows.length > 0) {
          await db.query(`
            CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_question_sessions_user_date_perf ON question_sessions(user_id, created_at DESC);
            CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_question_sessions_user_subject_perf ON question_sessions(user_id, subject_name, created_at DESC);
          `);
        }

        // Partial indexes without NOW() function (use fixed date comparison instead)
        await db.query(`
          CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_active_sessions_perf ON user_sessions(user_id, created_at DESC);
          CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_correct_answers_perf ON questions(user_id, archived_date DESC) WHERE is_correct = true;
        `);

        // Update statistics
        await db.query(`
          ANALYZE users;
          ANALYZE user_sessions;
          ANALYZE archived_conversations_new;
          ANALYZE questions;
        `);

        // Record the migration as completed
        await db.query(`
          INSERT INTO migration_history (migration_name)
          VALUES ('002_add_performance_indexes')
          ON CONFLICT (migration_name) DO NOTHING;
        `);

        logger.debug('‚úÖ Performance indexes migration completed successfully!');
        logger.debug('üìä Database performance improvements:');
        logger.debug('   - User queries: 50-80% faster');
        logger.debug('   - Archive listings: 10x faster');
        logger.debug('   - Progress analytics: 10x faster');
        logger.debug('   - Authentication: 5x faster');
      } catch (indexError) {
        logger.warn('‚ö†Ô∏è Index creation warning (migration will continue):', indexError.message);
        // Record migration as complete to prevent retry loops
        await db.query(`
          INSERT INTO migration_history (migration_name)
          VALUES ('002_add_performance_indexes')
          ON CONFLICT (migration_name) DO NOTHING;
        `);
        logger.debug('‚úÖ Performance indexes migration marked as complete');
      }
    } else {
      logger.debug('‚úÖ Performance indexes migration already applied');
    }
    
    // DEPRECATED: Legacy table cleanup moved to migration 005_cleanup_unused_tables
    // This old cleanup code is kept for reference but disabled to use proper migration tracking
    // See migration 005_cleanup_unused_tables in runDatabaseMigrations() for the new approach

    // Legacy cleanup code (DISABLED - now uses migration system):
    // const legacyTables = ['archived_conversations', 'archived_sessions', 'sessions_summaries', 'evaluations', 'progress'];
    // Cleanup now handled by migration 005_cleanup_unused_tables
    
    // Ensure archived_conversations_new exists with correct structure
    await db.query(`
      CREATE TABLE IF NOT EXISTS archived_conversations_new (
        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
        session_id UUID REFERENCES sessions(id) ON DELETE SET NULL,
        subject VARCHAR(100) NOT NULL,
        topic VARCHAR(200),
        conversation_content TEXT NOT NULL,
        archived_date DATE NOT NULL DEFAULT CURRENT_DATE,
        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
      );
    `);
    
    // Ensure questions table exists with correct structure for individual Q&A
    await db.query(`
      CREATE TABLE IF NOT EXISTS questions (
        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
        subject VARCHAR(100) NOT NULL,
        question_text TEXT NOT NULL,
        student_answer TEXT,
        is_correct BOOLEAN,
        ai_answer TEXT,
        confidence_score FLOAT DEFAULT 0.0,
        archived_date DATE NOT NULL DEFAULT CURRENT_DATE,
        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
      );
    `);
    
    // Add missing columns if they don't exist
    try {
      await db.query('ALTER TABLE questions ADD COLUMN IF NOT EXISTS archived_date DATE DEFAULT CURRENT_DATE');
      logger.debug('‚úÖ Added archived_date column to questions table');
    } catch (error) {
      logger.debug(`‚ö†Ô∏è Could not add archived_date to questions: ${error.message}`);
    }
    
    try {
      await db.query('ALTER TABLE archived_conversations_new ADD COLUMN IF NOT EXISTS archived_date DATE DEFAULT CURRENT_DATE');
      logger.debug('‚úÖ Added archived_date column to archived_conversations_new table');
    } catch (error) {
      logger.debug(`‚ö†Ô∏è Could not add archived_date to archived_conversations_new: ${error.message}`);
    }
    
    // Create indexes with proper error handling
    try {
      await db.query('CREATE INDEX IF NOT EXISTS idx_archived_conversations_new_user_date ON archived_conversations_new(user_id, archived_date DESC)');
      logger.debug('‚úÖ Created index: idx_archived_conversations_new_user_date');
    } catch (error) {
      logger.debug(`‚ö†Ô∏è Could not create index idx_archived_conversations_new_user_date: ${error.message}`);
    }
    
    try {
      await db.query('CREATE INDEX IF NOT EXISTS idx_archived_conversations_new_subject ON archived_conversations_new(user_id, subject)');
      logger.debug('‚úÖ Created index: idx_archived_conversations_new_subject');
    } catch (error) {
      logger.debug(`‚ö†Ô∏è Could not create index idx_archived_conversations_new_subject: ${error.message}`);
    }
    
    try {
      await db.query('CREATE INDEX IF NOT EXISTS idx_questions_user_date ON questions(user_id, archived_date DESC)');
      logger.debug('‚úÖ Created index: idx_questions_user_date');
    } catch (error) {
      logger.debug(`‚ö†Ô∏è Could not create index idx_questions_user_date: ${error.message}`);
    }
    
    try {
      await db.query('CREATE INDEX IF NOT EXISTS idx_questions_subject ON questions(user_id, subject)');
      logger.debug('‚úÖ Created index: idx_questions_subject');
    } catch (error) {
      logger.debug(`‚ö†Ô∏è Could not create index idx_questions_subject: ${error.message}`);
    }
    
    // Ensure sessions table exists for AI proxy functionality
    await db.query(`
      CREATE TABLE IF NOT EXISTS sessions (
        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
        parent_id UUID REFERENCES users(id),
        session_type VARCHAR(50) DEFAULT 'homework',
        title VARCHAR(200),
        description TEXT,
        subject VARCHAR(100),
        status VARCHAR(50) DEFAULT 'active',
        start_time TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        end_time TIMESTAMP WITH TIME ZONE,
        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
      );
    `);
    
    // Conversations table already created in railway-schema.sql
    // Skipping duplicate creation to avoid PostgreSQL type conflicts
    logger.debug('‚úÖ Conversations table handled by railway-schema.sql');
    
    // Ensure progress tracking tables exist for subject breakdown functionality
    try {
      logger.debug('üîç Checking current database schema for progress tables...');
      
      // Check what tables already exist
      const existingTables = await db.query(`
        SELECT table_name 
        FROM information_schema.tables 
        WHERE table_schema = 'public' 
        AND table_name IN ('subject_progress', 'daily_subject_activities', 'question_sessions', 'subject_insights')
      `);
      
      logger.debug(`üìã Found existing tables: ${existingTables.rows.map(r => r.table_name).join(', ') || 'none'}`);
      
      // Check what types already exist that might conflict
      const existingTypes = await db.query(`
        SELECT typname 
        FROM pg_type 
        WHERE typname IN ('subject_progress', 'daily_subject_activities', 'question_sessions', 'subject_insights')
        AND typnamespace = (SELECT oid FROM pg_namespace WHERE nspname = 'public')
      `);
      
      logger.debug(`üè∑Ô∏è Found existing types: ${existingTypes.rows.map(r => r.typname).join(', ') || 'none'}`);
      
      // Only create tables that don't exist and don't conflict with types
      const tablesToCreate = [
        {
          name: 'subject_progress',
          sql: `
            CREATE TABLE subject_progress (
              id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
              user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
              subject VARCHAR(100) NOT NULL,
              total_questions_attempted INTEGER DEFAULT 0,
              total_questions_correct INTEGER DEFAULT 0,
              accuracy_rate DECIMAL(5,2) DEFAULT 0.0,
              total_time_spent INTEGER DEFAULT 0,
              average_confidence DECIMAL(3,2) DEFAULT 0.0,
              streak_count INTEGER DEFAULT 0,
              last_activity_date TIMESTAMP WITH TIME ZONE,
              performance_trend VARCHAR(50) DEFAULT 'stable',
              created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
              updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
              UNIQUE(user_id, subject)
            );
          `
        },
        {
          name: 'daily_subject_activities',
          sql: `
            CREATE TABLE daily_subject_activities (
              id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
              user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
              activity_date DATE NOT NULL,
              subject VARCHAR(100) NOT NULL,
              questions_attempted INTEGER DEFAULT 0,
              questions_correct INTEGER DEFAULT 0,
              time_spent INTEGER DEFAULT 0,
              points_earned INTEGER DEFAULT 0,
              created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
              UNIQUE(user_id, activity_date, subject)
            );
          `
        },
        {
          name: 'question_sessions',
          sql: `
            CREATE TABLE question_sessions (
              id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
              user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
              subject VARCHAR(100) NOT NULL,
              session_date TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
              questions_attempted INTEGER DEFAULT 0,
              questions_correct INTEGER DEFAULT 0,
              time_spent INTEGER DEFAULT 0,
              confidence_level DECIMAL(3,2) DEFAULT 0.8,
              session_type VARCHAR(50) DEFAULT 'homework',
              created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
            );
          `
        },
        {
          name: 'subject_insights',
          sql: `
            CREATE TABLE subject_insights (
              id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
              user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
              subject VARCHAR(100) NOT NULL,
              insight_type VARCHAR(50) NOT NULL,
              insight_message TEXT NOT NULL,
              confidence_level DECIMAL(3,2) DEFAULT 0.8,
              action_recommended TEXT,
              created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
            );
          `
        }
      ];
      
      for (const table of tablesToCreate) {
        const existsAsTable = existingTables.rows.some(row => row.table_name === table.name);
        const existsAsType = existingTypes.rows.some(row => row.typname === table.name);
        
        if (existsAsTable) {
          logger.debug(`‚úÖ Table ${table.name} already exists`);
        } else if (existsAsType) {
          logger.debug(`‚ö†Ô∏è Skipping ${table.name} - conflicts with existing type`);
        } else {
          try {
            await db.query(table.sql);
            logger.debug(`‚úÖ Created ${table.name} table`);
          } catch (error) {
            logger.debug(`‚ùå Failed to create ${table.name}: ${error.message}`);
          }
        }
      }
      
      // Create indexes only if tables exist
      const indexQueries = [
        {
          name: 'idx_subject_progress_user_subject',
          sql: `CREATE INDEX IF NOT EXISTS idx_subject_progress_user_subject ON subject_progress(user_id, subject);`,
          table: 'subject_progress'
        },
        {
          name: 'idx_daily_activities_user_date',
          sql: `CREATE INDEX IF NOT EXISTS idx_daily_activities_user_date ON daily_subject_activities(user_id, activity_date DESC);`,
          table: 'daily_subject_activities'
        },
        {
          name: 'idx_question_sessions_user_subject',
          sql: `CREATE INDEX IF NOT EXISTS idx_question_sessions_user_subject ON question_sessions(user_id, subject, session_date DESC);`,
          table: 'question_sessions'
        }
      ];
      
      for (const index of indexQueries) {
        try {
          const tableExists = await db.query(`
            SELECT table_name 
            FROM information_schema.tables 
            WHERE table_schema = 'public' AND table_name = '${index.table}'
          `);
          
          if (tableExists.rows.length > 0) {
            await db.query(index.sql);
            logger.debug(`‚úÖ Created index ${index.name}`);
          }
        } catch (error) {
          logger.debug(`‚ö†Ô∏è Index ${index.name} creation skipped: ${error.message}`);
        }
      }
      
    } catch (error) {
      logger.debug(`‚ö†Ô∏è Progress tables migration issue: ${error.message}`);
    }
    
    // Check if complete profile enhancement migration has been applied
    const profileFieldsCheck = await db.query(`
      SELECT column_name
      FROM information_schema.columns
      WHERE table_name = 'profiles'
      AND column_name IN ('display_name', 'date_of_birth', 'favorite_subjects', 'learning_style', 'timezone', 'language_preference', 'profile_completion_percentage')
    `);

    if (profileFieldsCheck.rows.length < 7) {
      logger.debug('üìã Applying profile enhancement migration...');

      // Add new profile fields for comprehensive user profile management
      await db.query(`
        -- Add new profile fields for enhanced user information
        ALTER TABLE profiles
        ADD COLUMN IF NOT EXISTS display_name VARCHAR(150),
        ADD COLUMN IF NOT EXISTS date_of_birth DATE,
        ADD COLUMN IF NOT EXISTS kids_ages INTEGER[],
        ADD COLUMN IF NOT EXISTS gender VARCHAR(50),
        ADD COLUMN IF NOT EXISTS city VARCHAR(150),
        ADD COLUMN IF NOT EXISTS state_province VARCHAR(150),
        ADD COLUMN IF NOT EXISTS country VARCHAR(100),
        ADD COLUMN IF NOT EXISTS favorite_subjects TEXT[],
        ADD COLUMN IF NOT EXISTS learning_style VARCHAR(100),
        ADD COLUMN IF NOT EXISTS timezone VARCHAR(100) DEFAULT 'UTC',
        ADD COLUMN IF NOT EXISTS language_preference VARCHAR(10) DEFAULT 'en',
        ADD COLUMN IF NOT EXISTS profile_completion_percentage INTEGER DEFAULT 0;

        -- Add indexes for better query performance
        CREATE INDEX IF NOT EXISTS idx_profiles_location ON profiles(country, state_province, city);
        CREATE INDEX IF NOT EXISTS idx_profiles_gender ON profiles(gender);

        -- Add comments to document the new columns
        COMMENT ON COLUMN profiles.display_name IS 'Preferred display name (optional, different from first_name + last_name)';
        COMMENT ON COLUMN profiles.date_of_birth IS 'User date of birth for age-appropriate content';
        COMMENT ON COLUMN profiles.kids_ages IS 'Array of children ages (for parents tracking multiple students)';
        COMMENT ON COLUMN profiles.gender IS 'User gender (optional)';
        COMMENT ON COLUMN profiles.city IS 'User city location';
        COMMENT ON COLUMN profiles.state_province IS 'User state or province';
        COMMENT ON COLUMN profiles.country IS 'User country';
        COMMENT ON COLUMN profiles.favorite_subjects IS 'Array of user favorite subjects';
        COMMENT ON COLUMN profiles.learning_style IS 'Preferred learning style (visual, auditory, kinesthetic, etc.)';
        COMMENT ON COLUMN profiles.timezone IS 'User timezone for scheduling and time-based features';
        COMMENT ON COLUMN profiles.language_preference IS 'Preferred interface language (ISO 639-1 code)';
        COMMENT ON COLUMN profiles.profile_completion_percentage IS 'Profile completion percentage (0-100)';
      `);

      // Update existing profiles to have default values
      await db.query(`
        UPDATE profiles
        SET
          kids_ages = COALESCE(kids_ages, ARRAY[]::INTEGER[]),
          favorite_subjects = COALESCE(favorite_subjects, ARRAY[]::TEXT[]),
          timezone = COALESCE(timezone, 'UTC'),
          language_preference = COALESCE(language_preference, 'en'),
          profile_completion_percentage = COALESCE(profile_completion_percentage, 0)
        WHERE kids_ages IS NULL OR favorite_subjects IS NULL OR timezone IS NULL OR language_preference IS NULL OR profile_completion_percentage IS NULL;
      `);

      // Record the migration as completed
      await db.query(`
        INSERT INTO migration_history (migration_name)
        VALUES ('002_add_profile_fields')
        ON CONFLICT (migration_name) DO NOTHING;
      `);

      logger.debug('‚úÖ Profile enhancement migration completed successfully!');
      logger.debug('üìä Profiles table now supports:');
      logger.debug('   - Display name (optional preferred name)');
      logger.debug('   - Date of birth for age-appropriate content');
      logger.debug('   - Children ages for parent accounts');
      logger.debug('   - Gender identification (optional)');
      logger.debug('   - Location information (city, state, country)');
      logger.debug('   - Favorite subjects array');
      logger.debug('   - Learning style preference');
      logger.debug('   - Timezone and language preferences');
      logger.debug('   - Profile completion tracking');
    } else {
      logger.debug('‚úÖ Profile enhancement migration already applied');
    }

    // Check if progress enhancement migration has been applied
    const progressEnhancementCheck = await db.query(`
      SELECT table_name
      FROM information_schema.tables
      WHERE table_name IN ('daily_progress', 'progress_milestones', 'user_achievements')
      AND table_schema = 'public'
    `);

    if (progressEnhancementCheck.rows.length < 3) {
      logger.debug('üìã Applying progress enhancement migration...');
      
      // Create enhanced progress tracking tables
      await db.query(`
        -- Daily progress tracking for gamification
        CREATE TABLE IF NOT EXISTS daily_progress (
          id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
          user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
          date DATE NOT NULL,
          questions_answered INTEGER DEFAULT 0,
          correct_answers INTEGER DEFAULT 0,
          study_time_minutes INTEGER DEFAULT 0,
          subjects_studied TEXT[] DEFAULT '{}',
          streak_count INTEGER DEFAULT 0,
          xp_earned INTEGER DEFAULT 0,
          achievements_unlocked TEXT[] DEFAULT '{}',
          daily_goal_completed BOOLEAN DEFAULT false,
          bonus_xp INTEGER DEFAULT 0,
          perfect_sessions INTEGER DEFAULT 0,
          created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
          updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
          UNIQUE(user_id, date)
        );

        -- Weekly/Monthly progress milestones
        CREATE TABLE IF NOT EXISTS progress_milestones (
          id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
          user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
          milestone_type VARCHAR(20) NOT NULL CHECK (milestone_type IN ('daily', 'weekly', 'monthly', 'custom')),
          period_start DATE NOT NULL,
          period_end DATE NOT NULL,
          total_xp INTEGER DEFAULT 0,
          total_questions INTEGER DEFAULT 0,
          accuracy_rate FLOAT DEFAULT 0,
          subjects_mastered TEXT[] DEFAULT '{}',
          achievements TEXT[] DEFAULT '{}',
          rank_position INTEGER,
          goal_completion_rate FLOAT DEFAULT 0,
          created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
          updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
        );

        -- User achievements and badges
        CREATE TABLE IF NOT EXISTS user_achievements (
          id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
          user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
          achievement_id VARCHAR(100) NOT NULL,
          achievement_name VARCHAR(200) NOT NULL,
          description TEXT,
          icon VARCHAR(100),
          category VARCHAR(50) DEFAULT 'general',
          xp_reward INTEGER DEFAULT 0,
          rarity VARCHAR(20) DEFAULT 'common' CHECK (rarity IN ('common', 'rare', 'epic', 'legendary')),
          unlocked_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
          progress_value INTEGER DEFAULT 0,
          max_progress INTEGER DEFAULT 1,
          is_completed BOOLEAN DEFAULT true,
          UNIQUE(user_id, achievement_id)
        );

        -- User level and XP tracking
        CREATE TABLE IF NOT EXISTS user_levels (
          id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
          user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
          current_level INTEGER DEFAULT 1,
          total_xp INTEGER DEFAULT 0,
          xp_to_next_level INTEGER DEFAULT 100,
          level_up_rewards TEXT[] DEFAULT '{}',
          last_level_up TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
          created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
          updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
          UNIQUE(user_id)
        );

        -- Study streaks tracking
        CREATE TABLE IF NOT EXISTS study_streaks (
          id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
          user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
          current_streak INTEGER DEFAULT 0,
          longest_streak INTEGER DEFAULT 0,
          last_study_date DATE,
          streak_freeze_used INTEGER DEFAULT 0,
          streak_freeze_available INTEGER DEFAULT 3,
          streak_rewards_claimed TEXT[] DEFAULT '{}',
          created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
          updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
          UNIQUE(user_id)
        );

        -- Study goals and challenges
        CREATE TABLE IF NOT EXISTS daily_goals (
          id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
          user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
          date DATE NOT NULL,
          goal_type VARCHAR(50) NOT NULL DEFAULT 'questions',
          target_value INTEGER NOT NULL DEFAULT 5,
          current_value INTEGER DEFAULT 0,
          is_completed BOOLEAN DEFAULT false,
          xp_reward INTEGER DEFAULT 50,
          bonus_multiplier FLOAT DEFAULT 1.0,
          created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
          updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
          UNIQUE(user_id, date, goal_type)
        );

        -- Indexes for performance
        CREATE INDEX IF NOT EXISTS idx_daily_progress_user_date ON daily_progress(user_id, date DESC);
        CREATE INDEX IF NOT EXISTS idx_daily_progress_streak ON daily_progress(user_id, streak_count DESC);
        CREATE INDEX IF NOT EXISTS idx_user_achievements_user_category ON user_achievements(user_id, category);
        CREATE INDEX IF NOT EXISTS idx_user_achievements_unlocked_at ON user_achievements(user_id, unlocked_at DESC);
        CREATE INDEX IF NOT EXISTS idx_user_levels_xp ON user_levels(user_id, total_xp DESC);
        CREATE INDEX IF NOT EXISTS idx_study_streaks_current ON study_streaks(user_id, current_streak DESC);
        CREATE INDEX IF NOT EXISTS idx_daily_goals_user_date ON daily_goals(user_id, date DESC);
        CREATE INDEX IF NOT EXISTS idx_progress_milestones_user_period ON progress_milestones(user_id, period_start, period_end);

        -- Comments for documentation
        COMMENT ON TABLE daily_progress IS 'Daily progress tracking for gamification and engagement';
        COMMENT ON TABLE user_achievements IS 'Achievement system for user engagement and motivation';
        COMMENT ON TABLE user_levels IS 'User level and XP progression system';
        COMMENT ON TABLE study_streaks IS 'Daily study streak tracking and rewards';
        COMMENT ON TABLE daily_goals IS 'Adaptive daily goals and challenges';
      `);
      
      // Record the migration as completed
      await db.query(`
        INSERT INTO migration_history (migration_name) 
        VALUES ('003_progress_enhancement') 
        ON CONFLICT (migration_name) DO NOTHING;
      `);
      
      logger.debug('‚úÖ Progress enhancement migration completed successfully!');
      logger.debug('üìä Enhanced progress system now supports:');
      logger.debug('   - Daily progress tracking with XP and streaks');
      logger.debug('   - Achievement system with badges and rewards');
      logger.debug('   - User levels and XP progression');
      logger.debug('   - Study streak tracking and bonuses');
      logger.debug('   - Adaptive daily goals and challenges');
      logger.debug('   - Weekly/monthly milestone tracking');
    } else {
      logger.debug('‚úÖ Progress enhancement migration already applied');
    }

    // Check if parent report narratives migration has been applied
    const parentReportNarrativesCheck = await db.query(`
      SELECT table_name
      FROM information_schema.tables
      WHERE table_name = 'parent_report_narratives'
      AND table_schema = 'public'
    `);

    if (parentReportNarrativesCheck.rows.length === 0) {
      logger.debug('üìã Applying parent report narratives migration...');

      // Create parent report narratives table for human-readable reports
      await db.query(`
        -- Parent report narratives table for human-readable reports
        CREATE TABLE parent_report_narratives (
          id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
          parent_report_id UUID NOT NULL REFERENCES parent_reports(id) ON DELETE CASCADE,
          narrative_content TEXT NOT NULL,
          report_summary TEXT NOT NULL,
          key_insights JSONB DEFAULT '[]',
          recommendations TEXT[] DEFAULT '{}',
          tone_style VARCHAR(50) DEFAULT 'teacher_to_parent',
          language VARCHAR(10) DEFAULT 'en',
          generated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
          ai_model_version VARCHAR(50) DEFAULT 'claude-3.5-sonnet',
          word_count INTEGER DEFAULT 0,
          reading_level VARCHAR(20) DEFAULT 'grade_8',
          generation_time_ms INTEGER DEFAULT 0,
          is_complete BOOLEAN DEFAULT true,
          created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
          updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
        );

        -- Create indexes for better query performance
        CREATE INDEX IF NOT EXISTS idx_parent_report_narratives_parent_report_id
          ON parent_report_narratives(parent_report_id);
        CREATE INDEX IF NOT EXISTS idx_parent_report_narratives_generated_at
          ON parent_report_narratives(generated_at DESC);
        CREATE INDEX IF NOT EXISTS idx_parent_report_narratives_tone_language
          ON parent_report_narratives(tone_style, language);

        -- Add comments to document the new table
        COMMENT ON TABLE parent_report_narratives IS 'Human-readable narrative reports generated from analytics data';
        COMMENT ON COLUMN parent_report_narratives.narrative_content IS 'Full human-readable report content in teacher-to-parent tone';
        COMMENT ON COLUMN parent_report_narratives.report_summary IS 'Brief executive summary of the report';
        COMMENT ON COLUMN parent_report_narratives.key_insights IS 'JSON array of key insights and highlights';
        COMMENT ON COLUMN parent_report_narratives.recommendations IS 'Array of actionable recommendations for parents';
        COMMENT ON COLUMN parent_report_narratives.tone_style IS 'Writing tone: teacher_to_parent, formal, casual, etc.';
        COMMENT ON COLUMN parent_report_narratives.word_count IS 'Total word count of narrative content';
        COMMENT ON COLUMN parent_report_narratives.reading_level IS 'Target reading level for accessibility';
      `);

      // Record the migration as completed
      await db.query(`
        INSERT INTO migration_history (migration_name)
        VALUES ('004_parent_report_narratives')
        ON CONFLICT (migration_name) DO NOTHING;
      `);

      logger.debug('‚úÖ Parent report narratives migration completed successfully!');
      logger.debug('üìä Parent report narratives table now supports:');
      logger.debug('   - Human-readable narrative content');
      logger.debug('   - Executive summaries for busy parents');
      logger.debug('   - Key insights and recommendations');
      logger.debug('   - Multiple tone styles and languages');
      logger.debug('   - Reading level optimization');
      logger.debug('   - Performance tracking and analytics');
    } else {
      logger.debug('‚úÖ Parent report narratives migration already applied');
    }

    // ============================================
    // MIGRATION: Drop Unused Tables (2025-01-06)
    // ============================================
    const cleanupCheck = await db.query(`
      SELECT 1 FROM migration_history WHERE migration_name = '005_cleanup_unused_tables'
    `);

    if (cleanupCheck.rows.length === 0) {
      logger.debug('üßπ Applying database cleanup migration...');
      logger.debug('üìä Removing 6 unused tables to simplify schema by 26%');

      try {
        // Drop unused parent reports system tables (not implemented)
        const unusedTables = [
          'mental_health_indicators',
          'report_metrics',
          'student_progress_history',
          'evaluations',
          'sessions_summaries',
          'progress'
        ];

        let droppedCount = 0;
        for (const tableName of unusedTables) {
          try {
            // Check if table exists first
            const tableExists = await db.query(`
              SELECT 1 FROM information_schema.tables
              WHERE table_name = $1 AND table_schema = 'public'
            `, [tableName]);

            if (tableExists.rows.length > 0) {
              await db.query(`DROP TABLE IF EXISTS ${tableName} CASCADE`);
              droppedCount++;
              logger.debug(`   ‚úÖ Dropped unused table: ${tableName}`);
            } else {
              logger.debug(`   ‚è≠Ô∏è  Table ${tableName} does not exist (skipped)`);
            }
          } catch (tableError) {
            logger.debug(`   ‚ö†Ô∏è  Could not drop ${tableName}: ${tableError.message}`);
          }
        }

        // Record the migration as completed
        await db.query(`
          INSERT INTO migration_history (migration_name)
          VALUES ('005_cleanup_unused_tables')
          ON CONFLICT (migration_name) DO NOTHING;
        `);

        logger.debug('‚úÖ Database cleanup migration completed successfully!');
        logger.debug(`üìä Cleanup results:`);
        logger.debug(`   - Tables dropped: ${droppedCount}/6`);
        logger.debug(`   - Schema complexity reduced by ~26%`);
        logger.debug(`   - Maintenance overhead reduced`);
        logger.debug('üìã Removed tables:');
        logger.debug('   - mental_health_indicators (not implemented)');
        logger.debug('   - report_metrics (use app logging instead)');
        logger.debug('   - student_progress_history (superseded by time-series queries)');
        logger.debug('   - evaluations (feature not implemented)');
        logger.debug('   - sessions_summaries (calculated on-demand)');
        logger.debug('   - progress (superseded by subject_progress + daily_subject_activities)');
      } catch (cleanupError) {
        logger.warn('‚ö†Ô∏è Cleanup migration warning (migration will continue):', cleanupError.message);
        // Record migration as complete to prevent retry loops
        await db.query(`
          INSERT INTO migration_history (migration_name)
          VALUES ('005_cleanup_unused_tables')
          ON CONFLICT (migration_name) DO NOTHING;
        `);
        logger.debug('‚úÖ Database cleanup migration marked as complete');
      }
    } else {
      logger.debug('‚úÖ Database cleanup migration already applied');
    }

    // ============================================
    // MIGRATION: Add raw_question_text column (2025-10-16)
    // ============================================
    const rawQuestionTextCheck = await db.query(`
      SELECT 1 FROM migration_history WHERE migration_name = '006_add_raw_question_text'
    `);

    if (rawQuestionTextCheck.rows.length === 0) {
      logger.debug('üìã Applying raw_question_text migration...');
      logger.debug('üìä Adding raw_question_text column for full original question storage');

      try {
        // Add raw_question_text column (allows NULL for backward compatibility)
        await db.query(`
          ALTER TABLE archived_questions
          ADD COLUMN IF NOT EXISTS raw_question_text TEXT;
        `);

        // Backfill existing records: copy question_text to raw_question_text
        await db.query(`
          UPDATE archived_questions
          SET raw_question_text = question_text
          WHERE raw_question_text IS NULL;
        `);

        // Check if index already exists before creating
        const indexCheck = await db.query(`
          SELECT indexname FROM pg_indexes
          WHERE indexname = 'idx_archived_questions_raw_text'
          AND tablename = 'archived_questions'
        `);

        if (indexCheck.rows.length === 0) {
          // Add index for searching raw question text
          await db.query(`
            CREATE INDEX idx_archived_questions_raw_text
            ON archived_questions USING gin(to_tsvector('english', raw_question_text));
          `);
          logger.debug('‚úÖ Created full-text search index on raw_question_text');
        } else {
          logger.debug('‚úÖ Index idx_archived_questions_raw_text already exists');
        }

        // Add comment to document the column
        await db.query(`
          COMMENT ON COLUMN archived_questions.raw_question_text IS 'Full original question text from image (before AI cleaning/simplification)';
        `);

        // Record the migration as completed
        await db.query(`
          INSERT INTO migration_history (migration_name)
          VALUES ('006_add_raw_question_text')
          ON CONFLICT (migration_name) DO NOTHING;
        `);

        logger.debug('‚úÖ raw_question_text migration completed successfully!');
        logger.debug('üìä archived_questions table now supports:');
        logger.debug('   - Full original question text from homework images');
        logger.debug('   - Preserves complete context and wording');
        logger.debug('   - Full-text search on raw question content');
      } catch (rawQuestionError) {
        logger.warn('‚ö†Ô∏è raw_question_text migration warning (migration will continue):', rawQuestionError.message);
        // Record migration as complete to prevent retry loops
        await db.query(`
          INSERT INTO migration_history (migration_name)
          VALUES ('006_add_raw_question_text')
          ON CONFLICT (migration_name) DO NOTHING;
        `);
        logger.debug('‚úÖ raw_question_text migration marked as complete');
      }
    } else {
      logger.debug('‚úÖ raw_question_text migration already applied');
    }

    // ============================================
    // MIGRATION: Add metadata column to sessions (2025-01-27)
    // ============================================
    const sessionsMetadataCheck = await db.query(`
      SELECT 1 FROM migration_history WHERE migration_name = '007_add_sessions_metadata_column'
    `);

    if (sessionsMetadataCheck.rows.length === 0) {
      logger.debug('üìã Applying sessions metadata migration...');
      logger.debug('üìä Adding metadata JSONB column to sessions table');

      try {
        // Check if sessions table exists first
        const sessionsTableCheck = await db.query(`
          SELECT 1 FROM information_schema.tables
          WHERE table_name = 'sessions'
        `);

        if (sessionsTableCheck.rows.length > 0) {
          // Add metadata column (allows NULL for backward compatibility)
          await db.query(`
            ALTER TABLE sessions
            ADD COLUMN IF NOT EXISTS metadata JSONB DEFAULT '{}'::jsonb;
          `);

          // Add comment to document the column's purpose
          await db.query(`
            COMMENT ON COLUMN sessions.metadata IS 'Flexible JSON storage for session preferences (language, theme, etc.)';
          `);

          // Check if index already exists before creating
          const indexCheck = await db.query(`
            SELECT indexname FROM pg_indexes
            WHERE indexname = 'idx_sessions_metadata'
            AND tablename = 'sessions'
          `);

          if (indexCheck.rows.length === 0) {
            // Add GIN index for efficient JSONB queries
            await db.query(`
              CREATE INDEX idx_sessions_metadata
              ON sessions USING gin(metadata);
            `);
            logger.debug('‚úÖ Created GIN index on sessions.metadata');
          } else {
            logger.debug('‚úÖ Index idx_sessions_metadata already exists');
          }

          logger.debug('‚úÖ sessions.metadata column added successfully');
        } else {
          logger.debug('‚ö†Ô∏è sessions table does not exist yet, skipping metadata migration');
        }

        // Record the migration as completed
        await db.query(`
          INSERT INTO migration_history (migration_name)
          VALUES ('007_add_sessions_metadata_column')
          ON CONFLICT (migration_name) DO NOTHING;
        `);

        logger.debug('‚úÖ sessions metadata migration completed successfully!');
        logger.debug('üìä sessions table now supports:');
        logger.debug('   - Flexible metadata storage (JSONB)');
        logger.debug('   - Language preferences per session');
        logger.debug('   - Future-proof extensibility for session context');
      } catch (sessionsMetadataError) {
        logger.warn('‚ö†Ô∏è sessions metadata migration warning (migration will continue):', sessionsMetadataError.message);
        // Record migration as complete to prevent retry loops
        await db.query(`
          INSERT INTO migration_history (migration_name)
          VALUES ('007_add_sessions_metadata_column')
          ON CONFLICT (migration_name) DO NOTHING;
        `);
        logger.debug('‚úÖ sessions metadata migration marked as complete');
      }
    } else {
      logger.debug('‚úÖ sessions metadata migration already applied');
    }

    // ============================================
    // MIGRATION: COPPA Consent Management (2025-01-27)
    // ============================================
    const coppaConsentCheck = await db.query(`
      SELECT 1 FROM migration_history WHERE migration_name = '008_add_coppa_consent_management'
    `);

    if (coppaConsentCheck.rows.length === 0) {
      logger.debug('üìã Applying COPPA consent management migration...');
      logger.debug('üìä Adding parental consent tracking for COPPA compliance (users under 13)');

      try {
        // Create parental consents table
        await db.query(`
          CREATE TABLE IF NOT EXISTS parental_consents (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),

            -- Child user information
            child_user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
            child_email VARCHAR(255) NOT NULL,
            child_date_of_birth DATE NOT NULL,
            child_age_at_consent INTEGER NOT NULL,

            -- Parent information
            parent_email VARCHAR(255) NOT NULL,
            parent_name VARCHAR(255) NOT NULL,
            parent_relationship VARCHAR(50),

            -- Consent details
            consent_method VARCHAR(50) NOT NULL,
            consent_status VARCHAR(50) NOT NULL DEFAULT 'pending',
            consent_granted_at TIMESTAMP WITH TIME ZONE,
            consent_expires_at TIMESTAMP WITH TIME ZONE,

            -- Verification
            verification_code VARCHAR(6),
            verification_code_expires_at TIMESTAMP WITH TIME ZONE,
            verification_attempts INTEGER DEFAULT 0,
            verified_at TIMESTAMP WITH TIME ZONE,

            -- Audit trail
            request_ip VARCHAR(45),
            request_user_agent TEXT,
            request_metadata JSONB,
            consent_scope JSONB DEFAULT '{"data_collection": true, "data_sharing": false, "marketing": false}'::jsonb,

            created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
            updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
            revoked_at TIMESTAMP WITH TIME ZONE,
            revoked_reason TEXT,

            CONSTRAINT unique_child_active_consent UNIQUE (child_user_id, consent_status)
          );
        `);

        // Create indexes for parental_consents
        await db.query(`
          CREATE INDEX IF NOT EXISTS idx_parental_consents_child_user ON parental_consents(child_user_id);
          CREATE INDEX IF NOT EXISTS idx_parental_consents_parent_email ON parental_consents(parent_email);
          CREATE INDEX IF NOT EXISTS idx_parental_consents_status ON parental_consents(consent_status);
          CREATE INDEX IF NOT EXISTS idx_parental_consents_expires_at ON parental_consents(consent_expires_at) WHERE consent_status = 'granted';
        `);

        // Create age_verifications table
        await db.query(`
          CREATE TABLE IF NOT EXISTS age_verifications (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),

            user_id UUID REFERENCES users(id) ON DELETE CASCADE,
            email VARCHAR(255) NOT NULL,

            provided_date_of_birth DATE,
            calculated_age INTEGER,
            is_minor BOOLEAN,
            is_coppa_protected BOOLEAN,

            verification_method VARCHAR(50) NOT NULL,
            verification_status VARCHAR(50) NOT NULL DEFAULT 'pending',

            verification_ip VARCHAR(45),
            verification_user_agent TEXT,
            verification_metadata JSONB,

            created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
            verified_at TIMESTAMP WITH TIME ZONE,
            notes TEXT
          );
        `);

        // Create indexes for age_verifications
        await db.query(`
          CREATE INDEX IF NOT EXISTS idx_age_verifications_user_id ON age_verifications(user_id);
          CREATE INDEX IF NOT EXISTS idx_age_verifications_email ON age_verifications(email);
        `);

        // Create consent_audit_log table
        await db.query(`
          CREATE TABLE IF NOT EXISTS consent_audit_log (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),

            consent_id UUID REFERENCES parental_consents(id) ON DELETE CASCADE,

            action_type VARCHAR(50) NOT NULL,
            action_by VARCHAR(50) NOT NULL,
            action_by_email VARCHAR(255),

            previous_state JSONB,
            new_state JSONB,

            action_reason TEXT,
            action_ip VARCHAR(45),
            action_metadata JSONB,

            created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
          );
        `);

        // Create indexes for consent_audit_log
        await db.query(`
          CREATE INDEX IF NOT EXISTS idx_consent_audit_log_consent_id ON consent_audit_log(consent_id);
          CREATE INDEX IF NOT EXISTS idx_consent_audit_log_created_at ON consent_audit_log(created_at DESC);
        `);

        // Add COPPA-related columns to users table
        await db.query(`
          DO $$
          BEGIN
            IF NOT EXISTS (
              SELECT 1 FROM information_schema.columns
              WHERE table_name = 'users' AND column_name = 'requires_parental_consent'
            ) THEN
              ALTER TABLE users ADD COLUMN requires_parental_consent BOOLEAN DEFAULT FALSE;
            END IF;

            IF NOT EXISTS (
              SELECT 1 FROM information_schema.columns
              WHERE table_name = 'users' AND column_name = 'parental_consent_status'
            ) THEN
              ALTER TABLE users ADD COLUMN parental_consent_status VARCHAR(50) DEFAULT 'not_required';
            END IF;

            IF NOT EXISTS (
              SELECT 1 FROM information_schema.columns
              WHERE table_name = 'users' AND column_name = 'account_restricted'
            ) THEN
              ALTER TABLE users ADD COLUMN account_restricted BOOLEAN DEFAULT FALSE;
            END IF;

            IF NOT EXISTS (
              SELECT 1 FROM information_schema.columns
              WHERE table_name = 'users' AND column_name = 'restriction_reason'
            ) THEN
              ALTER TABLE users ADD COLUMN restriction_reason TEXT;
            END IF;
          END$$;
        `);

        // Create indexes for users table
        await db.query(`
          CREATE INDEX IF NOT EXISTS idx_users_requires_consent ON users(requires_parental_consent) WHERE requires_parental_consent = true;
          CREATE INDEX IF NOT EXISTS idx_users_consent_status ON users(parental_consent_status);
        `);

        // Create helper functions
        await db.query(`
          CREATE OR REPLACE FUNCTION is_coppa_protected(user_date_of_birth DATE)
          RETURNS BOOLEAN AS $$
          BEGIN
            RETURN EXTRACT(YEAR FROM AGE(NOW(), user_date_of_birth)) < 13;
          END;
          $$ LANGUAGE plpgsql IMMUTABLE;

          CREATE OR REPLACE FUNCTION calculate_age(date_of_birth DATE)
          RETURNS INTEGER AS $$
          BEGIN
            RETURN EXTRACT(YEAR FROM AGE(NOW(), date_of_birth))::INTEGER;
          END;
          $$ LANGUAGE plpgsql IMMUTABLE;
        `);

        // Create trigger for auto-updating parental consent requirement
        await db.query(`
          CREATE OR REPLACE FUNCTION update_parental_consent_requirement()
          RETURNS TRIGGER AS $$
          BEGIN
            IF NEW.date_of_birth IS NOT NULL THEN
              UPDATE users
              SET
                requires_parental_consent = is_coppa_protected(NEW.date_of_birth),
                parental_consent_status = CASE
                  WHEN is_coppa_protected(NEW.date_of_birth) THEN 'required'
                  ELSE 'not_required'
                END
              WHERE id = NEW.user_id;
            END IF;
            RETURN NEW;
          END;
          $$ LANGUAGE plpgsql;
        `);

        // Create trigger on profiles table (if it exists)
        const profilesTableCheck = await db.query(`
          SELECT 1 FROM information_schema.tables WHERE table_name = 'profiles'
        `);

        if (profilesTableCheck.rows.length > 0) {
          await db.query(`
            DROP TRIGGER IF EXISTS trigger_update_consent_requirement ON profiles;
            CREATE TRIGGER trigger_update_consent_requirement
              AFTER INSERT OR UPDATE OF date_of_birth ON profiles
              FOR EACH ROW
              EXECUTE FUNCTION update_parental_consent_requirement();
          `);
          logger.debug('‚úÖ Created trigger on profiles table for auto-updating consent requirements');
        }

        // Create function to auto-expire consents
        await db.query(`
          CREATE OR REPLACE FUNCTION expire_old_consents()
          RETURNS void AS $$
          BEGIN
            UPDATE parental_consents
            SET
              consent_status = 'expired',
              updated_at = NOW()
            WHERE
              consent_status = 'granted'
              AND consent_expires_at < NOW();

            UPDATE users u
            SET
              account_restricted = true,
              restriction_reason = 'Parental consent expired'
            WHERE
              requires_parental_consent = true
              AND EXISTS (
                SELECT 1 FROM parental_consents pc
                WHERE pc.child_user_id = u.id
                AND pc.consent_status = 'expired'
              );
          END;
          $$ LANGUAGE plpgsql;
        `);

        // Add table comments
        await db.query(`
          COMMENT ON TABLE parental_consents IS 'COPPA compliance: Tracks parental consent for users under 13';
          COMMENT ON TABLE age_verifications IS 'Audit log of age verification attempts for COPPA compliance';
          COMMENT ON TABLE consent_audit_log IS 'Complete audit trail of all consent-related actions';
          COMMENT ON COLUMN parental_consents.consent_scope IS 'JSONB object defining what data collection/sharing is consented to';
          COMMENT ON COLUMN parental_consents.consent_expires_at IS 'COPPA best practice: Re-verify consent annually';
        `);

        // Record the migration as completed
        await db.query(`
          INSERT INTO migration_history (migration_name)
          VALUES ('008_add_coppa_consent_management')
          ON CONFLICT (migration_name) DO NOTHING;
        `);

        logger.debug('‚úÖ COPPA consent management migration completed successfully!');
        logger.debug('üìä COPPA consent system now supports:');
        logger.debug('   - Parental consent requests with email verification');
        logger.debug('   - Age verification and COPPA protection (users under 13)');
        logger.debug('   - 6-digit verification codes with 24-hour expiration');
        logger.debug('   - Complete audit trail of all consent actions');
        logger.debug('   - Account restrictions for users without consent');
        logger.debug('   - Annual consent renewal (COPPA best practice)');
        logger.debug('   - Automatic consent expiration and account locking');
      } catch (coppaConsentError) {
        logger.warn('‚ö†Ô∏è COPPA consent management migration warning (migration will continue):', coppaConsentError.message);
        // Record migration as complete to prevent retry loops
        await db.query(`
          INSERT INTO migration_history (migration_name)
          VALUES ('008_add_coppa_consent_management')
          ON CONFLICT (migration_name) DO NOTHING;
        `);
        logger.debug('‚úÖ COPPA consent management migration marked as complete');
      }
    } else {
      logger.debug('‚úÖ COPPA consent management migration already applied');
    }

    // ============================================
    // MIGRATION: Fix avatar_id and enum types (2025-01-27)
    // ============================================
    const avatarIdFixCheck = await db.query(`
      SELECT 1 FROM migration_history WHERE migration_name = '009_fix_avatar_id_and_enums'
    `);

    if (avatarIdFixCheck.rows.length === 0) {
      logger.debug('üìã Applying avatar_id and enum types fix migration...');

      try {
        // Add avatar_id column to profiles table if it doesn't exist
        await db.query(`
          DO $$
          BEGIN
            IF NOT EXISTS (
              SELECT 1 FROM information_schema.columns
              WHERE table_name = 'profiles' AND column_name = 'avatar_id'
            ) THEN
              ALTER TABLE profiles ADD COLUMN avatar_id VARCHAR(50);
              COMMENT ON COLUMN profiles.avatar_id IS 'Reference to selected avatar (e.g., avatar_1, avatar_2, etc.)';
            END IF;
          END $$;
        `);
        logger.debug('‚úÖ Added avatar_id column to profiles table');

        // Create enum types with proper error handling to avoid duplicate errors
        await db.query(`
          DO $$
          BEGIN
            -- Create subject_category enum if it doesn't exist
            IF NOT EXISTS (SELECT 1 FROM pg_type WHERE typname = 'subject_category') THEN
              CREATE TYPE subject_category AS ENUM (
                'Mathematics',
                'Physics',
                'Chemistry',
                'Biology',
                'English',
                'History',
                'Geography',
                'Computer Science',
                'Foreign Language',
                'Arts',
                'Other'
              );
            END IF;

            -- Create difficulty_level enum if it doesn't exist
            IF NOT EXISTS (SELECT 1 FROM pg_type WHERE typname = 'difficulty_level') THEN
              CREATE TYPE difficulty_level AS ENUM (
                'Beginner',
                'Intermediate',
                'Advanced',
                'Expert'
              );
            END IF;
          END $$;
        `);
        logger.debug('‚úÖ Created enum types (subject_category, difficulty_level)');

        // Record migration completion
        await db.query(`
          INSERT INTO migration_history (migration_name)
          VALUES ('009_fix_avatar_id_and_enums')
          ON CONFLICT (migration_name) DO NOTHING;
        `);
        logger.debug('‚úÖ Avatar_id and enum types fix migration marked as complete');
      } catch (migrationError) {
        logger.error('‚ùå Error in avatar_id and enum types fix migration:', migrationError.message);
        // Continue even if this fails - non-critical migration
      }
    } else {
      logger.debug('‚úÖ Avatar_id and enum types fix migration already applied');
    }

    // ============================================
    // MIGRATION: Fix profiles table user_id column (2025-01-27)
    // ============================================
    const profilesUserIdCheck = await db.query(`
      SELECT 1 FROM migration_history WHERE migration_name = '010_fix_profiles_user_id'
    `);

    if (profilesUserIdCheck.rows.length === 0) {
      logger.debug('üìã Applying profiles user_id fix migration...');

      try {
        // Check if profiles table exists
        const profilesTableExists = await db.query(`
          SELECT EXISTS (
            SELECT FROM information_schema.tables
            WHERE table_name = 'profiles'
          );
        `);

        if (profilesTableExists.rows[0].exists) {
          // Add user_id column if it doesn't exist using PostgreSQL's built-in IF NOT EXISTS
          await db.query(`
            DO $$
            BEGIN
              -- Try to add user_id column (will be skipped if exists due to exception handling)
              BEGIN
                ALTER TABLE profiles ADD COLUMN user_id UUID;
                RAISE NOTICE '‚úÖ Added user_id column to profiles table';
              EXCEPTION
                WHEN duplicate_column THEN
                  RAISE NOTICE '‚úÖ user_id column already exists in profiles table';
              END;

              -- Populate user_id from email if any rows have NULL user_id
              UPDATE profiles p
              SET user_id = u.id
              FROM users u
              WHERE p.email = u.email AND p.user_id IS NULL;

              -- Make user_id NOT NULL if it's currently nullable
              BEGIN
                ALTER TABLE profiles ALTER COLUMN user_id SET NOT NULL;
              EXCEPTION
                WHEN OTHERS THEN
                  RAISE NOTICE 'user_id already NOT NULL or constraint exists';
              END;

              -- Try to add foreign key constraint
              BEGIN
                ALTER TABLE profiles
                ADD CONSTRAINT fk_profiles_user_id
                FOREIGN KEY (user_id) REFERENCES users(id) ON DELETE CASCADE;
              EXCEPTION
                WHEN duplicate_object THEN
                  RAISE NOTICE 'Foreign key constraint already exists';
              END;

              -- Create index for better performance
              CREATE INDEX IF NOT EXISTS idx_profiles_user_id ON profiles(user_id);

            END $$;
          `);
          logger.debug('‚úÖ Profiles user_id column verified/added');
        } else {
          logger.debug('‚ÑπÔ∏è Profiles table does not exist yet - will be created with correct schema');
        }

        // Record migration completion
        await db.query(`
          INSERT INTO migration_history (migration_name)
          VALUES ('010_fix_profiles_user_id')
          ON CONFLICT (migration_name) DO NOTHING;
        `);
        logger.debug('‚úÖ Profiles user_id fix migration marked as complete');
      } catch (migrationError) {
        logger.error('‚ùå Error in profiles user_id fix migration:', migrationError.message);
        // Migration is idempotent with exception handling, so errors are unexpected
        // Continue - migration will retry on next deployment if needed
      }
    } else {
      logger.debug('‚úÖ Profiles user_id fix migration already applied');
    }

    // ============================================
    // MIGRATION: Add Data Retention Policy (2025-01-15)
    // ============================================
    const dataRetentionCheck = await db.query(`
      SELECT 1 FROM migration_history WHERE migration_name = '011_add_data_retention_policy'
    `);

    if (dataRetentionCheck.rows.length === 0) {
      logger.debug('üìã Applying data retention policy migration...');

      try {
        // Step 1: Add deleted_at and retention_expires_at columns to archived_conversations_new
        await db.query(`
          ALTER TABLE archived_conversations_new
          ADD COLUMN IF NOT EXISTS deleted_at TIMESTAMP,
          ADD COLUMN IF NOT EXISTS retention_expires_at TIMESTAMP DEFAULT (CURRENT_TIMESTAMP + INTERVAL '90 days');
        `);

        // Step 2: Create indexes for efficient cleanup queries (separate queries to avoid constraint issues)
        try {
          await db.query(`
            CREATE INDEX IF NOT EXISTS idx_archived_conversations_retention
            ON archived_conversations_new(retention_expires_at)
            WHERE deleted_at IS NULL;
          `);
        } catch (indexError) {
          if (indexError.code !== '42P07') { // Ignore "already exists" errors
            logger.warn('‚ö†Ô∏è Index idx_archived_conversations_retention creation warning:', indexError.message);
          }
        }

        try {
          await db.query(`
            CREATE INDEX IF NOT EXISTS idx_archived_conversations_deleted
            ON archived_conversations_new(deleted_at)
            WHERE deleted_at IS NOT NULL;
          `);
        } catch (indexError) {
          if (indexError.code !== '42P07') {
            logger.warn('‚ö†Ô∏è Index idx_archived_conversations_deleted creation warning:', indexError.message);
          }
        }

        // Step 3: Add retention policy to question_sessions table (if exists)
        const questionSessionsExists = await db.query(`
          SELECT EXISTS (
            SELECT FROM information_schema.tables
            WHERE table_name = 'question_sessions'
          );
        `);

        if (questionSessionsExists.rows[0].exists) {
          await db.query(`
            ALTER TABLE question_sessions
            ADD COLUMN IF NOT EXISTS deleted_at TIMESTAMP,
            ADD COLUMN IF NOT EXISTS retention_expires_at TIMESTAMP DEFAULT (CURRENT_TIMESTAMP + INTERVAL '90 days');
          `);

          try {
            await db.query(`
              CREATE INDEX IF NOT EXISTS idx_question_sessions_retention
              ON question_sessions(retention_expires_at)
              WHERE deleted_at IS NULL;
            `);
          } catch (indexError) {
            if (indexError.code !== '42P07') {
              logger.warn('‚ö†Ô∏è Index idx_question_sessions_retention creation warning:', indexError.message);
            }
          }
        }

        // Step 4: Add retention policy to sessions table
        await db.query(`
          ALTER TABLE sessions
          ADD COLUMN IF NOT EXISTS deleted_at TIMESTAMP,
          ADD COLUMN IF NOT EXISTS retention_expires_at TIMESTAMP DEFAULT (CURRENT_TIMESTAMP + INTERVAL '90 days');
        `);

        try {
          await db.query(`
            CREATE INDEX IF NOT EXISTS idx_sessions_retention
            ON sessions(retention_expires_at)
            WHERE deleted_at IS NULL;
          `);
        } catch (indexError) {
          if (indexError.code !== '42P07') {
            logger.warn('‚ö†Ô∏è Index idx_sessions_retention creation warning:', indexError.message);
          }
        }

        // Step 5: Create function to soft delete expired data
        await db.query(`
          CREATE OR REPLACE FUNCTION soft_delete_expired_data()
          RETURNS TABLE(
            table_name TEXT,
            deleted_count BIGINT
          ) AS $$
          BEGIN
            -- Soft delete expired conversations
            UPDATE archived_conversations_new
            SET deleted_at = CURRENT_TIMESTAMP
            WHERE retention_expires_at < CURRENT_TIMESTAMP
              AND deleted_at IS NULL;

            table_name := 'archived_conversations_new';
            deleted_count := (SELECT COUNT(*) FROM archived_conversations_new WHERE deleted_at >= CURRENT_TIMESTAMP - INTERVAL '1 second');
            RETURN NEXT;

            -- Soft delete expired question sessions (if table exists)
            IF EXISTS (SELECT FROM information_schema.tables WHERE information_schema.tables.table_name = 'question_sessions') THEN
              UPDATE question_sessions
              SET deleted_at = CURRENT_TIMESTAMP
              WHERE retention_expires_at < CURRENT_TIMESTAMP
                AND deleted_at IS NULL;

              table_name := 'question_sessions';
              deleted_count := (SELECT COUNT(*) FROM question_sessions WHERE deleted_at >= CURRENT_TIMESTAMP - INTERVAL '1 second');
              RETURN NEXT;
            END IF;

            -- Soft delete expired sessions
            UPDATE sessions
            SET deleted_at = CURRENT_TIMESTAMP
            WHERE retention_expires_at < CURRENT_TIMESTAMP
              AND deleted_at IS NULL;

            table_name := 'sessions';
            deleted_count := (SELECT COUNT(*) FROM sessions WHERE deleted_at >= CURRENT_TIMESTAMP - INTERVAL '1 second');
            RETURN NEXT;
          END;
          $$ LANGUAGE plpgsql;
        `);

        // Step 6: Create function to hard delete soft-deleted data after 30 days
        await db.query(`
          CREATE OR REPLACE FUNCTION hard_delete_old_soft_deleted()
          RETURNS TABLE(
            table_name TEXT,
            purged_count BIGINT
          ) AS $$
          BEGIN
            -- Hard delete conversations deleted > 30 days ago
            DELETE FROM archived_conversations_new
            WHERE deleted_at < (CURRENT_TIMESTAMP - INTERVAL '30 days');

            table_name := 'archived_conversations_new';
            GET DIAGNOSTICS purged_count = ROW_COUNT;
            RETURN NEXT;

            -- Hard delete question sessions deleted > 30 days ago (if table exists)
            IF EXISTS (SELECT FROM information_schema.tables WHERE information_schema.tables.table_name = 'question_sessions') THEN
              DELETE FROM question_sessions
              WHERE deleted_at < (CURRENT_TIMESTAMP - INTERVAL '30 days');

              table_name := 'question_sessions';
              GET DIAGNOSTICS purged_count = ROW_COUNT;
              RETURN NEXT;
            END IF;

            -- Hard delete sessions deleted > 30 days ago
            DELETE FROM sessions
            WHERE deleted_at < (CURRENT_TIMESTAMP - INTERVAL '30 days');

            table_name := 'sessions';
            GET DIAGNOSTICS purged_count = ROW_COUNT;
            RETURN NEXT;
          END;
          $$ LANGUAGE plpgsql;
        `);

        // Step 7: Update existing rows to have retention expiration date
        await db.query(`
          UPDATE archived_conversations_new
          SET retention_expires_at = archived_date + INTERVAL '90 days'
          WHERE retention_expires_at IS NULL;
        `);

        await db.query(`
          UPDATE sessions
          SET retention_expires_at = start_time + INTERVAL '90 days'
          WHERE retention_expires_at IS NULL;
        `);

        if (questionSessionsExists.rows[0].exists) {
          await db.query(`
            UPDATE question_sessions
            SET retention_expires_at = created_at + INTERVAL '90 days'
            WHERE retention_expires_at IS NULL;
          `);
        }

        // Step 8: Create views for non-deleted data
        await db.query(`
          CREATE OR REPLACE VIEW active_conversations AS
          SELECT *
          FROM archived_conversations_new
          WHERE deleted_at IS NULL;
        `);

        await db.query(`
          CREATE OR REPLACE VIEW active_sessions AS
          SELECT *
          FROM sessions
          WHERE deleted_at IS NULL;
        `);

        await db.query(`
          COMMENT ON VIEW active_conversations IS 'Only shows non-deleted conversations for GDPR compliance';
        `);

        await db.query(`
          COMMENT ON VIEW active_sessions IS 'Only shows non-deleted sessions for GDPR compliance';
        `);

        if (questionSessionsExists.rows[0].exists) {
          await db.query(`
            CREATE OR REPLACE VIEW active_question_sessions AS
            SELECT *
            FROM question_sessions
            WHERE deleted_at IS NULL;
          `);

          await db.query(`
            COMMENT ON VIEW active_question_sessions IS 'Only shows non-deleted question sessions for GDPR compliance';
          `);
        }

        // Record migration completion
        await db.query(`
          INSERT INTO migration_history (migration_name)
          VALUES ('011_add_data_retention_policy')
          ON CONFLICT (migration_name) DO NOTHING;
        `);

        logger.debug('‚úÖ Data retention policy migration completed successfully!');
      } catch (migrationError) {
        logger.error('‚ùå Error in data retention policy migration:', migrationError.message);
        logger.error(migrationError);
        // Don't throw - allow app to continue, will retry on next restart
      }
    } else {
      logger.debug('‚úÖ Data retention policy migration already applied');
    }

    // ============================================
    // MIGRATION: Passive Reports Tables (2025-01-20)
    // ============================================
    const passiveReportsCheck = await db.query(`
      SELECT tablename
      FROM pg_tables
      WHERE schemaname = 'public' AND tablename IN ('parent_report_batches', 'passive_reports', 'report_notification_preferences')
    `);

    if (passiveReportsCheck.rows.length < 3) {
      logger.debug('üìã Running passive reports tables migration...');

      try {
        // Create parent_report_batches table
        await db.query(`
          CREATE TABLE IF NOT EXISTS parent_report_batches (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            user_id UUID NOT NULL,
            period VARCHAR(20) NOT NULL, -- 'weekly' | 'monthly'
            start_date DATE NOT NULL,
            end_date DATE NOT NULL,
            generated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
            status VARCHAR(20) DEFAULT 'completed', -- 'pending' | 'processing' | 'completed' | 'failed'
            generation_time_ms INTEGER,

            -- Quick metrics for card display
            overall_grade VARCHAR(2), -- 'A+', 'B', etc.
            overall_accuracy FLOAT,
            question_count INTEGER,
            study_time_minutes INTEGER,
            current_streak INTEGER,

            -- Trends
            accuracy_trend VARCHAR(20), -- 'improving' | 'stable' | 'declining'
            activity_trend VARCHAR(20), -- 'increasing' | 'stable' | 'decreasing'

            -- Summary text
            one_line_summary TEXT,

            -- Metadata
            metadata JSONB,

            CONSTRAINT unique_user_period_date UNIQUE (user_id, period, start_date)
          );

          -- Indexes for efficient querying
          CREATE INDEX IF NOT EXISTS idx_report_batches_user_date ON parent_report_batches(user_id, start_date DESC);
          CREATE INDEX IF NOT EXISTS idx_report_batches_status ON parent_report_batches(status) WHERE status != 'completed';
          CREATE INDEX IF NOT EXISTS idx_report_batches_generated ON parent_report_batches(generated_at DESC);

          -- Comments
          COMMENT ON TABLE parent_report_batches IS 'Stores metadata for scheduled parent report batches (weekly/monthly)';
          COMMENT ON COLUMN parent_report_batches.period IS 'Report period: weekly or monthly';
          COMMENT ON COLUMN parent_report_batches.status IS 'Generation status: pending, processing, completed, failed';
          COMMENT ON COLUMN parent_report_batches.overall_grade IS 'Letter grade (A+, A, B+, etc.) for quick display';
        `);

        // Create passive_reports table
        await db.query(`
          CREATE TABLE IF NOT EXISTS passive_reports (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            batch_id UUID NOT NULL REFERENCES parent_report_batches(id) ON DELETE CASCADE,
            report_type VARCHAR(50) NOT NULL, -- 'academic_performance', 'learning_behavior', etc.

            -- Report content
            narrative_content TEXT NOT NULL,
            key_insights JSONB, -- Array of insight objects
            recommendations JSONB, -- Array of recommendation objects
            visual_data JSONB, -- Chart data for rendering

            -- Metadata
            word_count INTEGER,
            generation_time_ms INTEGER,
            ai_model_used VARCHAR(50) DEFAULT 'gpt-4o-mini',

            generated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),

            CONSTRAINT unique_batch_type UNIQUE (batch_id, report_type)
          );

          -- Indexes for passive_reports
          CREATE INDEX IF NOT EXISTS idx_passive_reports_batch ON passive_reports(batch_id);
          CREATE INDEX IF NOT EXISTS idx_passive_reports_type ON passive_reports(report_type);

          -- Comments
          COMMENT ON TABLE passive_reports IS 'Stores individual reports within a batch (8 report types per batch)';
          COMMENT ON COLUMN passive_reports.report_type IS 'One of 8 report types: executive_summary, academic_performance, learning_behavior, motivation_emotional, progress_trajectory, social_learning, risk_opportunity, action_plan';
          COMMENT ON COLUMN passive_reports.visual_data IS 'JSON data for generating charts/graphs in the report';
        `);

        // Create report_notification_preferences table
        await db.query(`
          CREATE TABLE IF NOT EXISTS report_notification_preferences (
            user_id UUID PRIMARY KEY,
            weekly_reports_enabled BOOLEAN DEFAULT true,
            monthly_reports_enabled BOOLEAN DEFAULT true,
            push_notifications_enabled BOOLEAN DEFAULT true,
            email_digest_enabled BOOLEAN DEFAULT false,
            email_address VARCHAR(255),
            created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
            updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
          );

          -- Comments
          COMMENT ON TABLE report_notification_preferences IS 'User preferences for report notifications';
        `);

        // Record migration
        await db.query(`
          INSERT INTO migration_history (migration_name)
          VALUES ('012_add_passive_reports_tables')
          ON CONFLICT (migration_name) DO NOTHING;
        `);

        logger.debug('‚úÖ Passive reports tables migration completed successfully!');
        logger.debug('üìä Created tables:');
        logger.debug('   - parent_report_batches (report batch metadata)');
        logger.debug('   - passive_reports (individual report content)');
        logger.debug('   - report_notification_preferences (user preferences)');
      } catch (migrationError) {
        logger.error('‚ùå Error in passive reports migration:', migrationError.message);
        logger.error(migrationError);
        // Don't throw - allow app to continue, will retry on next restart
      }
    } else {
      logger.debug('‚úÖ Passive reports tables migration already applied');
    }

  } catch (error) {
    logger.error('‚ùå Database migration failed:', error);
    // Don't throw - let the app continue with what it has
  }
}

async function createInlineSchema() {
  const schema = `
    -- Users table for authentication
    CREATE TABLE IF NOT EXISTS users (
      id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
      email VARCHAR(255) UNIQUE NOT NULL,
      name VARCHAR(255) NOT NULL,
      password_hash VARCHAR(255), -- For email/password auth
      profile_image_url TEXT,
      auth_provider VARCHAR(50) DEFAULT 'email', -- 'email', 'google', 'apple'
      google_id VARCHAR(255) UNIQUE,
      apple_id VARCHAR(255) UNIQUE,
      email_verified BOOLEAN DEFAULT false,
      is_active BOOLEAN DEFAULT true,
      last_login_at TIMESTAMP WITH TIME ZONE,
      created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
      updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
    );

    -- User sessions table for authentication tokens
    CREATE TABLE IF NOT EXISTS user_sessions (
      id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
      user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
      token_hash VARCHAR(64) NOT NULL UNIQUE,
      expires_at TIMESTAMP WITH TIME ZONE NOT NULL,
      device_info JSONB,
      ip_address INET,
      created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
    );

    -- Email verifications table for email verification codes
    CREATE TABLE IF NOT EXISTS email_verifications (
      id SERIAL PRIMARY KEY,
      email VARCHAR(255) NOT NULL UNIQUE,
      code VARCHAR(6) NOT NULL,
      name VARCHAR(255) NOT NULL,
      attempts INTEGER DEFAULT 0,
      created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
      expires_at TIMESTAMP WITH TIME ZONE NOT NULL
    );

    -- Email verifications indexes
    CREATE INDEX IF NOT EXISTS idx_email_verifications_email ON email_verifications(email);
    CREATE INDEX IF NOT EXISTS idx_email_verifications_expires ON email_verifications(expires_at);

    -- Enhanced profiles table for comprehensive user profile management
    CREATE TABLE IF NOT EXISTS profiles (
      id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
      user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
      email VARCHAR(255) UNIQUE NOT NULL,
      
      -- Basic Information
      role VARCHAR(50) DEFAULT 'student', -- 'student', 'parent', 'teacher', 'admin'
      parent_id UUID REFERENCES users(id),
      first_name VARCHAR(255),
      last_name VARCHAR(255),
      display_name VARCHAR(255), -- Optional custom display name
      avatar_id VARCHAR(50), -- Reference to selected avatar (e.g., avatar_1, avatar_2, etc.)
      
      -- Academic Information
      grade_level VARCHAR(50),
      school VARCHAR(255),
      school_district VARCHAR(255),
      academic_year VARCHAR(20), -- e.g., '2024-2025'
      
      -- Personal Information
      date_of_birth DATE,
      timezone VARCHAR(50) DEFAULT 'UTC',
      language_preference VARCHAR(10) DEFAULT 'en',
      
      -- Learning Preferences
      learning_style VARCHAR(50), -- 'visual', 'auditory', 'kinesthetic', 'reading'
      difficulty_preference VARCHAR(20) DEFAULT 'adaptive', -- 'easy', 'medium', 'hard', 'adaptive'
      favorite_subjects TEXT[], -- Array of preferred subjects
      
      -- Accessibility & Preferences
      accessibility_needs TEXT[], -- Array of accessibility requirements
      voice_enabled BOOLEAN DEFAULT true,
      auto_speak_responses BOOLEAN DEFAULT false,
      preferred_voice_type VARCHAR(50) DEFAULT 'friendly',
      
      -- Privacy & Parental Controls
      privacy_level VARCHAR(20) DEFAULT 'standard', -- 'minimal', 'standard', 'full'
      parental_controls_enabled BOOLEAN DEFAULT false,
      data_sharing_consent BOOLEAN DEFAULT false,
      
      -- Profile Completion & Status
      profile_completion_percentage INTEGER DEFAULT 0,
      onboarding_completed BOOLEAN DEFAULT false,
      is_active BOOLEAN DEFAULT true,
      
      -- Metadata
      last_profile_update TIMESTAMP WITH TIME ZONE,
      created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
      updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
      
      -- Constraints
      CONSTRAINT valid_role CHECK (role IN ('student', 'parent', 'teacher', 'admin')),
      CONSTRAINT valid_difficulty CHECK (difficulty_preference IN ('easy', 'medium', 'hard', 'adaptive')),
      CONSTRAINT valid_privacy CHECK (privacy_level IN ('minimal', 'standard', 'full')),
      CONSTRAINT valid_completion_percentage CHECK (profile_completion_percentage >= 0 AND profile_completion_percentage <= 100)
    );

    -- Sessions table for study sessions
    CREATE TABLE IF NOT EXISTS sessions (
      id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
      user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
      parent_id UUID REFERENCES users(id),
      session_type VARCHAR(50) DEFAULT 'homework',
      title VARCHAR(200),
      description TEXT,
      subject VARCHAR(100),
      status VARCHAR(50) DEFAULT 'active',
      start_time TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
      end_time TIMESTAMP WITH TIME ZONE,
      created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
      updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
    );

    -- Questions table
    CREATE TABLE IF NOT EXISTS questions (
      id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
      user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
      session_id UUID REFERENCES sessions(id) ON DELETE CASCADE,
      image_data BYTEA,
      image_url TEXT,
      question_text TEXT,
      subject VARCHAR(100),
      topic VARCHAR(255),
      difficulty_level INTEGER DEFAULT 3,
      ai_solution JSONB,
      explanation TEXT,
      confidence_score FLOAT DEFAULT 0.0,
      processing_time FLOAT DEFAULT 0.0,
      created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
      updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
    );

    -- Conversations table already created in railway-schema.sql
    -- Skipping duplicate creation to avoid PostgreSQL type conflicts

    -- Evaluations table
    CREATE TABLE IF NOT EXISTS evaluations (
      id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
      session_id UUID REFERENCES sessions(id) ON DELETE CASCADE,
      question_id UUID REFERENCES questions(id) ON DELETE CASCADE,
      user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
      student_answer TEXT,
      ai_feedback JSONB,
      score FLOAT,
      max_score FLOAT DEFAULT 100.0,
      time_spent INTEGER, -- in seconds
      is_correct BOOLEAN,
      rubric JSONB,
      created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
    );

    -- Progress table
    CREATE TABLE IF NOT EXISTS progress (
      id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
      user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
      subject VARCHAR(100) NOT NULL,
      topic VARCHAR(255) NOT NULL,
      skill_level INTEGER DEFAULT 1,
      mastery_level FLOAT DEFAULT 0.0,
      questions_attempted INTEGER DEFAULT 0,
      questions_correct INTEGER DEFAULT 0,
      total_time_spent INTEGER DEFAULT 0, -- in seconds
      last_practiced_at TIMESTAMP WITH TIME ZONE,
      streak_count INTEGER DEFAULT 0,
      created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
      updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
      UNIQUE(user_id, subject, topic)
    );

    -- Sessions summaries table
    CREATE TABLE IF NOT EXISTS sessions_summaries (
      id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
      session_id UUID UNIQUE REFERENCES sessions(id) ON DELETE CASCADE,
      user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
      total_questions INTEGER DEFAULT 0,
      questions_correct INTEGER DEFAULT 0,
      total_time_spent INTEGER DEFAULT 0, -- in seconds
      average_score FLOAT DEFAULT 0.0,
      subjects_covered TEXT[],
      key_topics TEXT[],
      areas_for_improvement TEXT[],
      summary_data JSONB,
      created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
    );

    -- Archived conversations table already created in railway-schema.sql
    -- Skipping duplicate creation to avoid PostgreSQL type conflicts

    -- Archived questions table (for individual Q&A pairs)
    CREATE TABLE IF NOT EXISTS archived_questions (
      id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
      user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
      subject VARCHAR(100) NOT NULL,
      question_text TEXT NOT NULL,
      student_answer TEXT,
      is_correct BOOLEAN,
      ai_answer TEXT, -- Optional AI provided answer
      archived_date DATE NOT NULL DEFAULT CURRENT_DATE,
      created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
    );

    -- Indexes for performance
    CREATE INDEX IF NOT EXISTS idx_users_email ON users(email);
    CREATE INDEX IF NOT EXISTS idx_users_google_id ON users(google_id);
    CREATE INDEX IF NOT EXISTS idx_users_apple_id ON users(apple_id);
    CREATE INDEX IF NOT EXISTS idx_user_sessions_token_hash ON user_sessions(token_hash);
    CREATE INDEX IF NOT EXISTS idx_user_sessions_user_id ON user_sessions(user_id);
    CREATE INDEX IF NOT EXISTS idx_user_sessions_expires_at ON user_sessions(expires_at);
    CREATE INDEX IF NOT EXISTS idx_sessions_user_id ON sessions(user_id);
    CREATE INDEX IF NOT EXISTS idx_questions_user_id ON questions(user_id);
    CREATE INDEX IF NOT EXISTS idx_questions_session_id ON questions(session_id);
    -- Conversation indexes already created in railway-schema.sql
    -- Skipping duplicate index creation
    
    -- Enhanced profile table indexes
    CREATE INDEX IF NOT EXISTS idx_profiles_user_id ON profiles(user_id);
    CREATE INDEX IF NOT EXISTS idx_profiles_email ON profiles(email);
    CREATE INDEX IF NOT EXISTS idx_profiles_parent_id ON profiles(parent_id);
    CREATE INDEX IF NOT EXISTS idx_profiles_role ON profiles(role);
    CREATE INDEX IF NOT EXISTS idx_profiles_completion ON profiles(profile_completion_percentage);
    CREATE INDEX IF NOT EXISTS idx_profiles_onboarding ON profiles(onboarding_completed);
    
    -- Archive table indexes already created in railway-schema.sql
    -- Skipping duplicate index creation
    CREATE INDEX IF NOT EXISTS idx_archived_questions_user_date ON archived_questions(user_id, archived_date DESC);
    CREATE INDEX IF NOT EXISTS idx_archived_questions_subject ON archived_questions(user_id, subject);

    -- Triggers for updated_at columns
    CREATE OR REPLACE FUNCTION update_updated_at_column()
    RETURNS TRIGGER AS $$
    BEGIN
        NEW.updated_at = NOW();
        RETURN NEW;
    END;
    $$ language 'plpgsql';

    CREATE TRIGGER IF NOT EXISTS update_users_updated_at 
        BEFORE UPDATE ON users 
        FOR EACH ROW 
        EXECUTE FUNCTION update_updated_at_column();

    CREATE TRIGGER IF NOT EXISTS update_profiles_updated_at 
        BEFORE UPDATE ON profiles 
        FOR EACH ROW 
        EXECUTE FUNCTION update_updated_at_column();

    CREATE TRIGGER IF NOT EXISTS update_sessions_updated_at 
        BEFORE UPDATE ON sessions 
        FOR EACH ROW 
        EXECUTE FUNCTION update_updated_at_column();

    CREATE TRIGGER IF NOT EXISTS update_questions_updated_at
        BEFORE UPDATE ON questions
        FOR EACH ROW
        EXECUTE FUNCTION update_updated_at_column();

    CREATE TRIGGER IF NOT EXISTS update_progress_updated_at
        BEFORE UPDATE ON progress
        FOR EACH ROW
        EXECUTE FUNCTION update_updated_at_column();

    -- Parent Reports Table (for weekly/monthly student progress reports)
    -- Added to inline schema to ensure automatic creation on deployment
    CREATE TABLE IF NOT EXISTS parent_reports (
        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        user_id UUID NOT NULL,
        report_type VARCHAR(50) NOT NULL CHECK (report_type IN ('weekly', 'monthly', 'custom', 'progress')),
        start_date DATE NOT NULL,
        end_date DATE NOT NULL,

        -- Core Report Data
        report_data JSONB NOT NULL,

        -- Progress Comparison Data
        previous_report_id UUID REFERENCES parent_reports(id),
        comparison_data JSONB,

        -- Report Metadata
        generated_at TIMESTAMP DEFAULT NOW(),
        expires_at TIMESTAMP NOT NULL DEFAULT NOW() + INTERVAL '7 days',
        report_version VARCHAR(10) DEFAULT '1.0',

        -- Status and Settings
        status VARCHAR(20) DEFAULT 'completed' CHECK (status IN ('generating', 'completed', 'failed', 'expired')),
        generation_time_ms INTEGER,
        ai_analysis_included BOOLEAN DEFAULT false,

        -- Foreign Key
        FOREIGN KEY (user_id) REFERENCES users(id) ON DELETE CASCADE,

        -- Constraints
        CHECK (start_date <= end_date),
        CHECK (generated_at <= expires_at)
    );

    -- Parent Reports Indexes
    CREATE INDEX IF NOT EXISTS idx_parent_reports_user_date ON parent_reports(user_id, start_date, end_date);
    CREATE INDEX IF NOT EXISTS idx_parent_reports_user_generated ON parent_reports(user_id, generated_at DESC);
    CREATE INDEX IF NOT EXISTS idx_parent_reports_status ON parent_reports(status, expires_at);

    -- Parent Report Narratives Table (for human-readable reports)
    -- Added to inline schema to ensure automatic creation on deployment
    CREATE TABLE IF NOT EXISTS parent_report_narratives (
        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        parent_report_id UUID NOT NULL REFERENCES parent_reports(id) ON DELETE CASCADE,
        narrative_content TEXT NOT NULL,
        report_summary TEXT NOT NULL,
        key_insights JSONB DEFAULT '[]',
        recommendations TEXT[] DEFAULT '{}',
        tone_style VARCHAR(50) DEFAULT 'teacher_to_parent',
        language VARCHAR(10) DEFAULT 'en',
        generated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        ai_model_version VARCHAR(50) DEFAULT 'claude-3.5-sonnet',
        word_count INTEGER DEFAULT 0,
        reading_level VARCHAR(20) DEFAULT 'grade_8',
        generation_time_ms INTEGER DEFAULT 0,
        is_complete BOOLEAN DEFAULT true,
        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
    );

    -- Parent Report Narratives Indexes
    CREATE INDEX IF NOT EXISTS idx_parent_report_narratives_parent_report_id ON parent_report_narratives(parent_report_id);
    CREATE INDEX IF NOT EXISTS idx_parent_report_narratives_generated_at ON parent_report_narratives(generated_at DESC);
    CREATE INDEX IF NOT EXISTS idx_parent_report_narratives_tone_language ON parent_report_narratives(tone_style, language);

    -- PASSIVE REPORTS SCHEMA (Scheduled Weekly/Monthly Reports)
    -- Added for automated parent report generation system
    CREATE TABLE IF NOT EXISTS parent_report_batches (
        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        user_id UUID NOT NULL,
        period VARCHAR(20) NOT NULL, -- 'weekly' | 'monthly'
        start_date DATE NOT NULL,
        end_date DATE NOT NULL,
        generated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        status VARCHAR(20) DEFAULT 'completed', -- 'pending' | 'processing' | 'completed' | 'failed'
        generation_time_ms INTEGER,

        -- Quick metrics for card display
        overall_grade VARCHAR(2), -- 'A+', 'B', etc.
        overall_accuracy FLOAT,
        question_count INTEGER,
        study_time_minutes INTEGER,
        current_streak INTEGER,

        -- Trends
        accuracy_trend VARCHAR(20), -- 'improving' | 'stable' | 'declining'
        activity_trend VARCHAR(20), -- 'increasing' | 'stable' | 'decreasing'

        -- Summary text
        one_line_summary TEXT,

        -- Metadata
        metadata JSONB,

        CONSTRAINT unique_user_period_date UNIQUE (user_id, period, start_date)
    );

    -- Indexes for efficient querying
    CREATE INDEX IF NOT EXISTS idx_report_batches_user_date ON parent_report_batches(user_id, start_date DESC);
    CREATE INDEX IF NOT EXISTS idx_report_batches_status ON parent_report_batches(status) WHERE status != 'completed';
    CREATE INDEX IF NOT EXISTS idx_report_batches_generated ON parent_report_batches(generated_at DESC);

    -- Individual reports within a batch
    CREATE TABLE IF NOT EXISTS passive_reports (
        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        batch_id UUID NOT NULL REFERENCES parent_report_batches(id) ON DELETE CASCADE,
        report_type VARCHAR(50) NOT NULL, -- 'academic_performance', 'learning_behavior', etc.

        -- Report content
        narrative_content TEXT NOT NULL,
        key_insights JSONB, -- Array of insight objects
        recommendations JSONB, -- Array of recommendation objects
        visual_data JSONB, -- Chart data for rendering

        -- Metadata
        word_count INTEGER,
        generation_time_ms INTEGER,
        ai_model_used VARCHAR(50) DEFAULT 'gpt-4o-mini',

        generated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),

        CONSTRAINT unique_batch_type UNIQUE (batch_id, report_type)
    );

    -- Indexes for passive_reports
    CREATE INDEX IF NOT EXISTS idx_passive_reports_batch ON passive_reports(batch_id);
    CREATE INDEX IF NOT EXISTS idx_passive_reports_type ON passive_reports(report_type);

    -- User notification preferences for reports
    CREATE TABLE IF NOT EXISTS report_notification_preferences (
        user_id UUID PRIMARY KEY,
        weekly_reports_enabled BOOLEAN DEFAULT true,
        monthly_reports_enabled BOOLEAN DEFAULT true,
        push_notifications_enabled BOOLEAN DEFAULT true,
        email_digest_enabled BOOLEAN DEFAULT false,
        email_address VARCHAR(255),
        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
    );
  `;

  await db.query(schema);
}

// MARK: - COPPA Consent Management Functions

db.createParentalConsentRequest = async function(consentData) {
  const {
    childUserId,
    childEmail,
    childDateOfBirth,
    parentEmail,
    parentName,
    parentRelationship = 'parent',
    requestIP,
    requestUserAgent,
    requestMetadata = {}
  } = consentData;

  // Calculate child's age
  const today = new Date();
  const birthDate = new Date(childDateOfBirth);
  const age = today.getFullYear() - birthDate.getFullYear();
  const monthDiff = today.getMonth() - birthDate.getMonth();
  const adjustedAge = monthDiff < 0 || (monthDiff === 0 && today.getDate() < birthDate.getDate())
    ? age - 1
    : age;

  // Generate verification code
  const verificationCode = Math.floor(100000 + Math.random() * 900000).toString();
  const verificationCodeExpiresAt = new Date(Date.now() + 24 * 60 * 60 * 1000); // 24 hours
  const consentExpiresAt = new Date(Date.now() + 365 * 24 * 60 * 60 * 1000); // 1 year (COPPA best practice)

  const query = `
    INSERT INTO parental_consents (
      child_user_id, child_email, child_date_of_birth, child_age_at_consent,
      parent_email, parent_name, parent_relationship,
      consent_method, consent_status,
      consent_expires_at,
      verification_code, verification_code_expires_at,
      request_ip, request_user_agent, request_metadata
    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15)
    RETURNING *
  `;

  const values = [
    childUserId, childEmail, childDateOfBirth, adjustedAge,
    parentEmail, parentName, parentRelationship,
    'email', 'pending',
    consentExpiresAt,
    verificationCode, verificationCodeExpiresAt,
    requestIP, requestUserAgent, JSON.stringify(requestMetadata)
  ];

  const result = await this.query(query, values);

  // Log in audit trail
  await this.logConsentAction(result.rows[0].id, 'created', 'system', null, {
    reason: 'User under 13 - COPPA compliance required',
    age: adjustedAge
  });

  return {
    ...result.rows[0],
    verification_code: verificationCode
  };
};

db.verifyParentalConsentCode = async function(childUserId, code) {
  // Get pending consent with matching code
  const selectQuery = `
    SELECT * FROM parental_consents
    WHERE child_user_id = $1
      AND verification_code = $2
      AND consent_status = 'pending'
      AND verification_code_expires_at > NOW()
    ORDER BY created_at DESC
    LIMIT 1
  `;

  const selectResult = await this.query(selectQuery, [childUserId, code]);

  if (selectResult.rows.length === 0) {
    // Invalid or expired code
    // Increment verification attempts
    await this.query(`
      UPDATE parental_consents
      SET verification_attempts = verification_attempts + 1
      WHERE child_user_id = $1 AND consent_status = 'pending'
    `, [childUserId]);

    return {
      success: false,
      error: 'INVALID_OR_EXPIRED_CODE'
    };
  }

  const consent = selectResult.rows[0];

  // Update consent status to granted
  const updateQuery = `
    UPDATE parental_consents
    SET
      consent_status = 'granted',
      consent_granted_at = NOW(),
      verified_at = NOW(),
      updated_at = NOW()
    WHERE id = $1
    RETURNING *
  `;

  const updateResult = await this.query(updateQuery, [consent.id]);

  // Update user account to remove restrictions
  await this.query(`
    UPDATE users
    SET
      parental_consent_status = 'granted',
      account_restricted = false,
      restriction_reason = null
    WHERE id = $1
  `, [childUserId]);

  // Log in audit trail
  await this.logConsentAction(consent.id, 'granted', 'parent', consent.parent_email, {
    reason: 'Parent verified consent via email code'
  });

  return {
    success: true,
    consent: updateResult.rows[0]
  };
};

db.revokeParentalConsent = async function(consentId, revokedBy, reason) {
  const query = `
    UPDATE parental_consents
    SET
      consent_status = 'revoked',
      revoked_at = NOW(),
      revoked_reason = $2,
      updated_at = NOW()
    WHERE id = $1
    RETURNING *
  `;

  const result = await this.query(query, [consentId, reason]);

  if (result.rows.length > 0) {
    const consent = result.rows[0];

    // Restrict user account
    await this.query(`
      UPDATE users
      SET
        parental_consent_status = 'revoked',
        account_restricted = true,
        restriction_reason = $2
      WHERE id = $1
    `, [consent.child_user_id, `Parental consent revoked: ${reason}`]);

    // Log in audit trail
    await this.logConsentAction(consentId, 'revoked', revokedBy, null, {
      reason: reason
    });
  }

  return result.rows[0];
};

db.getParentalConsentStatus = async function(userId) {
  const query = `
    SELECT * FROM parental_consents
    WHERE child_user_id = $1
    ORDER BY created_at DESC
    LIMIT 1
  `;

  const result = await this.query(query, [userId]);
  return result.rows[0] || null;
};

db.logAgeVerification = async function(verificationData) {
  const {
    userId,
    email,
    providedDateOfBirth,
    verificationMethod = 'self_reported',
    verificationIP,
    verificationUserAgent,
    verificationMetadata = {},
    notes = null
  } = verificationData;

  // Calculate age
  const today = new Date();
  const birthDate = new Date(providedDateOfBirth);
  const age = today.getFullYear() - birthDate.getFullYear();
  const monthDiff = today.getMonth() - birthDate.getMonth();
  const calculatedAge = monthDiff < 0 || (monthDiff === 0 && today.getDate() < birthDate.getDate())
    ? age - 1
    : age;

  const isMinor = calculatedAge < 18;
  const isCoppaProtected = calculatedAge < 13;

  const query = `
    INSERT INTO age_verifications (
      user_id, email, provided_date_of_birth, calculated_age,
      is_minor, is_coppa_protected,
      verification_method, verification_status,
      verification_ip, verification_user_agent, verification_metadata,
      verified_at, notes
    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, NOW(), $12)
    RETURNING *
  `;

  const values = [
    userId, email, providedDateOfBirth, calculatedAge,
    isMinor, isCoppaProtected,
    verificationMethod, 'verified',
    verificationIP, verificationUserAgent, JSON.stringify(verificationMetadata),
    notes
  ];

  const result = await this.query(query, values);
  return result.rows[0];
};

db.logConsentAction = async function(consentId, actionType, actionBy, actionByEmail, metadata = {}) {
  const query = `
    INSERT INTO consent_audit_log (
      consent_id, action_type, action_by, action_by_email,
      action_reason, action_metadata
    ) VALUES ($1, $2, $3, $4, $5, $6)
    RETURNING *
  `;

  const values = [
    consentId,
    actionType,
    actionBy,
    actionByEmail,
    metadata.reason || null,
    JSON.stringify(metadata)
  ];

  const result = await this.query(query, values);
  return result.rows[0];
};

db.getConsentAuditLog = async function(consentId) {
  const query = `
    SELECT * FROM consent_audit_log
    WHERE consent_id = $1
    ORDER BY created_at DESC
  `;

  const result = await this.query(query, [consentId]);
  return result.rows;
};

db.checkUserNeedsParentalConsent = async function(userId) {
  const query = `
    SELECT
      u.id,
      u.email,
      u.requires_parental_consent,
      u.parental_consent_status,
      u.account_restricted,
      p.date_of_birth,
      pc.consent_status as active_consent_status,
      pc.consent_granted_at,
      pc.consent_expires_at
    FROM users u
    LEFT JOIN profiles p ON u.id = p.user_id
    LEFT JOIN parental_consents pc ON u.id = pc.child_user_id AND pc.consent_status = 'granted'
    WHERE u.id = $1
  `;

  const result = await this.query(query, [userId]);
  return result.rows[0] || null;
};

// Graceful shutdown
process.on('SIGINT', async () => {
  logger.debug('üîÑ Closing PostgreSQL connection pool...');
  await pool.end();
  logger.debug('‚úÖ PostgreSQL connection pool closed');
  process.exit(0);
});

// PHASE 1 OPTIMIZATION: Pool statistics for monitoring
function getPoolStats() {
  return {
    // Connection pool status
    totalConnections: pool.totalCount,
    idleConnections: pool.idleCount,
    activeConnections: pool.totalCount - pool.idleCount,
    waitingRequests: pool.waitingCount,

    // Pool configuration
    maxConnections: 20,
    minConnections: 2,

    // Health indicators
    poolUtilization: ((pool.totalCount - pool.idleCount) / 20 * 100).toFixed(1) + '%',
    isHealthy: pool.waitingCount === 0 && (pool.totalCount - pool.idleCount) < 18,

    // Performance metrics
    connectionTimeouts: queryMetrics.connectionTimeouts,
    poolExhaustion: queryMetrics.poolExhaustion,

    // Recommendations
    warnings: []
  };
}

// Add warnings based on pool health
function getPoolHealth() {
  const stats = getPoolStats();

  if (stats.waitingRequests > 0) {
    stats.warnings.push('‚ö†Ô∏è Requests are waiting for connections - consider increasing pool size');
  }

  if (stats.activeConnections > 15) {
    stats.warnings.push('‚ö†Ô∏è High pool utilization (>75%) - monitor for bottlenecks');
  }

  if (stats.connectionTimeouts > 10) {
    stats.warnings.push('‚ùå High connection timeout rate - check database performance');
  }

  return stats;
}

module.exports = {
  db,
  initializeDatabase,
  getPoolStats,      // PHASE 1: Export pool statistics
  getPoolHealth      // PHASE 1: Export pool health check
};
/**
 * Gemini Live API Integration Module (v2 - Official Protocol)
 *
 * Implements a WebSocket proxy between iOS client and Google's official Gemini Live API.
 * Uses the official BidiGenerateContent WebSocket protocol.
 *
 * Official API: wss://generativelanguage.googleapis.com/ws/google.ai.generativelanguage.v1beta.GenerativeService.BidiGenerateContent
 *
 * Message Protocol:
 * - iOS â†’ Backend: Custom simplified messages
 * - Backend â†’ Google: Official BidiGenerateContent protocol
 * - Google â†’ Backend: Official server messages
 * - Backend â†’ iOS: Simplified response messages
 */

const WebSocket = require('ws');

module.exports = async function (fastify, opts) {
    const { db } = require('../../../../utils/railway-database');
    const logger = fastify.log;

    // Google Gemini Live API WebSocket endpoint
    const GEMINI_LIVE_ENDPOINT = 'wss://generativelanguage.googleapis.com/ws/google.ai.generativelanguage.v1beta.GenerativeService.BidiGenerateContent';

    /**
     * WebSocket proxy endpoint for Gemini Live
     *
     * Acts as a bridge between iOS client and Google's Gemini Live API.
     * Handles authentication, message translation, and bidirectional streaming.
     */
    fastify.get('/api/ai/gemini-live/connect', { websocket: true }, async (connection, req) => {
        // In @fastify/websocket, connection itself is the WebSocket (not connection.socket)
        const clientSocket = connection;
        let geminiSocket = null;
        let userId = null;
        let sessionId = null;
        let isSetupComplete = false;
        let isGeminiConnected = false;
        const messageQueue = []; // Queue messages until Gemini is ready
        let pendingStartSession = null; // Store start_session until Gemini WS is open
        let currentSubject = null; // Set on start_session; used to give image context

        // Accumulators for the current turn â€” reset on turn_complete / interrupted
        let currentUserTranscript = '';   // built from inputTranscription chunks
        let currentAiTranscript = '';     // built from outputTranscription chunks

        try {
            // ============================================
            // STEP 1: Authenticate iOS Client (BLOCKING)
            // ============================================
            const token = req.query.token;
            sessionId = req.query.sessionId;

            if (!token) {
                clientSocket.send(JSON.stringify({
                    type: 'error',
                    error: 'Missing authentication token'
                }));
                clientSocket.close(1008, 'Unauthorized');
                return;
            }

            // Verify session token (64-character hex format)
            try {
                const sessionData = await db.verifyUserSession(token);

                if (!sessionData || !sessionData.user_id) {
                    clientSocket.send(JSON.stringify({
                        type: 'error',
                        error: 'Invalid or expired authentication token'
                    }));
                    clientSocket.close(1008, 'Unauthorized');
                    return;
                }

                userId = sessionData.user_id;
            } catch (error) {
                logger.error({ error }, 'Session verification failed');
                clientSocket.send(JSON.stringify({
                    type: 'error',
                    error: 'Authentication failed'
                }));
                clientSocket.close(1008, 'Unauthorized');
                return;
            }

            // Verify session ownership (BLOCKING - prevent race condition)
            if (sessionId) {
                const result = await db.query('SELECT user_id FROM sessions WHERE id = $1', [sessionId]);

                if (result.rows.length === 0 || result.rows[0].user_id !== userId) {
                    clientSocket.send(JSON.stringify({
                        type: 'error',
                        error: 'Session not found or unauthorized'
                    }));
                    clientSocket.close(1008, 'Unauthorized');
                    return;
                }
            }

            // ============================================
            // STEP 2: Connect to Google Gemini Live API
            // ============================================
            const apiKey = process.env.GEMINI_API_KEY;
            if (!apiKey) {
                logger.error('GEMINI_API_KEY not configured');
                clientSocket.send(JSON.stringify({
                    type: 'error',
                    error: 'Server configuration error'
                }));
                clientSocket.close(1011, 'Configuration error');
                return;
            }

            // Connect to Google's WebSocket with API key
            const geminiUrl = `${GEMINI_LIVE_ENDPOINT}?key=${apiKey}`;

            try {
                geminiSocket = new WebSocket(geminiUrl);
            } catch (wsError) {
                logger.error({ wsError }, 'Failed to create WebSocket connection');
                clientSocket.send(JSON.stringify({
                    type: 'error',
                    error: 'Failed to connect to AI service'
                }));
                clientSocket.close(1011, 'Connection failed');
                return;
            }

            // ============================================
            // STEP 3: Handle Gemini Connection Events
            // ============================================
            geminiSocket.on('open', async () => {
                isGeminiConnected = true;

                // If start_session arrived before Gemini opened, process it now
                if (pendingStartSession) {
                    const msg = pendingStartSession;
                    pendingStartSession = null;
                    await handleClientMessage(msg);
                }
            });

            geminiSocket.on('message', (data) => {
                try {
                    const message = JSON.parse(data.toString());
                    handleGeminiMessage(message);
                } catch (error) {
                    logger.error({ error: error.message }, 'Failed to parse Gemini message');
                }
            });

            geminiSocket.on('error', (error) => {
                logger.error({ error: error.message, userId }, 'Gemini WebSocket error');
                clientSocket.send(JSON.stringify({
                    type: 'error',
                    error: 'Connection to AI service failed'
                }));
            });

            geminiSocket.on('close', (code, reason) => {
                const reasonStr = reason ? reason.toString() : 'No reason provided';
                logger.warn({ code, reason: reasonStr, sessionId, userId }, '[Live] Gemini WebSocket closed');
                if (code !== 1000) {
                    logger.error({ code, reason: reasonStr, userId }, 'Gemini WebSocket closed unexpectedly');
                }
                clearInterval(geminiKeepAliveInterval);
                clientSocket.send(JSON.stringify({
                    type: 'session_ended',
                    reason: 'AI service disconnected'
                }));
                clientSocket.close(1000, 'Gemini session ended');
            });

            // ============================================
            // STEP 4: Handle iOS Client Messages
            // ============================================
            clientSocket.on('message', async (data) => {
                try {
                    const message = JSON.parse(data.toString());

                    // Gate start_session on Gemini WS being open
                    if (message.type === 'start_session') {
                        if (!isGeminiConnected || !geminiSocket || geminiSocket.readyState !== WebSocket.OPEN) {
                            pendingStartSession = message;
                            return;
                        }
                        await handleClientMessage(message);
                        return;
                    }

                    // Queue other messages until setupComplete
                    if (!isSetupComplete) {
                        messageQueue.push(message);
                        return;
                    }

                    await handleClientMessage(message);
                } catch (error) {
                    logger.error({ error: error.message }, 'Error processing client message');
                    clientSocket.send(JSON.stringify({
                        type: 'error',
                        error: error.message
                    }));
                }
            });

            clientSocket.on('close', () => {
                clearInterval(keepAliveInterval);
                clearInterval(geminiKeepAliveInterval);
                if (geminiSocket && geminiSocket.readyState === WebSocket.OPEN) {
                    geminiSocket.close(1000, 'Client disconnected');
                }
            });

            clientSocket.on('error', (error) => {
                logger.error({ error: error.message, userId }, 'iOS client WebSocket error');
            });

            // Keep-alive ping every 20s to prevent Railway/proxy from closing idle connections
            const keepAliveInterval = setInterval(() => {
                if (clientSocket.readyState === WebSocket.OPEN) {
                    clientSocket.ping();
                } else {
                    clearInterval(keepAliveInterval);
                }
            }, 20000);

            // Keep-alive ping toward Gemini every 20s â€” prevents Gemini's idle timeout from
            // closing the connection during long AI audio responses (especially multilingual,
            // which produces larger audio bursts with longer silent gaps between iOS sends).
            const geminiKeepAliveInterval = setInterval(() => {
                if (geminiSocket && geminiSocket.readyState === WebSocket.OPEN) {
                    geminiSocket.ping();
                } else {
                    clearInterval(geminiKeepAliveInterval);
                }
            }, 20000);

            // ============================================
            // Message Handler: iOS Client â†’ Google Gemini
            // ============================================
            async function handleClientMessage(message) {
                const { type, ...data } = message;

                switch (type) {
                    case 'start_session':
                        await handleStartSession(data);
                        break;

                    case 'audio_chunk':
                        handleAudioChunk(data);
                        break;

                    case 'audio_stream_end':
                        handleAudioStreamEnd();
                        break;

                    case 'image_message':
                        handleImageChunk(data);
                        break;

                    case 'text_message':
                        handleTextMessage(data);
                        break;

                    case 'interrupt':
                        handleInterrupt();
                        break;

                    case 'end_session':
                        handleEndSession();
                        break;

                    default:
                        break;
                }
            }

            /**
             * HANDLER: Start Session
             * Translates iOS "start_session" to official "setup" message
             */
            async function handleStartSession(data) {
                const { subject, language, character } = data;
                logger.info({ sessionId, subject, language, character }, '[Live] handleStartSession');

                currentSubject = subject || null;

                // Map iOS character to Gemini prebuilt voice name
                const geminiVoiceMap = {
                    adam: 'Schedar',
                    eva:  'Despina',
                    max:  'Fenrir',
                    mia:  'Zephyr'
                };
                const voiceName = geminiVoiceMap[character] || 'Puck';

                const systemInstruction = buildSystemInstruction(subject, language);

                const setupMessage = {
                    setup: {
                        model: "models/gemini-2.5-flash-native-audio-preview-12-2025",
                        generationConfig: {
                            responseModalities: ["AUDIO"],
                            speechConfig: {
                                voiceConfig: {
                                    prebuiltVoiceConfig: {
                                        voiceName: voiceName
                                    }
                                }
                            }
                        },
                        // outputAudioTranscription gives clean spoken-text-only transcription,
                        // with no chain-of-thought content â€” the right source for text display.
                        inputAudioTranscription: {},
                        outputAudioTranscription: {},
                        systemInstruction: {
                            parts: [{ text: systemInstruction }]
                        }
                    }
                };

                if (geminiSocket && geminiSocket.readyState === WebSocket.OPEN) {
                    geminiSocket.send(JSON.stringify(setupMessage));

                    // If Gemini doesn't confirm setup within 8s, notify iOS so it can show
                    // "Tap to Reactivate" rather than hanging indefinitely in .connecting state.
                    setTimeout(() => {
                        if (!isSetupComplete) {
                            logger.error({ userId, sessionId }, 'Gemini did not respond to setup after 8 seconds');
                            if (clientSocket.readyState === WebSocket.OPEN) {
                                clientSocket.send(JSON.stringify({
                                    type: 'error',
                                    error: 'Connection to AI timed out. Tap to retry.'
                                }));
                            }
                        }
                    }, 8000);
                } else {
                    throw new Error('Gemini connection not ready');
                }
            }

            /**
             * HANDLER: Audio Chunk
             * Translates iOS audio to official realtimeInput message
             * NOTE: Input audio is 16kHz PCM
             */
            function handleAudioChunk(data) {
                const { audio } = data; // Base64 encoded audio

                // Build official BidiGenerateContentRealtimeInput message
                // âœ… Official protocol uses camelCase and audio field (not media_chunks)
                const realtimeInput = {
                    realtimeInput: {  // âœ… camelCase
                        audio: {  // âœ… Use audio field, not media_chunks
                            data: audio,
                            mimeType: 'audio/pcm;rate=24000'  // âœ… Match iOS recording rate (24kHz)
                        }
                    }
                };

                // Forward to Gemini
                if (geminiSocket && geminiSocket.readyState === WebSocket.OPEN) {
                    geminiSocket.send(JSON.stringify(realtimeInput));
                }
            }

            /**
             * HANDLER: Audio Stream End
             *
             * âœ… FIX: Send audioStreamEnd when mic stops to flush cached audio
             */
            function handleAudioStreamEnd() {
                logger.info({ sessionId }, '[Live] handleAudioStreamEnd: flushing audio to Gemini');
                const streamEnd = {
                    realtimeInput: {
                        audioStreamEnd: true
                    }
                };

                if (geminiSocket && geminiSocket.readyState === WebSocket.OPEN) {
                    geminiSocket.send(JSON.stringify(streamEnd));
                }
            }

            /**
             * HANDLER: Image Message
             * Sends image + subject-anchored prompt as a clientContent turn so Gemini
             * stores it in session history and can reference it in all subsequent turns.
             */
            function handleImageChunk(data) {
                const { imageBase64, mimeType = 'image/jpeg' } = data;

                if (!imageBase64) return;

                if (!geminiSocket || geminiSocket.readyState !== WebSocket.OPEN) return;

                const subjectHint = currentSubject ? `This is a ${currentSubject} problem.` : '';
                const instruction = `${subjectHint} Please look at this image carefully and help me with it. I may ask follow-up questions by voice.`.trim();

                // text part BEFORE inlineData (BidiGenerateContentClientContent spec order)
                const clientContent = {
                    clientContent: {
                        turns: [{
                            role: 'user',
                            parts: [
                                { text: instruction },
                                {
                                    inlineData: {
                                        mimeType: mimeType,
                                        data: imageBase64
                                    }
                                }
                            ]
                        }],
                        turnComplete: true
                    }
                };

                geminiSocket.send(JSON.stringify(clientContent));
            }

            /**
             * HANDLER: Text Message
             * Translates iOS text to official clientContent message
             */
            function handleTextMessage(data) {
                const { text } = data;

                // Build official BidiGenerateContentClientContent message
                // âœ… Official protocol uses camelCase
                const clientContent = {
                    clientContent: {  // âœ… camelCase
                        turns: [{
                            role: 'user',
                            parts: [{
                                text: text
                            }]
                        }],
                        turnComplete: true  // âœ… camelCase
                    }
                };

                if (geminiSocket && geminiSocket.readyState === WebSocket.OPEN) {
                    geminiSocket.send(JSON.stringify(clientContent));
                    storeMessage('user', text);
                }
            }

            /**
             * HANDLER: Interrupt
             *
             * âœ… FIX: With automatic VAD enabled (default), barge-in happens naturally
             * when new user audio/text is sent. Don't send activityEnd in this mode.
             *
             * If you need explicit control, set:
             * realtimeInputConfig.automaticActivityDetection.disabled = true
             * in setup, then use activityStart/activityEnd explicitly.
             */
            function handleInterrupt() {
                clientSocket.send(JSON.stringify({ type: 'interrupted' }));
            }

            function handleEndSession() {
                if (geminiSocket && geminiSocket.readyState === WebSocket.OPEN) {
                    geminiSocket.close(1000, 'Session ended by user');
                }
                clientSocket.send(JSON.stringify({ type: 'session_ended' }));
                clientSocket.close(1000, 'Session ended');
            }

            // ============================================
            // Message Handler: Google Gemini â†’ iOS Client
            // ============================================
            async function handleGeminiMessage(message) {
                // Handle setupComplete
                if (message.setupComplete || message.setup_complete) {
                    isSetupComplete = true;
                    logger.info({ sessionId }, '[Live] Gemini setupComplete received');

                    // Replay prior turns for this session so Gemini has full context.
                    // This is a no-op on first connect (no rows yet); on reconnect it restores
                    // everything the student said and Gemini answered before the drop.
                    await replaySessionContext();

                    clientSocket.send(JSON.stringify({
                        type: 'session_ready',
                        sessionId: sessionId
                    }));

                    // Flush queued messages after setupComplete
                    while (messageQueue.length > 0) {
                        await handleClientMessage(messageQueue.shift());
                    }
                }

                // Handle serverContent (AI response)
                const serverContent = message.serverContent || message.server_content;
                if (serverContent) {
                    const modelTurn = serverContent.modelTurn || serverContent.model_turn;
                    const turnComplete = serverContent.turnComplete || serverContent.turn_complete;
                    const interrupted = serverContent.interrupted;

                    // outputAudioTranscription (set up at session level) delivers clean spoken-text-only
                    // transcription with no chain-of-thought content. This is the correct and sole
                    // source for text display and storage. modelTurn.parts[].text contains COT and
                    // must NOT be used for text_chunk messages or transcript accumulation.
                    const outputTranscription = serverContent.outputTranscription || serverContent.output_transcription;
                    if (outputTranscription && outputTranscription.text) {
                        currentAiTranscript += outputTranscription.text;
                        clientSocket.send(JSON.stringify({
                            type: 'text_chunk',
                            text: outputTranscription.text
                        }));
                    }

                    // Process modelTurn parts for audio only â€” text parts are intentionally ignored
                    // here because they contain COT. Text comes exclusively from outputTranscription above.
                    if (modelTurn && modelTurn.parts) {
                        for (const part of modelTurn.parts) {
                            // Audio part â€” forward to iOS for playback
                            const inlineData = part.inlineData || part.inline_data;
                            if (inlineData) {
                                const mimeType = inlineData.mimeType || inlineData.mime_type;
                                if (mimeType && mimeType.startsWith('audio/')) {
                                    const bufferedAmount = clientSocket.bufferedAmount || 0;
                                    if (bufferedAmount > 65536) {
                                        logger.error({ userId, bufferKB: Math.round(bufferedAmount / 1024) }, 'WebSocket backpressure: client buffer full');
                                    }
                                    clientSocket.send(JSON.stringify({
                                        type: 'audio_chunk',
                                        data: inlineData.data
                                    }));
                                }
                            }
                        }
                    }

                    // Accumulate user speech transcript chunks
                    const inputTranscription = serverContent.inputTranscription || serverContent.input_transcription;
                    if (inputTranscription && inputTranscription.text) {
                        currentUserTranscript += inputTranscription.text;
                        clientSocket.send(JSON.stringify({
                            type: 'user_transcription',
                            text: inputTranscription.text
                        }));
                    }

                    // On turn_complete: persist one row each for user and AI, then reset accumulators
                    if (turnComplete) {
                        clientSocket.send(JSON.stringify({
                            type: 'turn_complete'
                        }));

                        const userText = currentUserTranscript.trim();
                        const aiText = currentAiTranscript.trim();
                        currentUserTranscript = '';
                        currentAiTranscript = '';

                        logger.info({ sessionId, userChars: userText.length, aiChars: aiText.length }, '[Live] turn_complete: storing');
                        if (userText) {
                            await storeMessage('user', `ðŸŽ™ï¸ ${userText}`);
                        }
                        if (aiText) {
                            await storeMessage('assistant', aiText);
                        }
                    }

                    // Signal interrupted â€” reset AI accumulator (incomplete response discarded)
                    if (interrupted) {
                        clientSocket.send(JSON.stringify({
                            type: 'interrupted'
                        }));
                        // Save any user speech that arrived before the interrupt
                        const userText = currentUserTranscript.trim();
                        if (userText) {
                            await storeMessage('user', `ðŸŽ™ï¸ ${userText}`);
                        }
                        currentUserTranscript = '';
                        currentAiTranscript = '';
                    }
                }

                // Handle toolCall (function calling)
                const toolCall = message.toolCall || message.tool_call;
                if (toolCall) {
                    handleToolCall(toolCall);
                }

                // Handle toolCallCancellation
                const toolCallCancellation = message.toolCallCancellation || message.tool_call_cancellation;
                if (toolCallCancellation) {
                    clientSocket.send(JSON.stringify({
                        type: 'tool_call_cancelled',
                        ids: toolCallCancellation.ids
                    }));
                }

                // Handle goAway (server requests disconnect)
                const goAway = message.goAway || message.go_away;
                if (goAway) {
                    clientSocket.send(JSON.stringify({
                        type: 'go_away',
                        timeLeft: goAway.timeLeft || goAway.time_left,
                        message: 'Server requesting disconnect - please reconnect'
                    }));

                    setTimeout(() => {
                        if (geminiSocket && geminiSocket.readyState === WebSocket.OPEN) {
                            geminiSocket.close(1000, 'goAway received');
                        }
                        if (clientSocket.readyState === WebSocket.OPEN) {
                            clientSocket.close(1000, 'Server requested disconnect');
                        }
                    }, 1000);
                }

                // Handle sessionResumptionUpdate (no action needed, token managed server-side)
                // Handle usageMetadata (no action needed)
            }

            /**
             * Handle function calls from Gemini
             */
            async function handleToolCall(toolCall) {
                const functionCalls = toolCall.functionCalls || toolCall.function_calls;

                for (const call of functionCalls) {
                    const { name, args, id } = call;
                    let result;
                    try {
                        switch (name) {
                            case 'fetch_homework_context':
                                result = await fetchHomeworkContext(args.sessionId);
                                break;

                            case 'search_archived_conversations':
                                result = await searchArchivedConversations(args.query, args.subject);
                                break;

                            default:
                                result = { error: 'Unknown function' };
                        }

                        // Send function response back to Gemini
                        // âœ… Official protocol uses camelCase
                        const toolResponse = {
                            toolResponse: {  // âœ… camelCase
                                functionResponses: [{  // âœ… camelCase
                                    id: id,
                                    name: name,
                                    response: result
                                }]
                            }
                        };

                        if (geminiSocket && geminiSocket.readyState === WebSocket.OPEN) {
                            geminiSocket.send(JSON.stringify(toolResponse));
                        }

                    } catch (error) {
                        logger.error({ error, name }, 'Error executing function call');

                        // Send error response
                        const errorResponse = {
                            toolResponse: {  // âœ… camelCase
                                functionResponses: [{  // âœ… camelCase
                                    id: id,
                                    name: name,
                                    response: { error: error.message }
                                }]
                            }
                        };

                        if (geminiSocket && geminiSocket.readyState === WebSocket.OPEN) {
                            geminiSocket.send(JSON.stringify(errorResponse));
                        }
                    }
                }
            }

            // ============================================
            // Database Functions
            // ============================================
            async function storeMessage(role, text) {
                if (!sessionId) return;

                try {
                    // Write into `conversations` â€” the same table getConversationHistory reads,
                    // so the standard archive endpoint can find Live voice messages.
                    await db.addConversationMessage({
                        userId,
                        sessionId,
                        questionId: null,
                        messageType: role,       // 'user' | 'assistant'
                        messageText: text,
                        messageData: null,
                        tokensUsed: 0
                    });
                } catch (error) {
                    logger.error({ error }, 'Error storing message');
                }
            }

            /**
             * Replay prior conversation turns into Gemini after setupComplete.
             *
             * Called on every connect â€” is a no-op when there are no stored rows yet (first
             * connect). On reconnect it sends a single multi-turn clientContent message so
             * Gemini has the full prior conversation in its context window.
             *
             * Capped at the 50 most recent turns to stay within Gemini's context limit.
             */
            async function replaySessionContext() {
                if (!sessionId) return;
                try {
                    const result = await db.query(`
                        SELECT message_type, message_text
                        FROM conversations
                        WHERE session_id = $1
                        ORDER BY created_at ASC
                        LIMIT 50
                    `, [sessionId]);

                    if (result.rows.length === 0) {
                        logger.info({ sessionId }, '[Live] replaySessionContext: no prior turns, starting fresh');
                        return;
                    }

                    logger.info({ sessionId, rows: result.rows.length }, '[Live] replaySessionContext: replaying turns');

                    // Sanitize stored text before replaying.
                    // Non-live streaming path stores raw SSE bytes if the extraction failed.
                    // AI Engine SSE event types: start (ignore), content (delta), end (full text)
                    function sanitizeMessageText(raw) {
                        if (!raw) return '';
                        if (!raw.includes('data: {')) return raw; // already clean text
                        let endContent = '';
                        let deltaAccum = '';
                        for (const line of raw.split('\n')) {
                            if (!line.startsWith('data: ')) continue;
                            try {
                                const event = JSON.parse(line.slice(6));
                                if (event.type === 'end' && event.content) {
                                    endContent = event.content; // canonical â€” prefer this
                                } else if (event.type === 'content' && event.delta) {
                                    deltaAccum += event.delta;  // fallback accumulation
                                }
                            } catch (_) {}
                        }
                        const extracted = endContent || deltaAccum;
                        return extracted || raw; // last resort: pass raw so Gemini can try
                    }

                    // Build multi-turn history as clientContent turns, skipping empty rows
                    const turns = [];
                    for (const row of result.rows) {
                        const cleanText = sanitizeMessageText(row.message_text);
                        if (!cleanText.trim()) continue;
                        const preview = cleanText.slice(0, 100).replace(/\n/g, ' ');
                        logger.info({ role: row.message_type, preview }, '[Live] replay turn');
                        turns.push({
                            role: row.message_type === 'user' ? 'user' : 'model',
                            parts: [{ text: cleanText }]
                        });
                    }

                    if (turns.length === 0) {
                        logger.info({ sessionId }, '[Live] replaySessionContext: all rows empty after sanitize, skipping');
                        return;
                    }

                    // Append a silent primer as the final model turn so Gemini doesn't
                    // immediately respond to the history injection ("talk-back").
                    turns.push({
                        role: 'model',
                        parts: [{ text: 'I have reviewed our previous conversation and I\'m ready to continue.' }]
                    });

                    // turnComplete: true â€” history injection, not an open user turn.
                    // With false, Gemini treats the batch as incomplete and waits for
                    // continuation instead of committing the turns to session memory.
                    const replayMessage = {
                        clientContent: {
                            turns,
                            turnComplete: true
                        }
                    };

                    if (geminiSocket && geminiSocket.readyState === WebSocket.OPEN) {
                        geminiSocket.send(JSON.stringify(replayMessage));
                        logger.info({ sessionId, turns: turns.length }, '[Live] replaySessionContext: sent to Gemini');
                    }
                } catch (error) {
                    // Non-fatal â€” Gemini starts fresh if replay fails
                    logger.error({ error }, '[Live] replaySessionContext: error');
                }
            }

            async function fetchHomeworkContext(sessionId) {
                try {
                    const result = await db.query(`
                        SELECT
                            s.subject,
                            s.created_at,
                            s.metadata,
                            array_agg(
                                json_build_object(
                                    'role', cm.message_type,
                                    'content', cm.message_text,
                                    'timestamp', cm.created_at
                                ) ORDER BY cm.created_at
                            ) as messages
                        FROM sessions s
                        LEFT JOIN conversations cm ON cm.session_id = s.id
                        WHERE s.id = $1 AND s.user_id = $2
                        GROUP BY s.id
                    `, [sessionId, userId]);

                    if (result.rows.length === 0) {
                        return { error: 'Session not found' };
                    }

                    return result.rows[0];
                } catch (error) {
                    logger.error({ error }, 'Error fetching homework context');
                    return { error: 'Failed to fetch context' };
                }
            }

            async function searchArchivedConversations(query, subject) {
                try {
                    let sql = `
                        SELECT
                            id,
                            subject,
                            conversation_content,
                            archived_date,
                            ts_rank(
                                to_tsvector('english', conversation_content),
                                plainto_tsquery('english', $1)
                            ) as relevance
                        FROM archived_conversations_new
                        WHERE user_id = $2
                        AND to_tsvector('english', conversation_content) @@ plainto_tsquery('english', $1)
                    `;

                    const params = [query, userId];

                    if (subject) {
                        sql += ' AND subject = $3';
                        params.push(subject);
                    }

                    sql += ' ORDER BY relevance DESC, archived_date DESC LIMIT 5';

                    const result = await db.query(sql, params);

                    return {
                        results: result.rows,
                        count: result.rows.length
                    };
                } catch (error) {
                    logger.error({ error }, 'Error searching archived conversations');
                    return { error: 'Failed to search archives' };
                }
            }

        } catch (error) {
            logger.error({
                error: error.message,
                stack: error.stack,
                userId
            }, 'Fatal error in WebSocket handler');

            // Safely close client socket if still open
            try {
                if (clientSocket && clientSocket.readyState === WebSocket.OPEN) {
                    clientSocket.send(JSON.stringify({
                        type: 'error',
                        error: 'Internal server error'
                    }));
                    clientSocket.close(1011, 'Internal error');
                }
            } catch (closeError) {
                logger.error({ closeError }, 'Error closing client socket');
            }
        }
    });

    /**
     * Build system instruction for educational tutor
     */
    function buildSystemInstruction(subject, language = 'en') {
        const subjectContext = subject || 'General';

        const prompts = {
            en: `You are an expert AI tutor specializing in ${subjectContext}.

ðŸš¨ CRITICAL INSTRUCTION - READ FIRST ðŸš¨
You are speaking OUT LOUD to a student via voice. NEVER say things like:
- "I've taken the user's feedback..."
- "I'm focusing on..."
- "I'm exploring ways to..."
- "My goal is to..."
- "I want to reassure them..."
- ANY form of self-reflection or meta-commentary

This is a LIVE VOICE conversation. The student can ONLY hear what you say out loud. Speak DIRECTLY and NATURALLY as a tutor would in person.

âŒ BAD (meta-commentary): "I've taken the user's feedback about clarity to heart and am actively exploring ways to improve my communication."
âœ… GOOD (direct response): "I can hear you clearly now! What math topic would you like help with today?"

Your teaching philosophy:

1. Socratic Method: Guide students to discover answers through thoughtful questions
2. Scaffolding: Break complex problems into manageable steps
3. Patience: Never rush or show frustration, celebrate progress
4. Clarity: Use age-appropriate language and real-world examples
5. Engagement: Make learning interactive and fun

For math/science problems:
- Show step-by-step reasoning
- Explain the "why" behind each step
- Use analogies to clarify abstract concepts

For essays/analysis:
- Help organize thoughts and structure arguments
- Provide constructive feedback
- Encourage critical thinking

Always encourage the student and adapt your teaching style to their needs.`,

            'zh-Hans': `ä½ æ˜¯ä¸€ä½ä¸“ç²¾äºŽ${subjectContext}çš„AIå¯¼å¸ˆã€‚

ðŸš¨ å…³é”®æŒ‡ç¤º - é¦–å…ˆé˜…è¯» ðŸš¨
ä½ æ­£åœ¨é€šè¿‡è¯­éŸ³ä¸Žå­¦ç”Ÿè¿›è¡ŒçŽ°åœºå¯¹è¯ã€‚æ°¸è¿œä¸è¦è¯´ç±»ä¼¼ä»¥ä¸‹çš„è¯ï¼š
- "æˆ‘å·²ç»å¬å–äº†ç”¨æˆ·çš„åé¦ˆ..."
- "æˆ‘æ­£åœ¨ä¸“æ³¨äºŽ..."
- "æˆ‘æ­£åœ¨æŽ¢ç´¢æ–¹æ³•..."
- "æˆ‘çš„ç›®æ ‡æ˜¯..."
- "æˆ‘æƒ³è®©ä»–ä»¬æ”¾å¿ƒ..."
- ä»»ä½•å½¢å¼çš„è‡ªæˆ‘åæ€æˆ–å…ƒè¯„è®º

è¿™æ˜¯çŽ°åœºè¯­éŸ³å¯¹è¯ã€‚å­¦ç”Ÿåªèƒ½å¬åˆ°ä½ å¤§å£°è¯´å‡ºçš„è¯ã€‚åƒé¢å¯¹é¢çš„å¯¼å¸ˆä¸€æ ·ç›´æŽ¥ã€è‡ªç„¶åœ°è¯´è¯ã€‚

âŒ é”™è¯¯ï¼ˆå…ƒè¯„è®ºï¼‰ï¼š"æˆ‘å·²ç»å¬å–äº†ç”¨æˆ·å…³äºŽæ¸…æ™°åº¦çš„åé¦ˆï¼Œæ­£åœ¨ç§¯æžæŽ¢ç´¢æ”¹è¿›æ²Ÿé€šçš„æ–¹æ³•ã€‚"
âœ… æ­£ç¡®ï¼ˆç›´æŽ¥å›žåº”ï¼‰ï¼š"æˆ‘çŽ°åœ¨èƒ½æ¸…æ¥šåœ°å¬åˆ°ä½ äº†ï¼ä»Šå¤©ä½ æƒ³å­¦ä¹ ä»€ä¹ˆæ•°å­¦é¢˜ç›®ï¼Ÿ"

æ•™å­¦ç†å¿µï¼š

1. è‹æ ¼æ‹‰åº•å¼æ•™å­¦ï¼šé€šè¿‡æé—®å¼•å¯¼å­¦ç”Ÿå‘çŽ°ç­”æ¡ˆ
2. è„šæ‰‹æž¶æ•™å­¦ï¼šå°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºå¯ç®¡ç†çš„æ­¥éª¤
3. è€å¿ƒï¼šä»Žä¸æ€¥èºæˆ–è¡¨çŽ°æŒ«æŠ˜æ„Ÿï¼Œåº†ç¥è¿›æ­¥
4. æ¸…æ™°ï¼šä½¿ç”¨é€‚é¾„è¯­è¨€å’ŒçŽ°å®žä¾‹å­
5. äº’åŠ¨ï¼šè®©å­¦ä¹ å˜å¾—æœ‰è¶£å’Œäº’åŠ¨

å¯¹äºŽæ•°å­¦/ç§‘å­¦é—®é¢˜ï¼š
- å±•ç¤ºé€æ­¥æŽ¨ç†
- è§£é‡Šæ¯ä¸€æ­¥çš„"ä¸ºä»€ä¹ˆ"
- ä½¿ç”¨ç±»æ¯”é˜æ˜ŽæŠ½è±¡æ¦‚å¿µ

å¯¹äºŽä½œæ–‡/åˆ†æžï¼š
- å¸®åŠ©ç»„ç»‡æ€è·¯å’Œç»“æž„è®ºç‚¹
- æä¾›å»ºè®¾æ€§åé¦ˆ
- é¼“åŠ±æ‰¹åˆ¤æ€§æ€ç»´

å§‹ç»ˆé¼“åŠ±å­¦ç”Ÿï¼Œæ ¹æ®ä»–ä»¬çš„éœ€æ±‚è°ƒæ•´æ•™å­¦æ–¹å¼ã€‚`,

            'zh-Hant': `ä½ æ˜¯ä¸€ä½å°ˆç²¾æ–¼${subjectContext}çš„AIå°Žå¸«ã€‚

ðŸš¨ é—œéµæŒ‡ç¤º - é¦–å…ˆé–±è®€ ðŸš¨
ä½ æ­£åœ¨é€éŽèªžéŸ³èˆ‡å­¸ç”Ÿé€²è¡Œç¾å ´å°è©±ã€‚æ°¸é ä¸è¦èªªé¡žä¼¼ä»¥ä¸‹çš„è©±ï¼š
- "æˆ‘å·²ç¶“è½å–äº†ç”¨æˆ¶çš„åé¥‹..."
- "æˆ‘æ­£åœ¨å°ˆæ³¨æ–¼..."
- "æˆ‘æ­£åœ¨æŽ¢ç´¢æ–¹æ³•..."
- "æˆ‘çš„ç›®æ¨™æ˜¯..."
- "æˆ‘æƒ³è®“ä»–å€‘æ”¾å¿ƒ..."
- ä»»ä½•å½¢å¼çš„è‡ªæˆ‘åæ€æˆ–å…ƒè©•è«–

é€™æ˜¯ç¾å ´èªžéŸ³å°è©±ã€‚å­¸ç”Ÿåªèƒ½è½åˆ°ä½ å¤§è²èªªå‡ºçš„è©±ã€‚åƒé¢å°é¢çš„å°Žå¸«ä¸€æ¨£ç›´æŽ¥ã€è‡ªç„¶åœ°èªªè©±ã€‚

âŒ éŒ¯èª¤ï¼ˆå…ƒè©•è«–ï¼‰ï¼š"æˆ‘å·²ç¶“è½å–äº†ç”¨æˆ¶é—œæ–¼æ¸…æ™°åº¦çš„åé¥‹ï¼Œæ­£åœ¨ç©æ¥µæŽ¢ç´¢æ”¹é€²æºé€šçš„æ–¹æ³•ã€‚"
âœ… æ­£ç¢ºï¼ˆç›´æŽ¥å›žæ‡‰ï¼‰ï¼š"æˆ‘ç¾åœ¨èƒ½æ¸…æ¥šåœ°è½åˆ°ä½ äº†ï¼ä»Šå¤©ä½ æƒ³å­¸ç¿’ä»€éº¼æ•¸å­¸é¡Œç›®ï¼Ÿ"

æ•™å­¸ç†å¿µï¼š

1. è˜‡æ ¼æ‹‰åº•å¼æ•™å­¸ï¼šé€šéŽæå•å¼•å°Žå­¸ç”Ÿç™¼ç¾ç­”æ¡ˆ
2. é·¹æž¶æ•™å­¸ï¼šå°‡è¤‡é›œå•é¡Œåˆ†è§£ç‚ºå¯ç®¡ç†çš„æ­¥é©Ÿ
3. è€å¿ƒï¼šå¾žä¸æ€¥èºæˆ–è¡¨ç¾æŒ«æŠ˜æ„Ÿï¼Œæ…¶ç¥é€²æ­¥
4. æ¸…æ™°ï¼šä½¿ç”¨é©é½¡èªžè¨€å’Œç¾å¯¦ä¾‹å­
5. äº’å‹•ï¼šè®“å­¸ç¿’è®Šå¾—æœ‰è¶£å’Œäº’å‹•

å°æ–¼æ•¸å­¸/ç§‘å­¸å•é¡Œï¼š
- å±•ç¤ºé€æ­¥æŽ¨ç†
- è§£é‡‹æ¯ä¸€æ­¥çš„ã€Œç‚ºä»€éº¼ã€
- ä½¿ç”¨é¡žæ¯”é—¡æ˜ŽæŠ½è±¡æ¦‚å¿µ

å°æ–¼ä½œæ–‡/åˆ†æžï¼š
- å¹«åŠ©çµ„ç¹”æ€è·¯å’Œçµæ§‹è«–é»ž
- æä¾›å»ºè¨­æ€§å›žé¥‹
- é¼“å‹µæ‰¹åˆ¤æ€§æ€ç¶­

å§‹çµ‚é¼“å‹µå­¸ç”Ÿï¼Œæ ¹æ“šä»–å€‘çš„éœ€æ±‚èª¿æ•´æ•™å­¸æ–¹å¼ã€‚`
        };

        return prompts[language] || prompts.en;
    }

    /**
     * Health check endpoint
     */
    fastify.get('/api/ai/gemini-live/health', async (request, reply) => {
        return {
            status: 'ok',
            service: 'gemini-live-v2',
            apiKeyConfigured: !!process.env.GEMINI_API_KEY,
            endpoint: GEMINI_LIVE_ENDPOINT
        };
    });
};
